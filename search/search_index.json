{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#recent-updates","title":"Recent Updates","text":"<ul> <li>3x-ui V2Ray</li> <li>Ghost CMS Docker</li> <li>Docker Setup On Linux Server</li> <li>Wireguard</li> <li>Tailscale in Docker</li> <li>Speedtest</li> <li>jlesage VNC Apps</li> <li>OliveTin</li> <li>Samba(SMB) Setup</li> <li>IPv6 Oracle Cloud</li> </ul>"},{"location":"mkdocs/","title":"Mkdocs","text":""},{"location":"mkdocs/#mkdocs-gotchas","title":"Mkdocs Gotchas","text":"<ul> <li><code>yaml</code> highlighting is broken with <code>mdx-breakless-lists</code></li> <li>when using heading <code>#</code>, if there are no line breaks between headings, any lists that is after content of the second heading will not be rendered properly, even with <code>mdx-breakless-lists</code></li> <li>furthermore, if using lists right after a <code>yaml</code> code block, the list will also not be rendered correctly</li> <li></li> <li>when referencing a subheading in another file, mkdocs uses <code>[](file.md#heading-with-space)</code> while obsidian uses <code>[](file.md#heading%20with%20space)</code></li> <li>Before switching from lists to normal content, a line break is needed, otherwise the text below will be rendered with a indent</li> <li>mkdocs subheadings <code>[](#subheadings)</code> must be in lower case</li> </ul>"},{"location":"mkdocs/#admonitioncallouts","title":"Admonition/Callouts","text":"Mkdocs native callout <p>callout content mkdocs</p> <p>Nested</p> <p>Nesting</p> <ul> <li><code>???</code> is also valid syntax for mkdocs</li> <li><code>???+</code> makes the callout collapsible and opens by default, while <code>???-</code> makes it closed by default <pre><code>!!! notes \"Title\"\n    content\n</code></pre> Obsidian callouts requires the plugin <code>mkdocs-callouts</code></li> </ul> Obsidian Native Callout <p>Callout content mkdocs</p> <p>Nested callout</p> <p>callout</p> <pre><code>&gt; [!notes]+/- Callout title\n&gt; Callout content\n</code></pre> <ul> <li>obsidian callout syntax also follows the same <code>+</code>,<code>-</code> for collapsing, it is to be inserted after the brackets</li> </ul> <p>Available callouts include <code>notes</code>, <code>info</code>, <code>warning</code>, <code>danger</code>, <code>success</code>, <code>failure</code>, <code>example</code>, <code>abstract</code>, <code>tip</code>, <code>question</code>, <code>bug</code>.  </p>"},{"location":"mkdocs/#keys-caret-mark-tilde","title":"Keys, Caret, Mark, Tilde","text":"<p>Keys <code>++ctrl+alt+plus++</code> Ctrl+Alt++ mark highlighting tilde strikethrough</p>"},{"location":"mkdocs/#tabbed-content","title":"Tabbed Content","text":"Tab 1Tab 2 <p>Tab 1 content mkdocs Second line here.</p> <p>Tab 2 content</p> <p><pre><code>=== \"Tab Name\"\n    Tab content\n</code></pre> </p> <ul> <li>not supported in obsidian</li> </ul>"},{"location":"mkdocs/#attr_list","title":"attr_list","text":"<p>Fancy Buttons mkdocs <code>[button text](.\\mkdocs.md){ .md-button }</code> Tooltip I\u2019m a tooltip that you can hover or click. <code>[tooltip](https://link \"hover text\")</code> Annotation I\u2019m an annotation, but you need to click the plus icon (1) to show. (2) </p> <ol> <li>annotation 1</li> <li>annotation 2 <pre><code>Annotation location 1 (1), location (2)\n{ .annotate }\n1. annotation text to be shown\n</code></pre> </li> </ol> <p>Footnote Insert footnote like <code>[^1]</code> <sup>1</sup></p> <ul> <li>for inserting footnote <code>[^1]</code></li> <li><code>[^1]:</code> at the end to explain the footnote; not supported in obsidian</li> </ul>"},{"location":"mkdocs/#code-highlighting","title":"Code Highlighting","text":"<pre><code>from python import python\npython.run(arg1=123, arg2=\"mystr\")[2]\n</code></pre> <pre><code>#!/bin/bash\nvar=\"myvar\"\necho $var+3\n</code></pre> <pre><code># yaml highlighting has to be `yaml` not `yml` and it's broken\n---\nversion: \"2.1\"\nservices:\n  clarkson:\n    image: lscr.io/linuxserver/clarkson\n    container_name: clarkson\n    environment:\n\n      - PUID=1000\n      - PGID=1000\n    ports:\n      - 3000:3000\n    restart: unless-stopped\n</code></pre> <ol> <li> <p>explaining the footnote.\u00a0\u21a9</p> </li> </ol>"},{"location":"Cloud%20VPS/basic-server-setup-caddy-docker-tailscale/","title":"Basic Server Setup, Caddy, Docker, Tailscale","text":""},{"location":"Cloud%20VPS/basic-server-setup-caddy-docker-tailscale/#basics","title":"Basics","text":""},{"location":"Cloud%20VPS/basic-server-setup-caddy-docker-tailscale/#creating-the-vm-in-oracle-cloud","title":"Creating the VM in oracle cloud","text":"<ol> <li>Go to instances, new instance.</li> <li>Select the Always Free image, ARM or x86, recommended 4GB RAM.</li> <li>Choose Ubuntu image.</li> <li>Download the SSH key and name it accordingly.</li> </ol>"},{"location":"Cloud%20VPS/basic-server-setup-caddy-docker-tailscale/#ssh-keys","title":"SSH Keys","text":"<p>Using PuttyGen.</p> <ul> <li>Place the key in <code>./ssh/openssh_keys</code></li> <li>Open PuttyGen, conversion -&gt; import keys</li> <li>Save the key files as ppk file in root folder of <code>./ssh</code></li> </ul> <p>Putty</p> <ul> <li>Grab the IP address in the cloud console</li> <li>Give a name in saved sessions</li> <li>Go to behavior, choose these options</li> <li>Under Data, make sure Terminal-type string is xterm-256color</li> <li>Under Terminal -&gt; Features, check \u201cdisable application keypad mode\u201d to fix issues with nano</li> <li>The private key needs to be load in Connection -&gt; SSH -&gt; Auth -&gt; Credentials</li> </ul> <p></p> <p></p> <p>To get the IP address of the VPS at any time</p> <pre><code>curl ifconfig.me\n</code></pre> <p>Useful packages to install <pre><code>htop iotop iftop fio curl gnupg wget neofetch ca-certificates lsb-release fzf screen firewalld net-tools bash-completion\n</code></pre></p>"},{"location":"Cloud%20VPS/basic-server-setup-caddy-docker-tailscale/#docker","title":"Docker","text":"<p>https://docs.docker.com/engine/install/ubuntu/ <pre><code>sudo apt-get update\nsudo apt-get install \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\n\nsudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin docker-compose\n</code></pre></p> <pre><code>sudo groupadd docker \\\nsudo usermod -aG docker ubuntu\nnewgrp docker # activate docker group immediately\n</code></pre> <p>Alternative use <code>docker.io</code> for out of the box install as this package is provided by the distro repo. The machine needs to be rebooted from Oracle Cloud console to finish installation.</p> <p>Follow docker for more setup.</p>"},{"location":"Cloud%20VPS/basic-server-setup-caddy-docker-tailscale/#caddy","title":"Caddy","text":""},{"location":"Cloud%20VPS/basic-server-setup-caddy-docker-tailscale/#docker-version-install","title":"Docker Version Install","text":"<p>Detailed information on installing Caddy has moved to caddy If Nginx is installed alongside Caddy, it needs to be changed to listen on port 81 instead. <pre><code>sudo nano /etc/nginx/sites-enabled/default\n</code></pre></p> <ul> <li>change the <code>server</code> block\u2019s <code>listen</code> from 80 to 81 <pre><code>sudo service nginx restart\n</code></pre></li> </ul>"},{"location":"Cloud%20VPS/basic-server-setup-caddy-docker-tailscale/#port-forwarding","title":"Port Forwarding","text":"<p>On the Oracle Cloud side, login and go to <code>Virtual Cloud Networks</code>, click the one that\u2019s available, then the default subnet, this will bring up the <code>Security Lists</code> </p> <ul> <li>this is an example of SSH port, configure by <code>Add Ingress Rules</code> and add the ports accordingly; it\u2019s also possible to allow everything and install a firewall in the OS itself</li> </ul> <p>On the Linux machine, either use <code>iptables</code> or <code>firewall-cmd</code></p> Firewall-cmd (recommended)iptables <pre><code>sudo firewall-cmd --zone=public --add-port 19132/tcp --permanent\nsudo firewall-cmd --zone=public --add-port 19132/udp --permanent\nsudo firewall-cmd --zone=public --add-port 25565/tcp --permanent\nsudo firewall-cmd --zone=public --add-port 25565/udp --permanent\nsudo firewall-cmd --zone=public --add-port 80/tcp --permanent\nsudo firewall-cmd --zone=public --add-port 443/tcp --permanent\nsudo firewall-cmd --zone=public --add-port 5800/tcp --permanent\nsudo firewall-cmd --reload\n</code></pre> <p><code>sudo iptables -I INPUT 6 -m state --state NEW -p tcp --dport 443 -j ACCEPT sudo iptables -I INPUT 6 -m state --state NEW -p tcp --dport 80 -j ACCEPT sudo iptables -I INPUT 6 -m state --state NEW -p tcp --dport 25565 -j ACCEPT sudo iptables -I INPUT 6 -m state --state NEW -p tcp --dport 19132 -j ACCEPT sudo iptables -I INPUT 6 -m state --state NEW -p udp --dport 25565 -j ACCEPT sudo iptables -I INPUT 6 -m state --state NEW -p udp --dport 19132 -j ACCEPT sudo iptables -I INPUT 6 -m state --state NEW -p udp --dport 51820 -j ACCEPT sudo netfilter-persistent save</code> ^4f1e6a</p>"},{"location":"Cloud%20VPS/basic-server-setup-caddy-docker-tailscale/#troubleshooting","title":"Troubleshooting","text":"<p>For firewall-cmd, use this command to check all open ports.</p> <pre><code>sudo firewall-cmd --list-all\n</code></pre> <p>Using netstat, or pipe it to grep</p> <pre><code>netstat -tln\n# | grep 8080 etc...\n</code></pre>"},{"location":"Cloud%20VPS/basic-server-setup-caddy-docker-tailscale/#tailscale","title":"Tailscale","text":"<p>Installation and setup of basic services is covered in tunneling basic services. For usage such as exit-node and subnet-routes.</p>"},{"location":"Cloud%20VPS/basic-server-setup-caddy-docker-tailscale/#exit-nodesubnet-routes","title":"Exit Node/Subnet Routes","text":"<p>First need to enable IP forwarding. <pre><code>echo 'net.ipv4.ip_forward = 1' | sudo tee -a /etc/sysctl.conf\necho 'net.ipv6.conf.all.forwarding = 1' | sudo tee -a /etc/sysctl.conf\nsudo sysctl -p /etc/sysctl.conf\n</code></pre> When using with firewalld, additional configuration is needed such as masquerade. <pre><code>sudo firewall-cmd --add-masquerade --zone=public --permanent \nsudo firewall-cmd --add-interface=tailscale0 --zone=trusted --permanent\nsudo firewall-cmd --reload\n</code></pre> Basic command to advertise as exit-node and subnet routes <pre><code>sudo tailscale up --advertise-exit-node --advertise-subnet-routes=10.10.120.0/24\n</code></pre> When connect tailscale in CLI, additional arguments is needed to accept routes (the command below also activate exit node) <pre><code>sudo tailscale up --advertise-exit-node --accept-routes\n</code></pre> To enable these features, need to go to admin console, go to each machine settings, <code>Edit Route Settings</code> and enable exit-node or subnet routes.</p>"},{"location":"Cloud%20VPS/basic-server-setup-caddy-docker-tailscale/#advanced","title":"Advanced","text":"<p>Tunneling Jellyfin and other web services with tailscale and caddy</p> <p>Minecraft Tunneling</p>"},{"location":"Cloud%20VPS/basic-server-setup-caddy-docker-tailscale/#archived","title":"Archived","text":"<p>Basic Setup + Docker</p> <ol> <li>Installing Caddy web server (simple to use reverse proxy), lightweight, easy and no need for docker. (Nginx is also a good candidate for reverse proxy as the command is easy to memorize and does not require consulting documentation sites. However, the syntax for nginx is extremely complex compared to caddy and might not be easily memorized.</li> </ol> <p>https://caddyserver.com/docs/install#debian-ubuntu-raspbian</p> <pre><code>sudo apt install -y debian-keyring debian-archive-keyring apt-transport-https\ncurl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/gpg.key' | sudo gpg --dearmor -o /usr/share/keyrings/caddy-stable-archive-keyring.gpg\ncurl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/debian.deb.txt' | sudo tee /etc/apt/sources.list.d/caddy-stable.list\nsudo apt update\nsudo apt install caddy net-tools\n# net-tools is good utility, optionally can install firewall-cmd or nginx\n# sudo apt install firewalld nginx\n</code></pre> <p>Basic Caddy Syntax (if applicable) If the server that is being setup or restored needs functional service like bookstack or uptime-kuma, reverse proxy is needed. <pre><code>sudo nano /etc/caddy/Caddyfile\n</code></pre></p> <pre><code>{\n    email weebly2x10@gmail.com\n}\n\nyour-uptime-kuma.yoursubdomain.duckdns.org {\n        reverse_proxy http://127.0.0.1:3001\n}\n\nwiki.yoursubdomain.duckdns.org {\n        reverse_proxy http://127.0.0.1:6975\n}\n</code></pre>"},{"location":"Cloud%20VPS/docker/","title":"Docker Setup On Linux Server","text":""},{"location":"Cloud%20VPS/docker/#networkingenvironments","title":"Networking/Environments","text":"<p>By default when mapping a port on Docker, it will bind to all the interfaces, this is fine for home user behind a firewall but not suitable for VPS as all ports will be wide open on the internet. To fix this, only bind to tailscale IP when mapping ports. For convenience, setup PUID and PGID for future container setups.</p> <ul> <li>put this in the <code>bashrc</code> for automatic loading of Tailscale IP variable</li> <li>now the docker compose, all the environment variables can be used.</li> </ul> <p><pre><code>export TAILSCALE_IP=$(tailscale ip --4)\nexport PUID=$(id -u)\nexport PGID=$(id -g)\n</code></pre> <pre><code>    ports:\n\n      - ${TAILSCALE_IP}:8080:8080\n</code></pre></p>"},{"location":"Cloud%20VPS/docker/#logs","title":"Logs","text":"<p>Change this in <code>/etc/docker/daemon.json</code> <pre><code>{\n  \"log-driver\": \"local\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": 10\n  }\n}\n</code></pre> These options limit the max size of each uncompressed log to 10MB and only keep 10 more compressed logs in <code>.gz</code> format, in total each container have 100MB of historical data. The options are useful for container that log a lot and prevent log size on system too large. This will apply to every container, to apply to a single container, use <code>logging</code> in the compose file.</p>"},{"location":"Cloud%20VPS/jdownloader/","title":"JDownloader","text":""},{"location":"Cloud%20VPS/jdownloader/#basic-setup","title":"Basic Setup","text":""},{"location":"Cloud%20VPS/jdownloader/#configuring-jdownloader","title":"Configuring JDownloader","text":"<ul> <li>Go to the JDownloader WebUI</li> <li>Go to Settings</li> <li>Under general, change the max number of downloads (2) and DL per hoster (1) to minimize issues</li> </ul> <ul> <li>Go to MyJDownloader and configure MyJDownloader account</li> </ul> <ul> <li>Go to extension modules, install and enable \u201cfolderwatch\u201d</li> </ul> <p>The configuration for JDownloader is complete and should appear and be functional in WebUI. Advanced JDownloader documentation will be covered in detailed in another section. It is recommended to close port 5800 after configuring to prevent others accessing.</p> <p>After setting up JDownloader and it appears well in WebUI.</p> <p>The section is useless now as UHDMV has shutdown and it\u2019s pointless to setup multiple automated JDownloader server on VPS.</p>"},{"location":"Cloud%20VPS/jdownloader/#settings-for-jdownloader","title":"Settings for JDownloader","text":"<p>Debloat settings  https://rentry.org/jdownloader2 Advanced Settings <code>GraphicalUserInterfaceSettings: Banner</code> -&gt; disable <code>GraphicalUserlnterfaceSettings: Premium Alert Task Column</code> - &gt; disable <code>GraphicalUserInterfaceSeftings: Premium Alert Speed Column</code> -&gt; disable <code>GraphicalUserInterfaceSettings: Premium Alert ETA Column</code> -&gt; disable <code>GraphicalUsserInterfaceSeftings: Special Deal Oboom Dialog Visible On Startup</code> -&gt; disable <code>GraphicalUsserInterfaceSeftings: Special Deals</code>\u00a0-&gt; disable <code>GraphicalUsserInterfaceSeftings: Donate Button State</code>\u00a0-&gt; <code>Hidden (automode)</code></p>"},{"location":"Cloud%20VPS/jdownloader/#theming","title":"Theming","text":"<p><code>GraphicalUserInterfaceSettings: Look And Feel Theme</code> - &gt; <code>BLACK_EYE</code> For Colors <code>LAFSettings: Color For</code></p> <ul> <li>Panel background and header background and alternate row background- <code>#ff222222</code></li> <li>Selected Rows Background - <code>#ff666666</code></li> <li>Package Row Background - <code>#ff333333</code></li> <li>Mouse Over Row Background - <code>#ff666666</code></li> <li>Panel Header Foreground, Tooltip Foreground, Selected Rows Foreground, Package Row Foreground, Mouse Over Row Foreground, Alternate Row Foreground,  Account Temp Error Row Foreground, Account Error Row Foreground- <code>#ffffffff</code><ul> <li>basically, change all the black values to white when searching for <code>color fore</code>, change everything except blue colors and error color</li> </ul> </li> <li>Enabled Text Color, Speed Meter Text, Speed Meter Average Text, Config Panel Description Text, Config Header Text Color - <code>#ffffffff</code></li> <li>Disabled Text Color - <code>#ff666666</code><ul> <li>basically, when searching for <code>color text</code>, change all to white except for disabled text</li> </ul> </li> </ul>"},{"location":"Cloud%20VPS/minecraft-multi-server-srv/","title":"Multiple Minecraft Server on Same IP with SRV Record","text":"<p>Overall architecture </p>"},{"location":"Cloud%20VPS/minecraft-multi-server-srv/#dynu-settings","title":"Dynu Settings","text":"<p>Go to https://www.dynu.com/en-US/ControlPanel/DDNSRecord</p> <ul> <li>click the edit icon</li> <li>Manage DNS Records</li> <li>click Edit to add a SRV record</li> </ul> <p>Dynu or DDNS SRV records are different</p> <p>Since with Dynu, it gives a subdomain <code>sub.dynu.ddns</code>, it\u2019s different than owning a full domain <code>dynu.ddns</code>. The settings are different compared to when using a real domain registrar.</p> <p>Node Name - this should be <code>_minecraft._tcp.newserver</code> where <code>newserver.my.dynu.ddns</code> is what the user will enter when reaching secondary server Type - <code>SRV</code> Priority/Weight - not tested, setting it to 0 will work Port - the secondary port of Minecraft server Target - <code>my.dynu.ddns</code>, this should be the base domain</p> <p>This is all the entries in screenshot </p>"},{"location":"Cloud%20VPS/minecraft-multi-server-srv/#results","title":"Results","text":""},{"location":"Cloud%20VPS/oracle-cloud-IPv6/","title":"IPv6 Oracle Cloud","text":""},{"location":"Cloud%20VPS/oracle-cloud-IPv6/#vcn-settings","title":"VCN Settings","text":"<p>https://cloud.oracle.com/networking/vcns?region=ca-toronto-1 Navigate to the main VCN, then under CIDR Blocks and Prefixes, click <code>Add CIDR Block</code>  A GUA IPv6 range will be assigned by Oracle.</p> <p>Go to Route Tables and click the default one. </p> <ul> <li>choose <code>IPv6</code></li> <li>type is <code>Internet Gateway</code></li> <li>enter <code>::/0</code> for all IPv6 addresses</li> </ul> <p>Go to Security Lists and click the default one Ingress Rules</p> <ul> <li>IPv6-ICMP to allow pings to the server</li> <li>The ingress ports are applied since without it the server cannot access IPv6 websites  Egress Rules</li> <li>same as IPv4, allow all egress </li> </ul>"},{"location":"Cloud%20VPS/oracle-cloud-IPv6/#instance-settings","title":"Instance Settings","text":"<p>https://cloud.oracle.com/compute/instances?region=ca-toronto-1 Click on the instance name, and scroll down until <code>Attached VNIC</code> and click on the default Under resources there is <code>IPv6 Addresses</code> and click on <code>Assign IPv6 Address</code> </p> <ul> <li>multiple addresses can be assigned to the same machine, however these are single <code>/128</code> and it can only be added via Oracle cloud console</li> </ul> <p>After adding the address, refresh network service in Ubuntu. <pre><code>sudo systemctl restart systemd-networkd\n</code></pre></p>"},{"location":"Cloud%20VPS/tunneling-basic-services-jellyfin-web-with-caddy-and-tailscale/","title":"Tunneling Basic Services (Jellyfin, Web) with Caddy and Tailscale","text":"<p>This procedure is not reproducible yet. Rigorous testing is still required before being documented. Here are the known procedures.</p> <p>The purpose is to tunnel normal web or network intensive traffic such as Jellyfin when faced with CG-NAT or similar situations (in this case locked down dorm internet), also configure hardware transcoding (in this case NVENC, but Intel QSV for future) to mitigate limitations with Canadian ISP(s).</p>"},{"location":"Cloud%20VPS/tunneling-basic-services-jellyfin-web-with-caddy-and-tailscale/#jellyfin","title":"Jellyfin","text":""},{"location":"Cloud%20VPS/tunneling-basic-services-jellyfin-web-with-caddy-and-tailscale/#install","title":"Install","text":"<p>https://jellyfin.org/downloads/server Download and run the server installer. Configure Jellyfin to your liking.</p>"},{"location":"Cloud%20VPS/tunneling-basic-services-jellyfin-web-with-caddy-and-tailscale/#tailscale","title":"Tailscale","text":""},{"location":"Cloud%20VPS/tunneling-basic-services-jellyfin-web-with-caddy-and-tailscale/#windows","title":"Windows","text":"<p>https://tailscale.com/download/windows Download, install and login.</p>"},{"location":"Cloud%20VPS/tunneling-basic-services-jellyfin-web-with-caddy-and-tailscale/#linux","title":"Linux","text":"<pre><code>curl -fsSL https://tailscale.com/install.sh | sh\n</code></pre> <pre><code>sudo tailscale up\n</code></pre> <p>All the tailscale management is done in the WebUI.</p> <p></p> <p>The Windows client is given a tailscale network IP address in 100 range. Check if Windows client is pingable on server.</p> <pre><code>ping 100.x.y.z #100.79.28.31\n</code></pre> <p>Check if Jellyfin is running and tunneled properly on Oracle cloud. It should get a webpage html rather than unable to resolve host etc.</p> <pre><code>curl http://100.x.y.z:8096\n</code></pre>"},{"location":"Cloud%20VPS/tunneling-basic-services-jellyfin-web-with-caddy-and-tailscale/#reverse-proxy","title":"Reverse Proxy","text":"<p>basic-server-setup-caddy-docker-tailscale</p> <p>Caddy installation and syntax is can be found on this page. Replace 127.0.0.1 with the tailscale IP address.</p> <pre><code>{\n    email weebly2x10@gmail.com\n}\n\nmovies.yoursubdomain.duckdns.org {\n        reverse_proxy http://100.x.y.z:8096\n}\n</code></pre> <p>It is possible to set use the root domain (yoursub.duckdns.org) or a subfolder domain (movies.yousub.duckdns.org) for Jellyfin. After configuring the Caddyfile.</p> <pre><code>sudo systemctl reload caddy\n</code></pre> <p>Use netstat to check port 80, 443 is being listened. Make sure to port forward Oracle VPS.</p> <p>Other Services</p> <p>Follow the same syntax as the caddy file provided, if the root domain is used, then a subdomain must be used for other services.</p> <p>Results</p> <p>Inconclusive yet, more testing required.</p>"},{"location":"Cloud%20VPS/tunneling-minecraft-server-tcp-only-with-nginx/","title":"Tunneling Minecraft Server TCP/UDP Geyser with Nginx","text":"<p>After setting up the VPS and configuring Tailscale. Ensure hosts are connectible via <code>ping &lt;tailscale-ip&gt;</code>.</p>"},{"location":"Cloud%20VPS/tunneling-minecraft-server-tcp-only-with-nginx/#nginx","title":"Nginx","text":"<p><pre><code>sudo apt install nginx # Install Nginx\n</code></pre> Change default port to listen to <code>81</code> (this is not needed if another reverse proxy is installed or needed). Edit the <code>listen 80</code> to <code>listen 81</code>. The line after is not relevant if not using IPv6. In the case of Oracle Cloud, only IPv4 is used. <pre><code>sudo nano /etc/nginx/sites-enabled/default\n</code></pre> <pre><code>server {\n        listen 81 default_server;\n        listen [::]:81 default_server;\n</code></pre></p>"},{"location":"Cloud%20VPS/tunneling-minecraft-server-tcp-only-with-nginx/#configuration","title":"Configuration","text":"<p>Edit <code>/etc/nginx/nginx.conf</code> add these these <code>stream</code> lines <pre><code>sudo nano /etc/nginx/nginx.conf\n</code></pre> <pre><code>stream {\n     server {\n           listen 25565;\n           proxy_pass &lt;tailscale_ip&gt;:25565;\n    }\n    server {\n           listen 19132 udp;\n           proxy_pass &lt;tailscale_ip&gt;:19132;\n    }\n}\n</code></pre></p> <ul> <li>it is also possible to add <code>25565 udp</code></li> </ul> <p>Reload nginx if required. <pre><code>sudo service reload nginx\n</code></pre></p>"},{"location":"Cloud%20VPS/tunneling-minecraft-server-tcp-only-with-nginx/#firewall","title":"Firewall","text":"<p>Allow firewall connection. First ensure firewall-cmd is installed. If not, install the package <code>firewalld</code>. <pre><code>sudo firewall-cmd --zone=public --add-port 19132/tcp --permanent\nsudo firewall-cmd --zone=public --add-port 19132/udp --permanent\nsudo firewall-cmd --zone=public --add-port 25565/tcp --permanent\nsudo firewall-cmd --zone=public --add-port 25565/udp --permanent\nsudo firewall-cmd --reload\n</code></pre></p>"},{"location":"Cloud%20VPS/tunneling-minecraft-server-tcp-only-with-nginx/#results","title":"Results","text":"<p>In Minecraft client, by typing the public IP address of the Oracle Cloud it will connect. It\u2019s also an option to use Duckdns to make the cloud IP address more recognizable. </p> <p>The server address is permanent and unchanging.</p> <p>The performance depends on the location of VPS. There will be significant loss in responsiveness if the tunnel VPS is far away from the host server.  In this example, the host server is in Vancouver while the VPS is located in Toronto (3500/7000km). The round-trip to connect to the tunneled server added 150ms of latency (from Minecraft mobile app), the speed difference is also visualized in the image above. There is no difference between the <code>/tps</code> command as in both cases it\u2019s 20 all around.</p>"},{"location":"Computer%20Stuff/demucs-nvidia/","title":"Demucs Nvidia","text":"<p>Demucs is an music separation tool that has potential for a karaoke setup.</p> <p>https://github.com/facebookresearch/demucs</p> <p>https://www.youtube.com/watch?v=9QnFMKWEFcI&amp;t=585s</p> <p>https://docs.google.com/document/d/1XMmLrz-Tct1Hdb_PatcwEeBrV9Wrt15wHB1xhkB2oiY/edit</p> <p>Installation on PC with Nvidia</p> <ol> <li>Firstly install Anaconda. Download Anaconda for Windows https://www.anaconda.com/products/distribution</li> <li>Install PyTorch. https://pytorch.org/get-started/locally/. Select the correct version of pytorch.</li> <li>Install ffmpeg. [https://www.gyan.dev/ffmpeg/builds/]](assets/gallery/2022-12/TwJimage.png)</li> </ol> <p>Demucs</p> <p>After installing the prerequesties.</p> <p>Open \u201cAnaconda terminal\u201d and type</p> <pre><code>python.exe -m pip install -U demucs\n</code></pre> <pre><code>pip install PySoundFile \n</code></pre> <p>Running Demucs</p> <pre><code>demucs \"C:\\path\\to\\music\\file.mp3\"\n</code></pre> <p>This will run demucs with CUDA GPU acceleration, make sure to put the path in double quote. The extracted file will be found in the directory where you run the command eg. the default Anaconda prompt starts in ~/separated</p>"},{"location":"Computer%20Stuff/yt-dlp%20oauth/","title":"yt-dlp oauth","text":"<p>Using yt-dlp may need oauth2 plugin in order to use on VPS or download private videos.</p>"},{"location":"Computer%20Stuff/yt-dlp%20oauth/#linux","title":"Linux","text":"<p>Download https://github.com/coletdjnz/yt-dlp-youtube-oauth2/releases Add the zip file into this folder, make it if not exist <code>mkdir -p</code> <pre><code>~/.yt-dlp/plugins\n</code></pre> Then run <code>yt-dlp -v</code> and make sure <code>oauth2</code> appears in the log.</p>"},{"location":"Computer%20Stuff/yt-dlp%20oauth/#setup","title":"Setup","text":"<p><pre><code>yt-dlp --username oauth2 --password '' https://youtube.com/private-video\n</code></pre> It will prompt to go to https://www.google.com/device to enter a code.</p> <p>After registered device, setup <code>.netrc</code> and add these contents <pre><code>touch ${HOME}/.netrc\nchmod a-rwx,u+rw ${HOME}/.netrc\n</code></pre> <pre><code>machine youtube login oauth2 password \"\"\n</code></pre></p> <p>Now every-time when running the program, must append <code>--netrc</code> as an option <pre><code>yt-dlp --netrc https://youtube.com/watch?v=private\n</code></pre></p>"},{"location":"Computer%20Stuff/yt-dlp%20oauth/#window","title":"Window","text":"<p>Similar setup to Linux, except the recommendation location of plugins and <code>netrc</code> file Install the plugins into  <pre><code>$env:appdata/yt-dlp/plugins\n</code></pre> Similarly, the <code>.netrc</code> file can be created in home directory so it\u2019s picked up by yt-dlp <pre><code>$env:userprofile\n</code></pre></p>"},{"location":"Docker%20Apps/01-docker-infra/","title":"01 Docker Infrastructure","text":""},{"location":"Docker%20Apps/01-docker-infra/#filesystem","title":"Filesystem","text":""},{"location":"Docker%20Apps/01-docker-infra/#compose","title":"Compose","text":"<p>All <code>docker-compose.yml</code> files are stored in <code>~/docker</code> folder, which then by default is under the network <code>docker_default</code>.</p> <ul> <li>by default for newly created apps, a new folder is created and <code>docker-compose.yml</code> is created for that app for testing<ul> <li>once app testing is complete, the compose file can be moved docker root folder if appropriate or remain</li> </ul> </li> <li>some apps can be grouped together and these compose files are in the root docker folder such as <code>media.yml</code>, <code>network.yml</code>, the grouping allows multiple services to be managed by a single compose. For grouping, some of the property can include<ul> <li>the apps share common properties such as <code>arrs</code> apps</li> <li>it is preferable for apps to live in same network, eg. <code>teslamate</code></li> <li>a large app requiring multiple containers eg. <code>frontend</code>, <code>mysql</code> etc..</li> <li>apps share similar/same category, such as <code>qBittorrent</code> and <code>nzbget</code> can be put together in <code>downloader.yml</code> even though they do not have common properties or require same networking</li> </ul> </li> </ul>"},{"location":"Docker%20Apps/01-docker-infra/#storage","title":"Storage","text":"<p>The storage used for all containers are bind mount.</p> <ul> <li>application configs are stored in <code>~/docker/[app]</code><ul> <li>if an app has multiple components needing persistence (eg. app with database, helpers), a folder will be created as such <code>~/docker/[app]/postgres</code> etc.</li> </ul> </li> <li>apps that also store non-config data (such as music, documents etc.) and not using a lot of space can bind mount <code>/mnt/nvme/share</code> (a directory on local or another SSD) for fast data access and without spinning up HDD</li> <li>exceptions are home assistant or its related home automation containers and these are stored at <code>/srv/homeassistant</code></li> </ul>"},{"location":"Docker%20Apps/01-docker-infra/#backup","title":"Backup","text":"<p>The entire docker root folder is copied to a NFS share on another computer. With exception of minecraft and home assistant which a specialized method is used.</p>"},{"location":"Docker%20Apps/01-docker-infra/#network","title":"Network","text":"<p>With <code>docker-compose</code>, a new network is created with the name of folder the compose is located, while it\u2019s possible to change network, it is not straightforward, therefore, there is no points in manually defining networks unless required.</p> <p>Public <code>172.80.0.0/16</code> - bridge network for public facing applications with reverse proxy, this way when configuring Nginx Proxy Manager, all it need is to enter <code>container_name:80</code> rather than IP address.</p> <ul> <li>Nginx Proxy Manager - <code>172.80.44.3</code></li> <li>Other containers will use docker DHCP to get address</li> <li>Containers that need to public facing can attach to this network Media <code>172.96.0.0/16</code> - bridge network for arrs, downloader and management applications for easy interconnection when configuring Minecraft <code>172.255.255.0/24</code> - bridge network for Minecraft related networks</li> <li>Minecraft server (mcserver) - <code>172.255.255.65</code></li> </ul>"},{"location":"Docker%20Apps/01-docker-infra/#categories","title":"Categories","text":"<p>Media Apps - apps related to media acquisition, curation and other functions services for Jellyfin Networking - reverse proxy, DNS, VPN and related services Home Automation - home assistant and its associated functions VNC - containers based on jlesage-vnc-apps or Linuxserver Kasm images, usually desktop apps run in a browser via noVNC Management - tools for managing docker containers or entire server Games - game servers and associated tools Filesharing - apps that share files to other clients Documentation - notes and operation procedures for server infrastructure Authentication - services that handle single sign-on (SSO) with users</p>"},{"location":"Docker%20Apps/02-docker-ratings/","title":"Ratings","text":"<p>Docker App Rating consist of a table that look at the docker app and evaluate its configurations, deployment and usage against some quality of life features such as easy backup/restore, migration, user mapping, time zone logs, single-sign on with multi-user support etc. These ratings will change as more testing are done.</p> Docker Apps Rating U/GID TZ SSO/Users Existing FS Portable Subfolder \u274e \u2705* \u274c\ud83e\udd35 \u2705 \u274c \u274c"},{"location":"Docker%20Apps/02-docker-ratings/#ugid","title":"UGID","text":"<p>The Docker container/application or stack supports user ID and group ID mapping and respect the ID matching the host system. For example, Linuxserver.io and jlesage containers are gold standard.</p> \u2705 Natively Supported\u274e Supported\ud83d\udfe8 Usable\u274cNot Supported <ul> <li>All Linuxserver and jlesage containers and projects that build with their baseimages, uses environment variables <code>PUID/PGID</code> for mapping</li> <li>Fully respect UID and GID mappings on the host, will be able to all bind mounted files on the host with the respective permission without permission error and app issues</li> <li>All the files the app need to write to the bind mount are written with the ID set in environment variable and are accessible via anything such as VSCode and other containers</li> <li>For apps that don\u2019t have environment variables as above, but still following host user ID and permissions when modifying files will also have this rating <code>eg. Audiobookshelf, Navidrome</code></li> <li>If the app require multiple containers deployed as a stack and if the main app or the app that stores configuration/appdata fully support it but other part of the app do not, it will have <code>\u2705*</code> rating <code>eg. Bookstack</code></li> </ul> <ul> <li>The container do not have these environment variables and by default when it needs to create files on the host, it creates them with <code>root:root</code> permission but functions correctly </li> <li>The container permission can be fixed simply with <code>user: 1000:1001</code> in compose</li> <li>After this fix, these should not be any permission issues and the container functions without issues and create files that are accessible via anything <code>eg. Authelia, Jellystat</code></li> </ul> <ul> <li>The container do not support environment variables and by using <code>user:</code>, the functionality of the container is broken and have permission issues or still writes files as <code>root</code></li> <li>However, the container do not write configuration data or there is no need to have shared access of data</li> <li><code>eg. a database application, app that is entirely configured via environment/labels</code></li> </ul> <ul> <li>The container exhibit symptoms of <code>Usable</code> rating but <code>user:</code> either breaks the containers or still won\u2019t fix permission</li> <li>The container bind mounts to configuration or shared data that needs to be accessible by other tools, it would need constant <code>chown -R</code> to ensure access by others are possible</li> <li>By setting <code>user:</code> or <code>chown</code> to make files accessible to the host and other tools, the container cease to function</li> <li>Only named volumes can be used not bind mounts</li> </ul>"},{"location":"Docker%20Apps/02-docker-ratings/#tz","title":"TZ","text":"<p>The container support standard timezone variable. All logs generated by the container follows the timezone specified by <code>TZ</code> or other supported environment variables. This is either \u2705 or \u274c.</p>"},{"location":"Docker%20Apps/02-docker-ratings/#sso","title":"SSO","text":"<p>Users</p> <ul> <li>\ud83e\udd35: Only a single user/session is supported at a time</li> <li>\ud83d\udc6a: Multiple users are supported via SSO or internally</li> </ul> <p>Authelia  Authelia is the SSO provider that is used for the setup. Only support and compatibility for this will be documented. Only the main app via an exposed web interface need to support it, otherwise it\u2019s not applicable. If there are zero reasons to expose this app to the internet and have multiple local users, this is n/a.</p> \u2705 Natively Supported\u274e Supported\ud83d\udfe8 Usable\u274c Not Supported <ul> <li>App has OIDC support that works with third-party provider <code>eg. Audiobookshelf, Portainer</code></li> <li>App without advanced OIDC but have documented other ways to integrate SSO for users <code>eg. Filebrowser, Navidrome</code></li> <li>The user via SSO can be mapped to existing users with same name or creates the user if not exist</li> <li>App is able to fully integrate with ALL third party service or mobile/desktop apps flawlessly even after installing SSO/2FA</li> <li>Authelia whitelist rules can be easily created to restore full functionality of the app (<code>eg. API, public portion</code>) without compromising security where Authelia is needed </li> </ul> <ul> <li>App do not provide native integration for third party sign-in providers; but has an option to fully disable internal authentication in favor of Authelia <code>eg. Radarr, Nzbget</code></li> <li>App do not have internal authentication <code>eg. Memos, jlesage VNC</code></li> <li>By adding Authelia to add authentication or to replace internal authentication, the app is able to fully integrate with ALL third party service or mobile/desktop apps flawlessly even after installing SSO/2FA</li> <li>Authelia whitelist rules can be easily created to restore full functionality of the app (<code>eg. API, public portion</code>) without compromising security where Authelia is needed</li> <li>The above only apply with single-user apps, if a multi-user app do not natively support 3p SSO provider, Authelia is unable to passthrough the correct user</li> </ul> <ul> <li>Apps that have removable authentication or no authentication which Authelia can be added</li> <li>The only logical way to access the app is via a web browser where Authelia is fully supported</li> <li>Accessing the app via third-party services is restricted to LAN only or behind a VPN where Authelia is not relevant <code>eg. Nginx Proxy Manager, Teslamate</code></li> </ul> <ul> <li>The app has internal authentication that cannot be disabled or integrate with Authelia</li> <li>After installing Authelia, only way to use the app is via web browser; third party integration and mobile/desktop apps no longer function even with whitelisting rules <code>eg. Jellyfin, Home Assistant</code></li> <li>Using whitelist rules to restore functionality with third party apps would compromise security where Authelia is needed</li> <li>No workarounds are possible to have both SSO and 3p integrations</li> </ul>"},{"location":"Docker%20Apps/02-docker-ratings/#existing-fs","title":"Existing-FS","text":"<p>Existing filesystem structures, the app do not require a folder structure that only the app can use and is able to use it as is and allow user to not change workflow when switching to this app. (This section is incomplete, more updates needed)</p> <ul> <li>config: type of files that governs how an app behaves <code>eg. configuration.yaml, app.conf</code></li> <li>media: files includes videos, photos, documents or other files the user want the app to manage</li> </ul> \u2705 Yes\ud83d\udfe8 Partially\u274c No <ul> <li>App work with a bind mount to a host path where other process can also access it and the app do not have conflict with other processes</li> <li>App do not modify existing file structures and permissions</li> <li>User is able to import/export/edit data stored in the app (both configs and media) freely with or without the app <code>eg. Jellyfin, Filebrowser</code></li> <li>User is able to move relatively freely to a similar app</li> </ul> <ul> <li>(To be updated)</li> </ul> <ul> <li>App store its data (both config and media) in encrypted blob, proprietary format, specific database only the app can read</li> <li>App modify existing file structure for it to work and the permissions it need are incompatible with other workflows, refer to U/GID</li> <li>The only way to import/export/edit data is via the app, it\u2019s difficult to use another workflow</li> </ul>"},{"location":"Docker%20Apps/02-docker-ratings/#portable","title":"Portable","text":"<p>The portability of the app refers to how easy is it to migrate, backup/restore an app\u2019s config. If the frequency of backup/restore is irrelevant or no persistence data is needed such the app runs entirely via docker-compose, it\u2019d be n/a.</p> \u2705 Yes\ud83d\udfe8 Partially\u274c No <ul> <li>The app will work on another machine simply by copying the bind mount to the new machine</li> <li>If U/GID are not supported and a named volume is used, copying the volume with various tools will transfer the app to the new machine</li> <li>If an app uses a database, it will still work after either copying the bind path or volume to the new machine; if not, a repeatable and documented way to dump and import the database is provide so the app will transfer smoothly </li> <li>After the app is migrated, zero user intervention is needed and the app to function exactly the same</li> </ul> <ul> <li>App does not work by simply copying over the persistent data, but only a quick user intervention is needed <code>eg. backup/restore file in WebUI</code></li> <li>App data migration will work, but might require complex scripts or other dependencies that makes scripting harder</li> </ul> <ul> <li>App cannot be migrated or restored by simply copying the files, the app stop workings</li> <li>The backup process is difficult and often fails</li> <li>Even with a migration, heavy user intervention is needed for the app to function exactly the same if it\u2019s possible</li> </ul>"},{"location":"Docker%20Apps/02-docker-ratings/#subfolder","title":"Subfolder","text":"\u2705 Yes\u274c No <ul> <li>Subfolder refers to the ability to reverse proxy on a subdirectory like <code>domain.tld/app</code> instead of <code>app.domain.tld</code></li> <li>apps that support subfolder usually have configuration for a <code>baseURL</code> or have explicit documentation on how to proxy over subfolder in major reverse proxies</li> </ul> <ul> <li>App can only be reverse proxied as <code>app.domain.tld</code></li> </ul>"},{"location":"Docker%20Apps/02-docker-ratings/#mobile","title":"Mobile","text":"<p>The mobile refers to mobiles apps section, this rating determines the quality of mobile integration (only Android tested) since an app on mobiles offers more function than a website.</p> \u2705 Great\u2714 App Present/PWA\u274cNot Mobile Friendly <ul> <li>The app has a mobile app on app store or APK either from the developer itself or has viable well-maintained third-party apps</li> <li>The mobile app enhance the experience of the app and offers better usability compared to a web browser</li> <li>Mobile app offers deep integration with Android OS or other apps with widgets, controls, intents where nessecary (eg. Audiobookshelf, Home Assistant, Jellyfin, share icon to and from app)</li> </ul> <ul> <li>The app website has a mobile-friendly layout which a progressive-web app can be used and the webapp offers equivalent functionality to desktop counterpart</li> <li>The app in question is basic and all its functions are supported via a website without deep system integration (eg. Dashboard app for display only)</li> <li>App will be given <code>*</code> rating if the app does not have a mobile app or support PWA but it\u2019s mobile friendly when opened in a traditional mobile browser</li> </ul> <ul> <li>The app either do not have a mobile friendly website/app or it\u2019s mobile counterpart is not useable that a lot of desktop functionality is lost (eg. Grafana, webtop)</li> </ul>"},{"location":"Docker%20Apps/SponsorBlock/","title":"SponsorBlock API Mirror Server","text":"Docker Apps Rating U/GID TZ SSO/Users Existing FS Portable Subfolder Mobile \u274c \u274c n/a \u274c* \ud83d\udfe8 n/a n/a #1 Sponsorblock Mirror Servers Usability and Extensibility is Crippled b/c of Clients <p>Almost all relevant clients utilizing SponsorBlock (ReVanced, NewPipe, Extension, FreeTube, SmartTube) uses hash lookup. Where the client sha256 hashes the videoID then lookup from the server, the server then return all videoIDs that matches the first 4 letters of the hash. There could be hundreds of videos that has matching hash, it is impossible for the server to know the videoID requested. While it improves \u201cpRiVaCy\u201d, it severely degrades functionality. What if the database does not contain sponsor segments for a video? The server can\u2019t do anything about it. Client behavior should not be modified because many clients exists and written in different language, it\u2019s not feasible to modified all clients and have all of it up-to-date. For the mirror to be useful whether main SponsorBlock server is down or not, the database must contain every single submission for all the video that ever existed and updated in real-time. Which is not possible as in #2 and #3.  </p> #2 This Server Works in Reverse <p>The server first queries the local database for video data, and only queries the main server if no data is found. This works well for <code>videoID</code> queries but causes issues with hashed <code>videoID</code>.</p> <p>For example, suppose Video A has <code>videoID</code> <code>aabbcc</code> and Video B has <code>videoID</code> <code>112233</code>, and both videos have sponsor segments. Video A\u2019s segments are in the local database, while Video B\u2019s segments are only on the main server (due to great difficulty in syncing database in #3). Both video hashes start with <code>abcd</code>, the client querying for Video B (<code>/api/skipSegments/abcd</code>) will get nothing. Since data is found when the server searches for <code>abcd</code>, the server doesn\u2019t query the main server for Video B\u2019s segments, returning incomplete results.</p> <p>The better option is to query the main server first since it\u2019s the most accurate, then lookup the local database when the main server is down. This increase the usefulness and accuracy of the mirror server. </p> The sb-mirror Project or Rsync is not Incremental nor Consistent Making it Difficult to Sync Data (extensive testing required) <p>The submitted SponsorBlock segments are periodically dumped in a very large (4GB) csv file. CSV text based files are not easy for machine to process. It also takes a long time to import to the compatible Postgres database (500-800s). Other have proposed the solution of rsync using <code>sb-mirror</code>, but that is problematic too. There are only 2 rsync providers <code>sponsorblock.kavin.rocks</code> and <code>sponsor.ajay.app</code> official. The one provided by <code>kavin</code> does not appear to be updated or the timestamping is broken. The official one is the most accurate and updated frequently. However, it is slow and inconsistent. It constantly have issues such as 200 max limit reached, when it did connect, the speed is 300-1000 kbps, which takes hours to sync <code>sponsorTimes.csv</code>. While rsync can be incremental, <code>sponsorTimes.csv</code> is not, every time it updates, the whole thing has to be downloaded. Again\u2026 Fast updates are impossible due to connection limits and slow speed and a huge amount of bandwidth is used because of non-incremental updates.</p> <p>The way SponsorBlock clients function in #1 require the mirror server to have everything that ever existed and updated frequently. The current way of <code>.csv</code> dumps are inefficient and not practical.</p> Modified Approach and Other Compromises <p>The project in this documentation uses a modified approach. It uses another mirror <code>sb.minibomba.pro</code>, while it still requires downloading every time it updates, the server is much faster and provided 1GB compressed files (takes only 2 min to download). Instead of importing everything in the DB which is intensive, this approach query the existing DB and get the latest timestamp, then filters the new csv file for entries above that timestamp, create a diff and import into database; the whole compute takes 15s. It will import any new segments in the new csv file, but older entries that was updated will not be reflected. A full DB reset is performed every week for clean start. This is a compromise, others include</p> <ul> <li>mirror server cannot provide non-sponsor segments (selfpromo, intros etc..)</li> <li>any disruptions to the mirror server will trigger full DB rebuild</li> </ul>"},{"location":"Docker%20Apps/SponsorBlock/#gosb","title":"gosb","text":"<p>Simple implementation of SponsorBlock in Go. https://github.com/wereii/gosb Not working 404</p>"},{"location":"Docker%20Apps/SponsorBlock/#sponsorblock-mirror","title":"SponsorBlock Mirror","text":"<p>https://github.com/TeamPiped/sponsorblock-mirror Rust implementation of mirror server. It consists of 3 services</p> <ul> <li>Mirror server</li> <li>Postgres database</li> <li>- Rsync Mirror</li> </ul>"},{"location":"Docker%20Apps/SponsorBlock/#configuration","title":"Configuration","text":"<p><pre><code>  postgres:\n    image: postgres:16-alpine\n    container_name: postgres-sb-mirror\n    shm_size: 1g\n    volumes:\n\n      - ~/docker/sponsorblock/db:/var/lib/postgresql/data\n      - ~/docker/sponsorblock/mirror:/mirror\n    env_file:\n      - .env\n    restart: unless-stopped\n\n  sponsorblock-mirror:\n    image: 1337kavin/sponsorblock-mirror:latest\n    container_name: sponsorblock-mirror\n    user: 1000:1001\n    volumes:\n\n      - ~/docker/sponsorblock/mirror:/app/mirror\n    ports:\n      - 6969:8000\n    restart: unless-stopped\n    depends_on:\n      - postgres\n</code></pre> Content of <code>.env</code> contains <pre><code>POSTGRES_DB=sponsorblock\nPOSTGRES_PASSWORD=\nPOSTGRES_USER=sponsorbl\n</code></pre></p>"},{"location":"Docker%20Apps/SponsorBlock/#db-dumps","title":"DB Dumps","text":"<p>The dumps are stored locally at  <code>./mirror/sponsorTimes.csv</code></p>"},{"location":"Docker%20Apps/SponsorBlock/#httpmanual","title":"HTTP/Manual","text":"<p>https://wiki.sponsor.ajay.app/w/API_Docs https://sb.ltn.fi/database/ ~ up to 1 week delay https://sb.minibomba.pro/mirror/ ~ 3hrs delay</p>"},{"location":"Docker%20Apps/SponsorBlock/#rsync","title":"Rsync","text":"<pre><code>rsync --list-only rsync:///rsync.sponsor.ajay.app:31111/sponsorblock\nrsync --list-only rsync://sponsorblock.kavin.rocks/sponsorblock\n</code></pre>"},{"location":"Docker%20Apps/SponsorBlock/#behavior","title":"Behavior","text":"<p>It\u2019s not ready to test the resiliency of server when SponsorBlock goes down yet. It is not feasible to use this in restricted network where dynamic DNS are blocked unless used via tailscale exit node.</p> Main Server (row) / CSV (col) segment exist does not exist segments exist CSV takes precedence depends, if there is not another video that has the first 4 letters of sha256 that exist in database -&gt; query main server for segments; if there is another video id that has the same sha256 as this one and that exist in the database -&gt; no segments will be provided does not exist CSV segments no segments <p>For the server to be useful, it must have an up-to-date csv dump. Any interruptions to the mirror server container will trigger a full database re-import which could take a long time.</p> <ul> <li>eg. reboot host, reboot/start container, container crash</li> <li>the re-import require 2x the database storage, the database will be shrunk once import is done</li> <li>for a 4.5GB csv file, the Postgres takes up 8.5GB space.</li> </ul>"},{"location":"Docker%20Apps/SponsorBlock/#postgres","title":"Postgres","text":"<p>To execute Postgres commands. <pre><code>docker exec -it postgres-sb-mirror psql -U sponsorblock\n</code></pre></p> <p>The following SQL will manually add an entry to the database, any updates to the database is immediate. <pre><code>INSERT INTO \"sponsorTimes\" \n(\"videoID\", \"startTime\", \"endTime\", \"votes\", \"locked\", \"incorrectVotes\", \"UUID\", \"userID\", \"timeSubmitted\", \"views\", \"category\", \"actionType\", \"service\", \"videoDuration\", \"hidden\", \"reputation\", \"shadowHidden\", \"hashedVideoID\", \"userAgent\", \"description\")\nVALUES \n('videoID', 699.111start, 893.201end, 10, 0, 0, 'UUID', 'userID', 1658232826797, 0, 'sponsor', 'skip', 'YouTube', 3839.661duration, 0, 0, 0, 'hashedvideoID', 'psqlmirror/v4.6.4', '');\n</code></pre></p> <ul> <li><code>videoID</code>, <code>startTime</code>, <code>endTime</code>, <code>timeSubmitted</code> are configurable</li> <li><code>hashedVideoID</code> is required and can be generated from the <code>videoID</code> in sha256</li> <li><code>UUID</code> is required and has to be unique</li> <li>the attributes <code>shadowhidden</code>, <code>hidden</code> and <code>locked</code> must be 0</li> </ul> <p>When manually importing, if there are overlapping sponsorship entries, the entry that have the longer end time will take precedence, not the one that is imported last.</p>"},{"location":"Docker%20Apps/SponsorBlock/#sb-mirror","title":"SB Mirror","text":"<p><pre><code>  sb-mirror:\n    image: mchangrh/sb-mirror:latest\n    user: 1000:1001\n    environment:\n\n      - MIRROR_URL=mirror.sb.mchang.xyz # override to set upstream mirror\n    volumes:\n      - ./mirror:/mirror\n</code></pre> For additional options: https://github.com/mchangrh/sb-mirror Although rsync is used, the transfer is not incremental, hence downloading from a fast compressed archive is preferred. eg. <code>sb.minibomba.pro</code></p>"},{"location":"Docker%20Apps/SponsorBlock/#modified-implementation","title":"Modified Implementation","text":"<p>The following modifications drastically speed up database refresh (from 500-800s to 10-15s) excluding download. It only import new entries (after the last database update). The full implementation is a bash script that can be automated using crontab, OliveTin or Home Assistant.</p> <p>Download and extract the database (use <code>aria2</code> for even faster downloads) <pre><code>wget https://sb.minibomba.pro/mirror/sponsorTimes.csv.zst\nzstd -d sponsorTimes.csv.zst -o sponsorTimes.new.csv\nrm sponsorTimes.csv.zst\n</code></pre> <pre><code>sudo apt install aria2 -y\naria2c -x 10 https://sb.minibomba.pro/mirror/sponsorTimes.csv.zst\n</code></pre></p> <p>Find the latest item\u2019s time submitted in Postgres <pre><code>docker exec -it postgres-sb-mirror psql -U sponsorblock -d sponsorblock -t -c 'SELECT \"timeSubmitted\" FROM \"sponsorTimes\" ORDER BY \"timeSubmitted\" DESC LIMIT 1;' | tr -d '[:space:]'\n</code></pre></p> <ul> <li>this also trim whitespace</li> </ul> <p>Create the diff <pre><code>awk -F, -v val=\"$val\" '$9 &gt; val' sponsorTimes.new.csv | grep -v \"hashedIP\" &gt; diff.csv\n</code></pre></p> <ul> <li>command also remove entries with <code>hashedIP</code> which can create errors with <code>awk</code> and the import</li> </ul> <p>Import the difference <pre><code>docker exec -it postgres-sb-mirror psql -U sponsorblock -d sponsorblock -c 'COPY \"sponsorTimes\" FROM '\\''/mirror/diff.csv'\\'' WITH (FORMAT csv, HEADER true);'\n</code></pre></p> <p>Cleanup <pre><code>mv sponsorTimes.new.csv sponsorTimes.csv\nrm diff.csv\n</code></pre></p>"},{"location":"Docker%20Apps/SponsorBlock/#future-considerations","title":"Future Considerations","text":"<p>Reverse the server logic, first query the official SponsorBlock server, if it times out, then lookup the database. Script/program that utilize YouTube RSS feed, subscriptions or other libraries to get the sponsor segments from popular or channels that is most likely to be watched more frequently and update the database.</p>"},{"location":"Docker%20Apps/bookstack/","title":"Bookstack","text":""},{"location":"Docker%20Apps/bookstack/#installation","title":"Installation","text":"<p>Change port to 6975</p> <p>Add in docker-compose: restart: unless-stopped</p> <p>$docker directory = /home/docker .... etc</p> <p>Docker-Compose file reference</p> <p>https://github.com/solidnerd/docker-bookstack/blob/master/docker-compose.yml</p> <pre><code>version: '2'\nservices:\n  mysql:\n    image: mysql:8.0\n    environment:\n\n    - MYSQL_ROOT_PASSWORD=secret\n    - MYSQL_DATABASE=bookstack\n    - MYSQL_USER=bookstack\n    - MYSQL_PASSWORD=secret\n    volumes:\n    - mysql-data:/var/lib/mysql\n    restart: unless-stopped\n\n  bookstack:\n    image: solidnerd/bookstack:22.10.2\n    depends_on:\n\n    - mysql\n    environment:\n    - DB_HOST=mysql:3306\n    - DB_DATABASE=bookstack\n    - DB_USERNAME=bookstack\n    - DB_PASSWORD=secret\n    #set the APP_ to the URL of bookstack without without a trailing slash APP_URL=https://example.com\n    - APP_URL=http://xxx.xxxmydomainxxx.duckdns.org\n    volumes:\n    - $docker/public-uploads:/var/www/bookstack/public/uploads\n    - $docker/storage-uploads:/var/www/bookstack/storage/uploads\n    ports:\n    - \"6975:8080\"\n    restart: unless-stopped\n</code></pre> <p>Notice: The default password for bookstack is</p> <p>admin@admin.com</p> <p>password</p> <p>Permissions: remember the set write permission on public-uploads folder so users can upload photos.</p>"},{"location":"Docker%20Apps/bookstack/#backup-and-restore","title":"Backup and Restore","text":"<p>Files Backup:</p> <pre><code>tar -czvf bookstack-files-backup.tar.gz public-uploads storage-uploads\n</code></pre> <p>Restore:</p> <pre><code>tar -xvzf bookstack-files-backup.tar.gz\n</code></pre> <p>Database backup:</p> <pre><code>sudo docker exec bookstack_mysql_1 /usr/bin/mysqldump -u root --password=secret bookstack &gt; ./bookstack/bookstack_db.sql\n</code></pre> <p>Restore:</p> <pre><code>sudo docker exec -i bookstack_mysql_1 mysql -u root --password=secret bookstack &lt; /$docker/bookstack/bookstack_db.sql\n</code></pre> <ul> <li>bookstack_mysql1 is the container name</li> <li>password is secret or the database password</li> </ul>"},{"location":"Docker%20Apps/bookstack/#reverse-proxy","title":"Reverse Proxy","text":"<p>Use subdomain in proxy manager.</p> <p>Backing Up and Restoring with LinuxServer.io container</p> <p>Due to limits or Oracle Cloud free tier. The only arm image is from linuxserver io container, and it is different than solidnerd image.</p> <p>Docker-Compose file</p> <pre><code>version: \"2\"\nservices:\n  bookstack:\n    image: lscr.io/linuxserver/bookstack\n    container_name: bookstack\n    environment:\n\n      - PUID=1001\n      - PGID=1001\n      - APP_URL=https://wiki.xxx.duckdns.org\n      - DB_HOST=bookstack_db\n      - DB_USER=bookstack\n      - DB_PASS=secret\n      - DB_DATABASE=bookstackapp\n    volumes:\n      - /home/ubuntu/bookstack:/config\n    ports:\n      - 6975:80\n    restart: unless-stopped\n    depends_on:\n      - bookstack_db\n\n  bookstack_db:\n    image: lscr.io/linuxserver/mariadb\n    container_name: bookstack_db\n    environment:\n\n      - PUID=1001\n      - PGID=1001\n      - MYSQL_ROOT_PASSWORD=secret\n      - TZ=Europe/London\n      - MYSQL_DATABASE=bookstackapp\n      - MYSQL_USER=bookstack\n      - MYSQL_PASSWORD=secret\n    volumes:\n      - /home/ubuntu/bookstack:/config\n    restart: unless-stopped\n</code></pre> <p>Notice: In Oracle cloud free tier, the default ubuntu user is 1001, not 1000. For database name, it it bookstackapp, keep in mind when executing restore command. The folder structure is also different. In the solidnerd container, the images are stored at /public-uploads while in LSIO container it is stored at /www/uploads</p>"},{"location":"Docker%20Apps/bookstack/#backing-up-from-home-pc","title":"Backing Up (from home PC)","text":"<p>Images</p> <p>cd into /public-uploads and make a tar archive</p> <pre><code>tar -czvf images.tar.gz images\n</code></pre> <p>Backup the database</p> <pre><code>sudo docker exec bookstack_mysql_1 /usr/bin/mysqldump -u root --password=secret bookstack &gt; ./bookstack_db.sql\n</code></pre> <p>Transfer to Oracle Cloud Server</p> <pre><code>scp -i oracle-arm-2.key images.tar.gz bookstack_db.sql ubuntu@$IPADDR:/home/ubuntu/bookstack/www/uploads\n</code></pre> <p>Take in consideration the location where LSIO image stores the images.</p>"},{"location":"Docker%20Apps/bookstack/#restore-into-oracle-cloud","title":"Restore (into Oracle Cloud)","text":"<p>Images (/home/ubuntu/bookstack/www/uploads)</p> <pre><code>tar -xvzf images.tar.gz\n</code></pre> <p>Database</p> <p>The image url in the database still refers to old server url, it needs to be changed. The following command replace the subdomain in the sq1 dump.</p> <pre><code>sed -i 's/wiki.$home.duckdns.org/wiki.$oracle.duckdns.org/g' bookstack_db.sql\n</code></pre> <p>Restore the database.</p> <pre><code>sudo docker exec -i bookstack_db mysql -u root --password=secret bookstackapp &lt; /home/ubuntu/bookstack/www/uploads/bookstack_db.sql\n</code></pre>"},{"location":"Docker%20Apps/bookstack/#crontab","title":"Crontab","text":"<p>On Home PC</p> <pre><code>0 23 * * 2,5 /home/karis/bookstack.sh\n</code></pre> <pre><code>#!/bin/bash\n\ncd ~/docker/bookstack/public-uploads #location of bookstack public uploads\ntar -czvf images.tar.gz images\nsudo docker exec bookstack_mysql_1 /usr/bin/mysqldump -u root --password=secret bookstack &gt; ./bookstack_db.sql\nscp -i oracle-arm-2.key images.tar.gz bookstack_db.sql ubuntu@$ORACLEIP:/home/ubuntu/bookstack/www/uploads\n</code></pre> <p>Make sure to copy the oracle-arm-2.key to the appropriate location (~/docker/bookstack/public-uploads)</p> <p>Also make sure the permission of oracle-arm-2.key is in correct permission (600). Especially changing the permission of public-uploads folder to allow write access.</p> <p>Do a backup sequence in crontab at 11pm every Tuesday and Friday.</p> <p>Oracle Cloud Server</p> <pre><code>0 8 * * 3,6 /home/ubuntu/bookstack.sh\n</code></pre> <pre><code>#!/bin/bash\n\ncd ~/bookstack/www/uploads #directory where bookstack files scp from home are located\ntar -xvzf images.tar.gz\nsed -i 's/wiki.$homeip.duckdns.org/wiki.$oracle.duckdns.org/g' bookstack_db.sql\nsudo docker exec -i bookstack_db mysql -u root --password=secret bookstackapp &lt; /home/ubuntu/bookstack/www/uploads/bookstack_db.sql\n</code></pre> <p>Restore the sequence after backup, every Wednesday and Saturday at 8am (need to consider the TZ between Vancouver, Edmonton and Toronto, or any the time zone of the remote server)</p>"},{"location":"Docker%20Apps/code-server/","title":"VSCode Server","text":"Docker Apps Rating U/GID TZ SSO/Users Existing FS Portable Subfolder Mobile \u274e \u2705* \u274e\ud83e\udd35 \u2705 \u2705 \u2714 \u2714 <p> <pre><code>services:\n  code-server:\n    image: lscr.io/linuxserver/code-server:latest\n    container_name: code-server\n    environment:\n\n      - PUID=1000\n      - PGID=1001\n      - TZ=America/Vancouver\n      - DEFAULT_WORKSPACE=/projects #optional\n      - DOCKER_MODS=linuxserver/mods:code-server-python3\n    env_file:\n      - .env\n    volumes:\n      # Master configuration\n      - ~/docker/code-server:/config\n      # dotfiles\n      - ~/.bashrc:/config/.bashrc\n      - ~/.ssh:/config/.ssh\n      - ~/.gitconfig:/config/.gitconfig\n      - ~/Documents/ssh:/config/Documents/ssh # ssh keys\n      # Workspace folders (eg. Docker, other projects)\n      - ~/docker:/docker\n      - ~/projects:/projects\n    ports:\n      - 4443:8443\n    networks:\n      - public\n    restart: unless-stopped\n\nnetworks:\n  public:\n    name: public\n    external: true\n</code></pre></p>"},{"location":"Docker%20Apps/code-server/#setup","title":"Setup","text":"<p>The setup follows the same 01-docker-infra, since this is externally accessible, it has a network of <code>public</code></p>"},{"location":"Docker%20Apps/code-server/#environments","title":"Environments","text":"<p><code>DEFAULT_WORKSPACE</code> - the directory that VSCode will open to when accessing it, defaults to <code>/config</code> The container is Linuxserver so it follows their standards of PUGID and TZ The Docker Mods will add python3 into the environment for debugging python files.</p>"},{"location":"Docker%20Apps/code-server/#env-file","title":"env file","text":"<p><pre><code>HASHED_PASSWORD=\n</code></pre> The environment file should contain the hashed password, use https://argon2.online/ to generate a hashed password. Although 2FA and SSO is supported, it is still recommended to put another layer of password since VSCode server have access to very sensitive files.</p>"},{"location":"Docker%20Apps/code-server/#directory","title":"Directory","text":"<p>The base configuration is stored in <code>~/docker/code-server</code> as usual</p> <ul> <li>the bind mount <code>/config</code> is the container is also the <code>XDG_HOME</code> which is the default Linux home directory Given that the config is also the home directory, many dotfiles such as bash, git and SSH configuration need to be bind mounted from the hosts <code>~</code> directory into containers <code>/config</code> or home directory.</li> </ul> <p>The workspace folder contains <code>docker</code> (docker configuration and data) and <code>projects</code> (cloned from git repo) which are frequently edited files.</p>"},{"location":"Docker%20Apps/code-server/#usage","title":"Usage","text":"<p>The app functions similarly to VSCode and mostly follows the shortcut of the desktop version. Such as Ctrl+Shift+P to open command palette. The app has high idle usage, try to close workspace/sign out and restart after editing, or use solutions to use on-demand. The app also have access to Github accounts, first clone a private repo and code-server will prompt for login for Github.</p>"},{"location":"Docker%20Apps/code-server/#problems","title":"Problems","text":"<p>Official account sync login doesn\u2019t work, third party extensions doesn\u2019t work either, so the settings has to be done manually.</p> <ul> <li>for basic configurations <code>keybindings.json</code> and <code>settings.json</code> contains all the theme and extension settings for a minimal viable VSCode</li> <li>the <code>json</code> files are periodically copied from main Desktop and synced to the server via Syncthing</li> </ul> <p>Remote SSH doesn\u2019t work, but can be solved by SSH extension. Github Copilot doesn\u2019t work. Python syntax highlighting doesn\u2019t work.</p>"},{"location":"Docker%20Apps/code-server/#extensions","title":"Extensions","text":""},{"location":"Docker%20Apps/code-server/#ssh","title":"SSH","text":"<p>Since the default Remote SSH doesn\u2019t work, the extension SSH FS can be used.  The configuration is done in <code>settings.json</code> and will show up in UI. <pre><code>\"sshfs.configs\": [\n\u00a0 \u00a0 {\n\u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"mediaserver-docker\",\n\u00a0 \u00a0 \u00a0 \u00a0 \"host\": \"10.10.120.16\",\n\u00a0 \u00a0 \u00a0 \u00a0 \"username\": \"karis\",\n\u00a0 \u00a0 \u00a0 \u00a0 \"privateKeyPath\": \"/config/Documents/ssh/openssh_keys/mediaserver.key\",\n\u00a0 \u00a0 \u00a0 \u00a0 \"root\": \"~/docker\"\n\u00a0 \u00a0 },\n\u00a0 \u00a0 {\n\u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"mediaserver-homeassistant\",\n\u00a0 \u00a0 \u00a0 \u00a0 \"host\": \"10.10.120.16\",\n\u00a0 \u00a0 \u00a0 \u00a0 \"username\": \"karis\",\n\u00a0 \u00a0 \u00a0 \u00a0 \"privateKeyPath\": \"/config/Documents/ssh/openssh_keys/mediaserver.key\",\n\u00a0 \u00a0 \u00a0 \u00a0 \"root\": \"/srv/homeassistant\"\n\u00a0 \u00a0 }\n\u00a0 \u00a0 ],\n</code></pre> The options from left to right are</p> <ul> <li>Open Folder - open the folder (configured as <code>root</code>) to workspace for editing, for SSH connections outside the network like VPS, the speed is slow</li> <li>Open Terminal - start a terminal session to the remote folder</li> <li>Settings</li> <li>Disconnect - after editing the remote folder, this will remote the remote folder from the workspace</li> </ul>"},{"location":"Docker%20Apps/code-server/#reverse-proxyauthentication","title":"Reverse Proxy/Authentication","text":""},{"location":"Docker%20Apps/code-server/#reverse-proxy","title":"Reverse Proxy","text":"<p>The app supports both subdomain and subpath for proxying in Nginx Proxy Manager.</p>"},{"location":"Docker%20Apps/code-server/#sso","title":"SSO","text":"<p>For Authelia SSO in Nginx Proxy Manager and custom location support. <pre><code>location /vscode/ {\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection $http_connection;\n    include /snippets/proxy.conf;\n    include /snippets/authelia-authrequest.conf;\n    proxy_pass http://10.10.120.12:4443/;\n}\n</code></pre> This configuration support all of websocket, subpath and authelia. No additional authelia whitelist is needed.</p>"},{"location":"Docker%20Apps/ddns-update/","title":"Dynamic DNS Updater Docker","text":"<p>Official Image: https://hub.docker.com/r/linuxserver/duckdns Custom Github Page: https://github.com/vttc08/docker-duckdns-dynu</p> <p>This is a docker container that automatically updates the public IPv4 address of the server every 5 minutes to dynamic DNS services Dynu and DuckDNS. It is the fork of Linuxserver DuckDNS container.</p>"},{"location":"Docker%20Apps/ddns-update/#docker-compose","title":"Docker Compose","text":"<pre><code>  services:\n      duckdns:\n        image: vttc08/docker-duckdns-dynu:latest\n        container_name: duckdns\n        env_file: ddns.env\n        environment:\n\n          - TZ=America/Vancouver\n          - PUID=1000\n          - PGID=1001\n        restart: unless-stopped\n</code></pre> <p>These need to be filled in the <code>ddns.env</code> <pre><code>DYNU_HOST= # full name of dynu domains\nDYNU_PASS= # md5 hashed dynu login pass\nSUBDOMAINS= # DuckDNS domains without the duckdns.org part\nTOKEN= # DuckDNS token \n</code></pre></p> <ul> <li>token will be visible in DuckDNS dashboard</li> <li>Dynu pass is the same as login; alternatively, it is possible to create a dedicated password  just for IP update  MD5 generator <pre><code>echo -n \"password\" | md5sum\n</code></pre></li> <li>when setting the IP to <code>10.0.0.0</code> in Dynu update API, dynu will automatically update the IP address to the IP address making that request</li> </ul>"},{"location":"Docker%20Apps/ddns-update/#other-usage","title":"Other Usage","text":"<p><code>docker restart duckdns</code> will manually run IP update <code>docker exec -it duckdns /app/debug.sh</code> or other scripts, debug script will print out IP address of subdomains resolved by Cloudflare</p>"},{"location":"Docker%20Apps/epic-games-free-games/","title":"Epic Games Free Games","text":"<p>Buy Free Games from Epic Games</p> <p>https://hub.docker.com/r/charlocharlie/epicgames-freegames</p> <p>Config</p> <p>NEED TO CHANGE</p> <p>Email: email address</p> <p>Password: password</p> <p>Webhook URL: make a discord channel and click settings. Go to integrations, then webhook, copy webhook URL.</p> <p>mentioned Users: right click your profile, and click Copy ID</p> <p>TOTP</p> <ol> <li>Go here to login. https://www.epicgames.com/account/password Login with Epic Games account.</li> <li>Click \u201cenable authenticator app.\u201d</li> <li>In the section labeled \u201cmanual entry key,\u201d copy the key.</li> <li>Use your authenticator app to add scan the QR code.</li> <li>Activate 2FA by completing the form and clicking activate.</li> <li>Once 2FA is enabled, use the key you copied as the value for the TOTP parameter.</li> </ol> <p>Docker</p> <pre><code>docker run -d -v /home/karis/docker/epicgames:/usr/app/config:rw -p 3000:3000 -m 2g --name epicgames --restart unless-stopped charlocharlie/epicgames-freegames:latest\n</code></pre> <p>Change the name of the container to a friendly name. Restart unless stopped so it restart automatically.</p> <p>Copy and Paste</p> <p>The default json configuration is located at /home/karis/docker/epicgames or $HOME/docker/epicgames.</p> <p>Fix Login Issue Using Cookies</p> <p>https://store.epicgames.com/en-US/</p> <ol> <li>Visit this site and make sure it\u2019s logged in.</li> <li>Install this extension EditThisCookie https://chrome.google.com/webstore/detail/editthiscookie/fngmhnnpilhplaeedifhccceomclgfbg/related</li> <li>Open the extension and change the url to epicgames.com/id as in screenshot below</li> <li>Export the cookie</li> </ol> <p></p> <ol> <li>Go to $HOME/docker/epicgames and create a new file email@gmail.com-cookies.json</li> <li>If the json file is already there, truncate it with \u2013size 0</li> <li>Paste the cookie value to the json file</li> <li>Restart container.</li> </ol> <p>Update</p> <pre><code>docker pull charlocharlie/epicgames-freegames:latest\ndocker rm -f epicgames\ndocker images | grep epicgames\n# use docker rmi to remote the corresponding image \n# re run the epicgames docker run command\n</code></pre>"},{"location":"Docker%20Apps/filebrowser/","title":"Filebrowser","text":"<p>Filebrowser app on a web browser, port 4455. o</p> <p>Docker-compose deployment</p> <pre><code>version: '3.9'\nservices:\n    filebrowser:\n        container_name: filebrowser\n        image: filebrowser/filebrowser\n        ports:\n\n            - '4455:80'\n        user: 1000:1001\n        restart: unless-stopped\n        volumes:\n            - '~/docker/filebrowser/.filebrowser.json:/.filebrowser.json'\n            - '~/docker/filebrowser/filebrowser.db:/filebrowser.db'\n            - '~/docker/filebrowser/config:/config' \n            - '~/docker/filebrowser/branding:/branding'\n            - '~/docker:/srv/docker'\n            - '/mnt/data:/srv/data'\n            - '/mnt/nvme/share:/srv/nvme-share'\n</code></pre> <p>The first 3 bind mount are for configuration of filebrowser, eg. config, database and branding files. On first deployment, need to create an empty <code>database.db</code> file. The remaining bind mount are for the folders that need to be accessed, the folders should be bound under <code>/srv</code>. Filebrowser by default create a volume under <code>/srv</code>, in this setup where folders are bind mount to subfolders in <code>/srv</code> and nothing bind mount directly, it could create a specific volume under docker just for <code>/srv</code> which is unavoidable.</p> <p>Additionally, a <code>config</code> folder is mounted, this is a write-accessible folder for both host and filebrowser. Since the CLI app does not work when filebrowser is running, a new database needs to be created but the filebrowser root folder is not write accessible, the config folder allow the database folder to be copied there and changes will be made into that instead. Example of CLI app. <pre><code>./filebrowser -d /config/filebrowser.db commands_here\n</code></pre></p> <p>This is the content of <code>.filebrowser.json</code></p> <pre><code>{\n    \"port\": 80,\n    \"baseURL\": \"\",\n    \"address\": \"\",\n    \"log\": \"stdout\",\n    \"database\": \"/filebrowser.db\",\n    \"root\": \"/srv\",\n    \"baseURL\": \"/baseurl\",\n  }\n</code></pre> <p><pre><code>\u00a0 \u00a0 \u00a0 \u00a0 healthcheck:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 test: [\"CMD\", \"/healthcheck.sh\", \"||\", \"exit\", \"1\"]\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 interval: 1h\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timeout: 10s\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 retries: 2\n</code></pre> This change makes the healthcheck less aggressive as by default the healthcheck occurs every 5 seconds.</p>"},{"location":"Docker%20Apps/filebrowser/#share","title":"Share","text":"<p>The user and share management in filebrowser is simple. The shares have a expiring time, and can optionally have a password. The recipient can view and download files in the share but cannot upload.</p>"},{"location":"Docker%20Apps/filebrowser/#user-management","title":"User Management","text":"<p>To create a new user, it\u2019s under settings <code>User Management</code>, and add a user and password accordingly, and give appropriate permission. The scope is where the root folder where the user have access to, since the docker data folder is bound at <code>/srv/docker</code> and /srv is defined as root folder in config, the folder name to put in scopes would be <code>/docker</code>. Only one scope is allowed.</p> <p></p> <p>It is also possible to add rules to prevent user access of files within a scope. Under rules, enter the path that is relative to the scope, for example <code>/docker/minecraft/config</code> would be <code>/config</code>, the default behavior for the folder added is block, unless allow is checked.</p> <p></p> <p>It is also possible to to add users via CLI <pre><code>./filebrowser -d /config/filebrowser.db users export config/users.json\n</code></pre> This will create a json file, create a copy of a user that needs to be duplicated, remove all existing users (because duplicate users cannot be imported), make sure the new user has a unique ID. <pre><code>./filebrowser -d /config/filebrowser.db users import config/users.json\n</code></pre> After importing, copy the database from <code>/config</code> to the root folder.</p>"},{"location":"Docker%20Apps/filebrowser/#personalization","title":"Personalization","text":"<p>Enable dark theme - Setting - Global Settings - Branding</p> <ul> <li>also change the branding directory path to /branding which is bind mount in docker</li> </ul> <p>Under the branding folder, create a file <code>custom.css</code>which is used for css customization. Then create a folder img and place logo.svg in it for custom icon. The icon is the same as egow entertainment and stored in OliveTin icon PSD file. Under the folder img, create a folder icons and use favicon generator site to create an icon archive and put all the content of that archive in the icons folder, the result should look like this.</p> <p></p>"},{"location":"Docker%20Apps/filebrowser/#proxysso","title":"Proxy/SSO","text":"<p>Reverse proxy is normal procedure using NPM. To add bookmark to a file location, use browser/homepages bookmark function.</p>"},{"location":"Docker%20Apps/filebrowser/#authelia","title":"Authelia","text":"<p>To enable Authelia, it is not possible to do so via configuration file, need to use the CLI app. First in file browser, create the appropriate users. The users must match the name that is created in Authelia, give admin if necessary. The password doesn\u2019t matter when using Authelia. Then use the command to change the database to allow external authentication. <pre><code>docker exec -it filebrowser /filebrowser -d config/filebrowser.db config set --auth.method=proxy --auth.header=Remote-User\n</code></pre></p> <ul> <li>because of the database situation, copy the temporary database to the main one.</li> </ul> <p>Authelia requires additional whitelisting rules for filebrowser to work properly. The rules works for both subdomain or subfolder. <pre><code>\u00a0 \u00a0 - domain:\n\u00a0 \u00a0 \u00a0 \u00a0 - 'files.{{ env \"DOMAIN_NAME\" }}'\n\u00a0 \u00a0 \u00a0 resources:\n\n        - \"^.*/api/public/.*\"\n        - \"^.*/share/*\"\n        - \"^.*/static/(js|css|img|themes|fonts|assets)/*\"\n\u00a0 \u00a0 \u00a0 policy: bypass\n</code></pre> - the bypass rule must be placed before the one/two factor rule for it to take effect</p>"},{"location":"Docker%20Apps/filebrowser/#subfolder","title":"Subfolder","text":"<p>The subfolder can be done via <code>.filebrowser.json</code> and adding <code>baseurl</code>, make sure to add <code>/</code>. To add the subfolder location, go to Nginx Proxy Manager and advanced config. <pre><code>location /baseurl/ {\n  include /snippets/proxy.conf;\n  include /snippets/authelia-authrequest.conf;\n  proxy_pass http://10.10.120.16:4455/;\n}\n</code></pre></p>"},{"location":"Docker%20Apps/filebrowser/#sso-behavior","title":"SSO Behavior","text":"In FB Not in FB User exists in Authelia Successful login to FB as the correct user Double login and FB local password doesn\u2019t work User not exist/not permitted in Authelia Forbidden Forbidden For users not in Authelia but exist in FB, they can only login locally (bypass reverse proxy). User cannot logout if authenticated via Authelia, they cannot access filebrowser at all."},{"location":"Docker%20Apps/fireshare/","title":"Fireshare","text":"Docker Apps Rating U/GID TZ SSO/Users Existing FS Portable Subfolder Mobile \u274e \u2705* \u274c\ud83e\udd35 \u2705 \u2705 \u274c \u2714"},{"location":"Docker%20Apps/fireshare/#configuration","title":"Configuration","text":"<pre><code>services:\n\u00a0 fireshare:\n\u00a0 \u00a0 image: shaneisrael/fireshare:develop\n\u00a0 \u00a0 container_name: fireshare\n\u00a0 \u00a0 environment:\n\u00a0 \u00a0 \u00a0 - MINUTES_BETWEEN_VIDEO_SCAN=30\n\u00a0 \u00a0 \u00a0 - PUID=1000\n\u00a0 \u00a0 \u00a0 - PGID=1001\n\u00a0 \u00a0 env_file:\n\u00a0 \u00a0 \u00a0 - .env # admin password\n\u00a0 \u00a0 volumes:\n\u00a0 \u00a0 \u00a0 - ~/docker/fireshare/data:/data:rw\n\u00a0 \u00a0 \u00a0 - ~/docker/fireshare/processed:/processed:rw\n\u00a0 \u00a0 \u00a0 - /mnt/nvme/share/gaming:/videos:rw\n\u00a0 \u00a0 networks:\n\u00a0 \u00a0 \u00a0 public:\n\u00a0 \u00a0 ports:\n\u00a0 \u00a0 \u00a0 - 8080:80\n\u00a0 \u00a0 restart: unless-stopped\n\nnetworks:\n\u00a0 public:\n\u00a0 \u00a0 name: public\n\u00a0 \u00a0 external: true\n</code></pre>"},{"location":"Docker%20Apps/fireshare/#environments","title":"Environments","text":"<p>Content of <code>.env</code> <pre><code>ADMIN_PASSWORD=\nDOMAIN=\n</code></pre> Setup user and group ID accordingly; more environment options are available https://github.com/ShaneIsrael/fireshare/wiki/Fireshare-Configurables</p>"},{"location":"Docker%20Apps/fireshare/#other","title":"Other","text":"<p>The software can also be configured via <code>config.json</code> located in <code>/data/config.json</code>. It\u2019s configuration is same as the WebUI.  <code>Default Video Privacy</code>: <code>false</code> set all the videos public viewable without sharing manually <code>Default Video Privacy</code>: <code>false</code> public user cannot upload videos <code>Sharable Link Domain</code>: link which fireshare append to when sharing files <code>Upload Folder</code>: folder that will be created in <code>/videos</code> directory when file is uploaded</p>"},{"location":"Docker%20Apps/fireshare/#usage","title":"Usage","text":"<p> By default, can view videos, admin and share links and the link will show preview and viewable in Discord. Admin can also upload directly in web interface. All the uploaded files are located in <code>/videos/uploads</code></p> <ul> <li>when uploading files through filesystem with a changed date via <code>touch</code> the changed date will also be reflected in the app</li> </ul>"},{"location":"Docker%20Apps/fireshare/#workflow","title":"Workflow","text":"<p>https://github.com/vttc08/fireshare-import Refer to this Github repo to setup. For personal documentation</p> <ul> <li>setup the project directory into <code>~/Documents/Projects</code></li> </ul>"},{"location":"Docker%20Apps/free-games-claimer/","title":"Free Games Claimer","text":"<p>https://github.com/vogler/free-games-claimer</p> <p>This is the Github repo for the new and advanced free games claimer. This is implemented after Epicgames FreeGames keeps failing.</p>"},{"location":"Docker%20Apps/free-games-claimer/#configuration","title":"Configuration","text":"<p>Using Docker-Compose</p> <p>In the folder structure</p> <pre><code>server: ~/docker/fgc$\n./data\ndocker-compose.yml\nfgc.env\n</code></pre> <p>fgc.env is the environment file for all the password/keys to login to different game services, fill it in manually or use a backup.</p> <pre><code>EG_OTPKEY=\nEG_EMAIL=\nEG_PASSWORD=\nNOTIFY=discord://123456/ABCD\nPG_EMAIL=\nPG_PASSWORD=\nGOG_EMAIL=\nGOG_PASSWORD=\nTIMEOUT=300\n</code></pre> <p><code>NOTIFY=discord://123456/ABCD</code> if the webhook looks like this <code>https://discord.com/api/webhooks/123456/ABCD</code></p> <p><code>TIMEOUT=300</code> sets the timeout to 300s before the container skip and error out due to EpicGames captcha problems. However, the impact on prime gaming and GOG are not tested.</p> <p>docker-compose.yml</p> <pre><code>services:\n  free-games-claimer:\n    container_name: FGC # is printed in front of every output line\n    image: ghcr.io/vogler/free-games-claimer # otherwise image name will be free-games-claimer-free-games-claimer\n    build: .\n    ports:\n\n      - \"5990:5900\" # VNC server\n      - \"5890:6080\" # noVNC (browser-based VNC client)\n    volumes:\n      - ~/docker/fgc:/fgc/data\n      - ~/docker/fgc/epic-games.js:/fgc/epic-games.js\n      - ~/docker/fgc/prime-gaming.js:/fgc/prime-gaming.js\n      - ~/docker/fgc/gog.js:/fgc/gog.js\n\u00a0 \u00a0 command: bash -c \"node gog; node epic-games; echo sleeping; chown -R 1000:1001 /fgc/data; sleep 1d\"\n    env_file:\n      - fgc.env\n    restart: unless-stopped\n</code></pre> <p>This docker-compose file use the environment file <code>fgc.env</code> as indicated above and runs once every day. It also contains VNC server/web based client.</p>"},{"location":"Docker%20Apps/free-games-claimer/#missing-captcha-session","title":"Missing Captcha Session","text":"<p>This should no longer be needed. Edit the line to epicgames.js code and replace with the following message. When the captcha is missed, it will send a notification for manual claiming.</p> <pre><code>await notify(`epic-games: got captcha challenge right before claim. Use VNC to solve it manually. Game link: \\n ${url}`)\n</code></pre> <p>EpicGames require a captcha to claim free games. If the 5 minute timeout window for EpicGames is missed, it is no longer possible to claim the games unless waiting for the next day, which due to the nature of discord notifications, there is a slim to none chance of catching the captcha at next day. To continuing claiming after acknowledging the missed session, use portainer, ConnectBot Android to temporarily restart the container to restore VNC session.</p> <p>In order to restore the default time of claiming the games. Eg. waking up on Thurs or Fri and a predictable time and claim games, use the linux at command. Need to install <code>at</code> using <code>apt</code>. <pre><code>at 9:20\n&gt; docker restart FGC\n&gt; &lt;EOT&gt;\n</code></pre></p> <p>This will run the command at 9:20 AM the next day. Ctrl-D to exit at prompt and verify the time is correct.</p>"},{"location":"Docker%20Apps/free-games-claimer/#login-session-problem","title":"Login Session Problem","text":"<p>After not logging in the VNC browser for a long time, it cannot login again, but without VNC in local environment it will work. Need to login locally and copy the files to the Docker server.</p> <ul> <li>download Node and clone repo, configure everything as in remote server</li> <li>run <code>npm install</code> in the cloned repo and run <code>node epic-games.js</code></li> <li>login normally</li> </ul> <p>The app will create <code>./data</code> folder in the current directory and a <code>./browser</code> folder inside it. Copy the files into the remote server\u2019s same location via WinSCP. Permission changes may be required. <pre><code>sudo chown -R $PUID:$PGID ./data\nrm ./data/browser/compatibility.ini\n</code></pre> If Firefox error occurs, need to delete the <code>compatibility.ini</code> file and the Firefox session should resume normally again.</p>"},{"location":"Docker%20Apps/ghost-docker/","title":"Ghost CMS Docker","text":"<p>Ghost is a website/blog management system with a database. This page is specific to installation and management of Ghost Docker container.</p>"},{"location":"Docker%20Apps/ghost-docker/#installation","title":"Installation","text":""},{"location":"Docker%20Apps/ghost-docker/#basic-configuration","title":"Basic Configuration","text":""},{"location":"Docker%20Apps/ghost-docker/#ghost-configuration","title":"Ghost Configuration","text":"<p>Will be documented in ghost-cms in depth.</p>"},{"location":"Docker%20Apps/invidious/","title":"Invidious","text":""},{"location":"Docker%20Apps/jlesage-vnc-apps/","title":"jlesage VNC Apps","text":"<p>VNC apps consists of desktop applications that have the GUI in a web browser, mostly from the creator jlesage. All such VNC apps follow a setup standard with exception to MegaBasterd, refer to Exception.</p>"},{"location":"Docker%20Apps/jlesage-vnc-apps/#environments","title":"Environments","text":"<p>At least for apps from jlesage, it supports an environment variable. Create an environment file called <code>vnc.env</code></p> <p>The environment file can be reference in many docker images from jlesage using docker-compose. The current environment variable specify U/GID, time zone and make every app dark mode. It is also possible to set VNC passwords. This is the full list of environment variables. For supported apps such as avidemux, there is an option <code>WEB_AUDIO=1</code> which allow audio to work.</p> <pre><code>USER_ID=1000\nGROUP_ID=1001\n\n\nKEEP_APP_RUNNING=1\n</code></pre> <p>The jlesage apps have 2 ports, port 5800 for viewing the VNC app on a web browser on desktop; port 5900 is for VNC protocol that can be used in dedicated VNC viewer or mobile viewing.</p>"},{"location":"Docker%20Apps/jlesage-vnc-apps/#general-bind-mounts","title":"General Bind Mounts","text":"<p>The appdata bind mount is located in the <code>~/docker/vnc</code>, as seen from the yml example, the vnc environment file <code>vnc.env</code> is placed in the appdata folder. For application requiring access to movie storage, the bind mount is on the corresponding hard drive or pool. As for applications requiring access to storage but not large media, it\u2019s best to put the files on a SSD.</p> <p>This is an example of VNC container of MKVToolNix. The <code>vnc.yml</code> file is backed up elsewhere.</p> <pre><code>    mkvtoolnix:\n        image: jlesage/mkvtoolnix\n        env_file:\n\n            - ./vnc/vnc.env\n        volumes:\n            - '/mnt/data/nzbget:/storage:rw'\n            - '~/docker/vnc/mkvtoolnix:/config:rw'\n        ports:\n            - '5820:5800'\n            - '5920:5900'\n        container_name: mkvtoolnix\n</code></pre>"},{"location":"Docker%20Apps/jlesage-vnc-apps/#ports","title":"Ports","text":"<p>The application port start from 5800/5900 for its corresponding access and add 10 for each application.</p> <ul> <li>for apps with high idle CPU or RAM, it\u2019s best to run the app on-demand and close it when not used</li> </ul> App Port Dialog Idle CPU RAM Additional Config JDownloader 5800 jdownloader Firefox 5810 MKVToolNix 5820 gtk MKVCleaver 5840 QT High MegaBasterd 5860 Github MCASelector 5870 High High Github Avidemux 5880 QT Med Med <code>WEB_AUDIO=1</code>"},{"location":"Docker%20Apps/jlesage-vnc-apps/#files","title":"Files","text":"<p><code>/config</code> is the directory which app configuration are stored and should have the correct permission, there are other additional bind mounts for <code>/storage</code> which is the default file choose location for some containers.</p> <ul> <li>any directory from host can be bind mount into anything in container; however if a directory is not created on host and the container has to create it, it\u2019s possible it will be owned by <code>root</code></li> </ul> <p>QT Based Apps that use QT based file explorer (eg. Avidemux) has the configuration stored in <code>${APP_CONFIG}/xdg/config/QtProject.ini</code>, this is used to setup file explorer shortcuts. <pre><code>[FileDialog]\nshortcuts=file:, file:///config, file:///storage, file:///mnt/data/nzbget, file:///mnt/data, file:///mnt/data2\n</code></pre></p> <p>GTK Based Apps that use GTK based file explorer (eg. MCASelector) has the configuration stored in <code>${APP_CONFIG}/xdg/config/gtk-3.0/bookmarks</code>, this is used to setup file explorer shortcuts. <pre><code>file:///world, file:///storage\n</code></pre></p> <p>There are also some application specific setup. For applications accessing hard drive or intensive apps, it is best to stop when not used. Lazytainer and ContainerNursery and possibly using DNS server can help automate this process.</p>"},{"location":"Docker%20Apps/jlesage-vnc-apps/#exception","title":"Exception","text":"<p>MegaBasterd MegaBasterd is now setup alongside gluetun as a part of WARPStack.</p>"},{"location":"Docker%20Apps/tesla-homepage/","title":"Tesla Homepage","text":"<p>This is a homepage that allows Tesla browser to enter full screen mode.</p> <p>Docker-compose</p> <pre><code>services:\n  homepage-for-tesla:\n    image: jessewebdotcom/homepage-for-tesla:latest\n    container_name: homepage-for-tesla\n    environment:\n\n      - DEFAULT_THEME=13\n    volumes:\n      - ~/docker/tesla/public/bookmarks.json:/app/public/bookmarks.json\n      - ~/docker/tesla/public/images:/app/public/images\n    ports:\n      - \"3000:3000\"\n</code></pre>"},{"location":"Docker%20Apps/uptime-kuma/","title":"Uptime Kuma","text":"Docker Apps Rating U/GID TZ SSO/Users Existing FS Portable Subfolder Mobile \u274e \u2705* \u274c\ud83e\udd35 \u2705 \u2705 \u274c \u2714"},{"location":"Docker%20Apps/uptime-kuma/#install","title":"Install","text":"<p>Docker Compose <pre><code>services:\n  uptime-kuma:\n    container_name: uptime-kuma\n    image: louislam/uptime-kuma\n    ports:\n\n      - 3001:3001\n    environment:\n      - PUID=1001\n      - PGID=1001\n    volumes:\n      - ~/docker/uptime-kuma:/app/data\n    restart: unless-stopped\n</code></pre></p> <ul> <li>Container support non-root users  via <code>PUID/PGID</code></li> <li>default port 3001</li> </ul>"},{"location":"Docker%20Apps/uptime-kuma/#monitoring","title":"Monitoring","text":"<p>To add a monitor, follow the GUI</p> <ul> <li>Friendly name is what is displayed on the dashboard</li> <li>There is an option to define how often to check, recheck and how many times to recheck</li> <li>Setup [notification]</li> </ul>"},{"location":"Docker%20Apps/uptime-kuma/#http","title":"HTTP","text":"<p> For HTTP monitoring, it will monitor a HTTP site and give out metrics as such up/down, and the response time. - accepted response code: eg. 200-299 anything that is not accepted will be considered as down - option to check HTTPS certificate expiration</p>"},{"location":"Docker%20Apps/uptime-kuma/#docker","title":"Docker","text":"Docker Health <p>Uptime Kuma does not notify if a Docker container is unhealthy, it will show as pending. No notification will be sent. Github Issue</p> <p>Go to <code>Settings</code> -&gt; <code>Docker Hosts</code> to create a Docker host. Under <code>Add a new monitor</code>, select <code>Docker container</code> and choose the corresponding Docker host</p>"},{"location":"Docker%20Apps/uptime-kuma/#remote-hosts","title":"Remote Hosts","text":"<p>By default, it requires mounted Docker sockets. It also supported socket over tcp or a socket proxy. For remote hosts it\u2019s best to use tailscale and expose the appropriate docker socket to tailscale only.</p>"},{"location":"Docker%20Apps/uptime-kuma/#notification","title":"Notification","text":"<p>Configured under <code>Settings</code> -&gt; <code>Notifications</code></p> <ul> <li>it\u2019s possible to apply a newly added notification to all existing monitors</li> <li>when it\u2019s set as default, all new monitors will have this notification</li> </ul>"},{"location":"Docker%20Apps/uptime-kuma/#tags","title":"Tags","text":"<p>Tags can be added in <code>Settings</code> -&gt; <code>Tags</code>, it can be applied to monitors. In the main page, tags can be filtered. - tags cannot be used as a filter for status page or maintenance </p>"},{"location":"Docker%20Apps/uptime-kuma/#status-page","title":"Status Page","text":"<p>Status page Maintenance SSO Monitor services behind Authelia Remote Docker Hosts Autokuma</p>"},{"location":"Docker%20Apps/webtop/","title":"Webtop (openbox-ubuntu)","text":"<pre><code>version: \"2.1\"\nservices:\n  webtop:\n    image: lscr.io/linuxserver/webtop:amd64-ubuntu-openbox\n    container_name: webtop-openbox\n    security_opt:\n\n      - seccomp:unconfined #optional\n    environment:\n      - PUID=1000\n      - PGID=1001\n      - TZ=America/Vancouver\n      - SUBFOLDER=/ # For reverse proxy\n      - TITLE=WebtopMate # The title as it shown in browser\n    volumes:\n      - ~/docker/webtop/config:/config # default home folder\n      - /mnt/data:/mnt/data\n      - /var/run/docker.sock:/var/run/docker.sock # Run docker inside docker\n    ports:\n      - 3050:3000\n    shm_size: \"1gb\" #optional\n    restart: unless-stopped\n</code></pre> <p>The default installation with config folder copied is not usable. Packages to be installed <pre><code>apt update\napt install wget terminator rsync ntp spacefm compton tint2 nitrogen nano lxappearance mousepad unrar unzip xarchiver mono-complete libhunspell-dev p7zip libmpv-dev tesseract-ocr vlc ffmpeg fonts-wqy-zenhei language-pack-zh-hans mediainfo mediainfo-gui p7zip\n</code></pre></p> <p>Packages that has to be installed manually <code>lxappearance, spacefm, tint2, nitrogen</code></p> <p>Desktop (tint2, nitrogen)</p> <ul> <li>nitrogen cannot keep <code>scaled</code> option after restarting and needs to change it manually</li> <li>nitrogen wallpaper are found in <code>/config/Pictures/wallpaper.jpg</code></li> </ul>"},{"location":"Docker%20Apps/webtop/#customization","title":"Customization","text":"<p>lxappearance</p> <ul> <li>theme: <code>Quixotic-blue</code>; location <code>.themes</code></li> <li>icon: <code>Desert-Dark-icons</code>; location <code>.icons</code> tint2</li> <li>tint2 with copied config, located in <code>.config/tint2</code></li> </ul>"},{"location":"Docker%20Apps/webtop/#firefox-browser","title":"Firefox Browser","text":"<p>policies.json <pre><code>// force install ublock, disable annoyances, add bookmarks\n{\n  \"policies\": {\n    \"ExtensionSettings\": {\n      \"uBlock0@raymondhill.net\": {\n        \"installation_mode\": \"force_installed\",\n        \"install_url\": \"https://addons.mozilla.org/firefox/downloads/latest/ublock-origin/latest.xpi\"\n      }\n    },\n    \"NoDefaultBookmarks\": true,\n    \"DisableTelemetry\": true,\n    \"Bookmarks\": [\n      {\n        \"Title\": \"zmk\",\n        \"URL\": \"https://zmk.pw\",\n        \"Placement\": \"toolbar\"\n      },\n      {\n        \"Title\": \"SubHD\",\n        \"URL\": \"https://subhd.tv\",\n        \"Placement\": \"toolbar\"\n      } // Add more bookmarks like this\n    ],\n    \"FirefoxHome\": {\n      \"Search\": true,\n      \"TopSites\": true,\n      \"SponsoredTopSites\": false,\n      \"Pocket\": false,\n      \"SponsoredPocket\": false,\n      \"Locked\": false\n    }\n  }\n}\n</code></pre></p> <ul> <li>it is not possible to backup bookmarks on the pinned menu via policies (only way is to restore from home folder)</li> <li>it\u2019s not possible to remove <code>import bookmarks</code> and <code>getting started</code> bookmarks with <code>policies.json</code> as documented here, it has to be removed manually Manual Configs</li> <li>ublock add Chinese filter</li> <li>pin bookmarks</li> <li>remove default bookmarks and getting started from toolbar</li> </ul>"},{"location":"Docker%20Apps/webtop/#files","title":"Files","text":"<p>SpaceFM</p> <ul> <li>upon installing, with config copied over, everything works fine</li> <li>configuration is stored in <code>~/.config/spacefm</code></li> </ul> <p>Movie-Renamer Script</p> <ul> <li>works after copying</li> </ul>"},{"location":"Docker%20Apps/webtop/#subtitles","title":"Subtitles","text":""},{"location":"Docker%20Apps/webtop/#subtitle-edit","title":"Subtitle Edit","text":"<p>Install dependencies Download subtitle-edit <pre><code>curl -s https://api.github.com/repos/SubtitleEdit/subtitleedit/releases/latest | grep -E \"browser_download_url.*SE[0-9]*\\.zip\" | cut -d : -f 2,3 | tr -d \\\" | wget -qi - -O SE.zip\nunzip SE.zip -d /config/subtitle-edit\n</code></pre> Subtitle-Edit Dark theme has to be changed manually</p> <ul> <li><code>Options</code> -&gt; <code>Settings</code> -&gt; <code>Appearance</code> -&gt; <code>Use Dark Theme</code></li> <li><code>Options</code> -&gt; <code>Settings</code> -&gt; <code>Syntax Coloring</code> -&gt; <code>Error color</code> and change to <code>27111D</code></li> <li><code>Options</code> -&gt; <code>Settings</code> -&gt; <code>Appearance</code> -&gt; <code>UI Font</code> -&gt; <code>General</code> and change to <code>WenQuanYi Zen Hei</code></li> </ul>"},{"location":"Docker%20Apps/webtop/#obsidian-webtop","title":"Obsidian Webtop","text":"<p>Make sure to close everything in Obsidian to reduce CPU usage.</p> <pre><code>services:\n  obsidian:\n    image: lscr.io/linuxserver/obsidian:latest\n    container_name: obsidian\n    security_opt:\n\n      - seccomp:unconfined #optional\n    environment:\n      - PUID=1000\n      - PGID=1001\n      - TZ=America/Vancouver\n      - SUBFOLDER=/obsidian/\n      - TITLE=Obsidian\n    volumes:\n      - ~/docker/obsidian-webtop:/config\n      - ~/Documents/notes:/notes\n    networks:\n      - public\n    ports:\n      - 3010:3000\n    devices:\n      - /dev/dri:/dev/dri\n    shm_size: \"1gb\"\n    restart: unless-stopped\n\nnetworks:\n  public:\n    name: public\n    external: true\n</code></pre> <ul> <li>standard procedure for PUGID, TZ and docker networks</li> <li>optional environment variable <code>PASSWORD</code> for HTTP basic auth</li> <li><code>SUBFOLDER</code> can be used for reverse proxy with custom location</li> </ul>"},{"location":"Docker%20Apps/webtop/#authentication","title":"Authentication","text":"<p>Setup of webtop with Authelia require more configurations. Needs to manually configure the custom location in Nginx Proxy Manager just like bluemap. <pre><code>location /obsidian/ {\n  proxy_set_header Upgrade $http_upgrade;\n  proxy_set_header Connection $http_connection;\n  include /snippets/proxy.conf;\n  include /snippets/authelia-authrequest.conf;\n  proxy_pass http://10.10.120.12:3010/obsidian/;\n}\n</code></pre></p> <ul> <li>the <code>proxy_set_header</code> lines are required because of websocket</li> </ul> <p>Authelia configuration (need whitelist VNC assets) <pre><code>    - domain: \"basedomainforsubfolder.mywire.org\"\n      resources:\n        - \"socket.io/*\"\n        - \"public/*\"\n        - \"vnc/*\"\n      policy: bypass\n    - domain: \"basedomainforsubfolder.mywire.org\"\n      policy: one_factor\n</code></pre></p>"},{"location":"Docker%20Apps/Downloading/ariang/","title":"Aria-NG Downloader","text":"Docker Apps Rating U/GID TZ SSO/Users Existing FS Portable Subfolder Mobile \u2705 \u274c \u274e\ud83e\udd35 \u2705 \u2705 \u274c \u2714 <p>https://github.com/hurlenko/aria2-ariang-docker</p>"},{"location":"Docker%20Apps/Downloading/ariang/#installation","title":"Installation","text":"<pre><code>services:\n  ariang:\n    container_name: ariang\n    image: hurlenko/aria2-ariang\n    networks:\n\n      - public\n    # Port 8080 default\n    volumes:\n      - ./dl:/aria2/data\n      - ./config:/aria2/conf\n    # use .env file for secret\n    environment:\n      - PUID=1000\n      - PGID=1001\n      - ARIA2RPCPORT=443\n    restart: unless-stopped\n\nnetworks:\n  public:\n    external: true\n</code></pre>"},{"location":"Docker%20Apps/Downloading/ariang/#environments","title":"Environments","text":"<p><pre><code>RPC_SECRET=\nEMBED_RPC_SECRET=${RPC_SECRET}\n</code></pre> The content of <code>.env</code>file include password needed to connect to the jsonrpc aria. The embed RPC option also changes the frontend code to automatically fill in secret when it starts up. </p> <ul> <li><code>ARIA2RPCPORT</code> is set as 443 for reverse proxy</li> <li>if not using reverse proxy, require a port to map to 8080 which is by default what the container images exposes, also set <code>ARIA2RPCPORT</code> to the exposed port (if that doesn\u2019t work try 8080)</li> </ul> <p>The container also support <code>BASIC_AUTH_USERNAME/PASSWORD</code>, or use an external authentication provider such as authelia or reverse proxy.</p>"},{"location":"Docker%20Apps/Downloading/ariang/#usage","title":"Usage","text":"<p>The files are located in <code>./config</code>, it include <code>aria2.conf</code> for detailed aria configuration. Since ariang is just a frontend, the browser portion is not persistent. Any configuration done on the browser do not get saved to the filesystem. </p>"},{"location":"Docker%20Apps/Downloading/ariang/#downloading","title":"Downloading","text":"<p>Aria2 support HTTP, and FTP downloads. To download, click New and add a download. For FTP downloads, use the URL in this format. <pre><code>ftp://user:pass@url/path/to/file\n</code></pre></p>"},{"location":"Docker%20Apps/Downloading/ariang/#troubleshooting","title":"Troubleshooting","text":"<p>In firefox, clear data and cache or use a private window and errors will likely disappear. Especially after reconfiguring environments/passwords.</p>"},{"location":"Docker%20Apps/Downloading/ariang/#security","title":"Security","text":"<p>It is possible to put this behind Authelia and whitelist the jsonrpc for third party apps while protecting the web interface. Since the option <code>EMBED_RPC_SECRET</code> is used, it will automatically connect to aria2 once web interface load, password on the web interface is required.</p>"},{"location":"Docker%20Apps/Downloading/ariang/#caddy","title":"Caddy","text":"<p>The following in Caddy sets a basic authentication password for the main ariang web interface while allowing <code>/jsonrpc</code> endpoint for third-party apps. <pre><code>        @aria host aria.{$WEBSITE}\n        handle @aria {\n                @jsonrpc path /jsonrpc\n                handle @jsonrpc {\n                        reverse_proxy ariang:8080\n                }\n                handle {\n                        basicauth /* {\n                                admin $bcrypthashedpass\n                        }\n                        reverse_proxy ariang:8080\n                }\n        }\n</code></pre></p> <ul> <li>the Caddyfile uses <code>basicauth</code> and a password which needs bcrypt hashed</li> <li>the reverse proxy bypass <code>jsonrpc</code> and points to port 8080 in the container</li> </ul>"},{"location":"Docker%20Apps/Downloading/ariang/#tailscale","title":"Tailscale","text":"<p>For internal use without public access, tailscale can be used. For cloud servers using Docker, using the port mapping will override any firewall rules on the server. Binding the tailscale machine IP to the port is needed. <pre><code>tailscale ip --4 # 100.100.123.100\n</code></pre> <pre><code>    ports:\n      - 100.100.120.100:8080:8080\n</code></pre> When accessing via ddns or direct IP is not possible, only possible to access port 8080 with tailscale.</p>"},{"location":"Docker%20Apps/Downloading/rutorrent/","title":"RuTorrent","text":"<p><code>/watched</code> folder allow dropping torrents files and autodownload, the watched folder is located in the base <code>/download</code> folder</p>"},{"location":"Docker%20Apps/Downloading/speedtest/","title":"Speedtest","text":"<p>Multiple servers with perferably subdir support Password protection</p>"},{"location":"Docker%20Apps/Downloading/speedtest/#openspeedtest","title":"OpenSpeedTest","text":""},{"location":"Docker%20Apps/Downloading/speedtest/#librespeed","title":"LibreSpeed","text":""},{"location":"Docker%20Apps/Media%20Apps/audiobookshelf/","title":"Audiobookshelf","text":"<p>Audiobooks and podcasts. </p> UID/GID <p>With the newer version of ABS. The environment variables <code>AUDIOBOOKSHELF_UID</code> and <code>GID</code> are removed, the container now runs as root with no ways to change it; if using the <code>user</code> flag in docker, there would be permission error on startup.</p> <p>Docker-compose, place it in the media apps compose media.yml</p> <pre><code>version: \"3.7\"\nservices:\n  audiobookshelf:\n    image: ghcr.io/advplyr/audiobookshelf:latest\n    ports:\n\n      - 13378:80\n    volumes:\n      - /mnt/m/Audios/audiobooks:/audiobooks # hard drive mount\n      - /mnt/m/Audios/podcasts:/podcasts # hard drive mount\n      - $HOME/audiobookshelf/config:/config\n      - $HOME/audiobookshelf/metadata:/metadata\n    restart: unless-stopped\n\n\u00a0 audiobookshelf-permfix:\n\u00a0 \u00a0 container_name: abs-permfix\n\u00a0 \u00a0 image: ubuntu\n\u00a0 \u00a0 networks:\n\u00a0 \u00a0 \u00a0 - public\n\u00a0 \u00a0 command: bash -c \"chown -R $${PUID}:$${PGID} /mnt; echo sleeping; sleep $${TIME}\"\n\u00a0 \u00a0 volumes:\n\u00a0 \u00a0 \u00a0 - /mnt/data/Audios/audiobooks:/mnt/audiobooks # hard drive mount\n\u00a0 \u00a0 \u00a0 - /mnt/data/Audios/podcasts:/mnt/podcasts # hard drive mount\n\u00a0 \u00a0 \u00a0 - ~/docker/audiobookshelf/config:/mnt/config\n\u00a0 \u00a0 \u00a0 - ~/docker/audiobookshelf/metadata:/mnt/metadata\n\u00a0 \u00a0 environment:\n\u00a0 \u00a0 \u00a0 - PUID=1000\n\u00a0 \u00a0 \u00a0 - PGID=1001\n\u00a0 \u00a0 \u00a0 - TIME=1h\n\u00a0 \u00a0 restart: unless-stopped\n</code></pre> <ul> <li>The change made to the docker-compose include a <code>permfix</code> that automatically <code>chown</code> everything in audiobookshelf bind mounts<ul> <li>mount everything into <code>/mnt</code></li> <li>change the user and group ID accordingly</li> </ul> </li> </ul>"},{"location":"Docker%20Apps/Media%20Apps/audiobookshelf/#usage","title":"Usage","text":"<p>To add a library, go to settings, libraries and add the path as mounted in docker.</p> <p>Go to Users, change the root password and create a new user. Note, the user cannot scan library, only the root can do that.</p> <p></p>"},{"location":"Docker%20Apps/Media%20Apps/audiobookshelf/#adding-media","title":"Adding Media","text":"<p>Make sure the contents are in a separate folder. Follow naming like this. A cover image can also be created. The best bitrate should be under 128 kbps for smooth playback.</p> <pre><code>/audiobooks\n--- ./Author - Book\n---  --- ./Cover.jpg\n---  --- ./book - 001 or book - chapter 1\n---  --- ./book - 002\n---  --- ./book - 003\n</code></pre> <p>In the WebUI, make sure logged in as root. Go to settings, library and scan. It will scan the newly added media. Also useful for dealing with unplayable file errors.</p> <p>It is also possible to upload via the WebUI. When files are uploaded this way, it is also be placed in the audiobooks folder. However, it is not possible to add more files via the web upload once it\u2019s scanned.</p> <p>Additional Metadata <code>Cover.jpg</code> - cover image <code>desc.txt</code> - descriptions <code>*.opf</code> - XML library file that contains additional metadata such as title, author etc.. Vocabulary <code>abridged/unabridged</code> - shortened listening version <code>primary/supplementary ebooks</code> - primary ebooks are</p> <p>If the media does not match or not have an image, go click the edit icon, go to <code>Match</code>, the best result is usually <code>Audible.com</code>.</p> <p></p> <p>If the chapter does not match, chapters can be edited manually. Go to Chapter and Lookup.</p>"},{"location":"Docker%20Apps/Media%20Apps/audiobookshelf/#mobile-app","title":"Mobile App","text":"<p>https://play.google.com/store/apps/details?id=com.audiobookshelf.app</p> <p>Mobile app also has download functionality; however, by default the download location is inaccessible.  Go to <code>Local Media</code> create a New Folder for books or audiobooks and, the app will ask for location.</p> <p>The statistic of minutes listened is the actual minutes listened, not the minutes of audiobook progress listened (eg. playing at faster speed).</p>"},{"location":"Docker%20Apps/Media%20Apps/audiobookshelf/#backuprestore","title":"Backup/Restore","text":"<p>In the WebUI, go to <code>Settings</code> &gt; <code>Backups</code> and there will be option for backup/restore. Alternatively, copy the entire appdata folder to another computer.</p>"},{"location":"Docker%20Apps/Media%20Apps/audiobookshelf/#proxysso","title":"Proxy/SSO","text":""},{"location":"Docker%20Apps/Media%20Apps/audiobookshelf/#reverse-proxy","title":"Reverse Proxy","text":"<p>To reverse proxy properly in Nginx Proxy Manager, ensure <code>Websocket Support</code>.</p>"},{"location":"Docker%20Apps/Media%20Apps/audiobookshelf/#authelia-oidc","title":"Authelia (OIDC)","text":"<p>For use with Authelia OIDC, subdomain (not subfolder) is required. No additional reverse proxy configuration needed. In the Authelia OIDC clients configuration <pre><code>      - id: audiobookshelf\n        client_name: Audiobookshelf\n        client_secret: '$plaintext${{ env \"AUDIOBOOKSHELF_SECRET\" }}'\n        public: false\n        authorization_policy: 'one_factor'\n        redirect_uris:\n          - 'https://abs.{{ env \"DOMAIN_NAME\"}}/auth/openid/callback'\n          - 'https://abs.{{ env \"DOMAIN_NAME\"}}/auth/openid/mobile-redirect'\n        scopes:\n          - 'openid'\n          - 'profile'\n          - 'email'\n</code></pre></p> <p>For configuration in Audiobookshelf, fill in the Issuer URL and click <code>Auto-Populate</code>, most important information will be filled out. Fill in the generated client secret manually. These settings need changed </p> <ul> <li>Matching existing users by <code>Match by username</code> (alternatively add email for each ABS user)</li> <li><code>Auto Register</code> - new users if not exist in ABS will be created and linked Despite OIDC enabled, the internal login can still work, non OIDC users can login.</li> </ul>"},{"location":"Docker%20Apps/Media%20Apps/audiobookshelf/#scripting-windows","title":"Scripting (Windows)","text":"<p>ffmpeg detect audio silence (for splitting a large audio file into multiple chapters)</p> <pre><code>ffmpeg -i input.mp3 -af silencedetect=n=-50dB:d=1.5 -f null -\n</code></pre> <pre><code>ffmpeg -i input.mp3 -af silencedetect=n=-50dB:d=1.5 -f null -loglevel debug 2&gt;&amp;1 - | findstr \"silence_duration\" | find /c /v \"\"\n</code></pre> <p>This will find silence parts below -50dB and duration threshold of 1.5s.</p> <p>The second code (windows cmd only) for linux use grep -c, finds how many silence parts can be detected, this should correlate to number of chapters.</p> <p>Once the optimal duration is set, use split.py.</p> <p>ffmpeg that remove silence from audio</p> <pre><code>ffmpeg -i input.mp4 -af silenceremove=stop_periods=-1:stop_duration=4:stop_threshold=-50dB -b:a 96k output.mp3\n</code></pre> <ul> <li>stop_duration (threshold duration for removing silence part)</li> <li>stop_periods = -1 (search for the entire audio track)</li> </ul> <p>Use edge_reader.py to utilize Edge AI reader to read the audiobook if only the pdf book is provided.</p> <p>After reading, put all the recorded files and pdf in the project folder and run processing.py twice.</p>"},{"location":"Docker%20Apps/Media%20Apps/jellystat/","title":"Jellystat","text":"Docker Apps Rating U/GID TZ SSO/Users Portable Subfolder \u274e \u2705* \u274c\ud83e\udd35 \u2705 \u274c <p>https://github.com/CyferShepard/Jellystat</p>"},{"location":"Docker%20Apps/Media%20Apps/jellystat/#install","title":"Install","text":"<p>Docker Compose (minimum viable setup) <pre><code>services:\n\u00a0 jellystat-db:\n\u00a0 \u00a0 container_name: jellystat-db\n\u00a0 \u00a0 image: postgres:15\n\u00a0 \u00a0 user: 1000:1001\n\u00a0 \u00a0 env_file:\n\u00a0 \u00a0 \u00a0 - jellystat.env\n\u00a0 \u00a0 environment:\n\u00a0 \u00a0 \u00a0 POSTGRES_DB: 'jellystat'\n\u00a0 \u00a0 \u00a0 TZ: 'America/Vancouver'\n\u00a0 \u00a0 \u00a0 PGTZ: 'America/Vancouver'\n\u00a0 \u00a0 volumes:\n\u00a0 \u00a0 - ~/docker/jellystat/db:/var/lib/postgresql/data # Mounting the volume\n\u00a0 \u00a0 restart: unless-stopped\n\n\u00a0 jellystat:\n\u00a0 \u00a0 image: cyfershepard/jellystat:latest\n\u00a0 \u00a0 container_name: jellystat\n\u00a0 \u00a0 user: 1000:1001\n\u00a0 \u00a0 env_file:\n\u00a0 \u00a0 \u00a0 - jellystat.env\n\u00a0 \u00a0 environment:\n\u00a0 \u00a0 \u00a0 POSTGRES_IP: jellystat-db\n\u00a0 \u00a0 \u00a0 POSTGRES_PORT: 5432\n\u00a0 \u00a0 ports:\n\u00a0 \u00a0 \u00a0 - \"5050:3000\" #Server Port\n\u00a0 \u00a0 volumes:\n\u00a0 \u00a0 \u00a0 - ~/docker/jellystat/app:/app/backend/backup-data # Mounting the volume\n\u00a0 \u00a0 depends_on:\n\u00a0 \u00a0 \u00a0 - jellystat-db\n\u00a0 \u00a0 restart: unless-stopped\n</code></pre></p> <p>The content of <code>jellystat.env</code> <pre><code>POSTGRES_USER=jellystat\nPOSTGRES_PASSWORD=\nJWT_SECRET=\n</code></pre></p> <ul> <li>Use both <code>PGTZ</code> and <code>TZ</code> to set timezone logging</li> <li>The environment <code>POSTGRES_DB</code> may not work, the default database is <code>jfstat</code> The secret can be generated with <pre><code>openssl rand -base64 64 | tr -d '\\ n'\n</code></pre></li> </ul>"},{"location":"Docker%20Apps/Media%20Apps/jellystat/#usage","title":"Usage","text":"<p>Jellyfin API key is needed to configure it. The app will show login/configuration screen. No other configurations are nessecary.</p>"},{"location":"Docker%20Apps/Media%20Apps/jellystat/#backuprestore","title":"Backup/Restore","text":"<p>If using bind mount, simply copy the files in the bind mount and everything will work on the new machine without issues. No database dumps, other steps are necessary.</p> <ul> <li>ensure the username/password/secret in the environments are matching</li> </ul>"},{"location":"Docker%20Apps/Media%20Apps/jellystat/#reverse-proxysso","title":"Reverse Proxy/SSO","text":"<p>App do not have SSO support. The internal login cannot be disabled, github issue. App do not support subfolders, only subpath supported. No special requirements needed when using Nginx Proxy Manager. If the frontend is in the same network as proxy, simply <code>jellystat:3000</code> is enough.</p> <p></p>"},{"location":"Docker%20Apps/Media%20Apps/rich-media/","title":"Rich Media","text":"<p>Hello Everyone</p> <p>This is a demo consisting of medias.</p> <p></p> <p>Some Code</p> <pre><code>docker-compose up -d\n</code></pre> <pre><code>import os\nimport time\n\nprint(\"hello world\")\nif a=b:\n  print(a)\nelif b=c:\n  try:\n    print(c)\n  except:\n    print(c+a)\nelse:\n  print(\"what is the meaning of life\")\n</code></pre> <p>More sample media</p> <p></p> <p>Portainer is a software for managing docker containers.</p>"},{"location":"Docker%20Apps/Minecraft/bluemap/","title":"Bluemap","text":"Docker Apps Rating U/GID TZ SSO/Users Portable Subfolder Mobile n/a n/a \u274e\ud83e\udd35 n/a \u2705 \u2714 <p>https://bluemap.bluecolored.de/wiki/</p>"},{"location":"Docker%20Apps/Minecraft/bluemap/#installation","title":"Installation","text":"<p>Download bluemap and place it in minecraft plugin folder, Docker version also available.</p>"},{"location":"Docker%20Apps/Minecraft/bluemap/#configuration","title":"Configuration","text":"<p>Config files are located in <code>plugins/Bluemap</code> Change the line in <code>core.conf</code> so the app functions <pre><code>accept-download: true\n</code></pre></p> <ul> <li><code>data: \"bluemap\"</code> the data location is not in <code>plugins</code> base folder but relative to base folder of the minecraft docker container<ul> <li>the default is located in <code>&lt;docker_mc_folder&gt;/bluemap</code></li> </ul> </li> <li>Default port is 8100, change in <code>webserver.conf</code></li> </ul>"},{"location":"Docker%20Apps/Minecraft/bluemap/#resource-pack","title":"Resource pack","text":"<p>Add a <code>.zip</code> into <code>plugin/Bluemap/packs</code> The <code>.zip</code> folder should have on the files in its root folder</p> <ul> <li><code>.zip</code> -&gt; <code>resource_pack\\</code> -&gt; <code>[pack.mcmeta, assets ...]</code> not OK</li> <li><code>.zip</code> -&gt; <code>[pack.mcmeta, assets ...]</code> OK</li> </ul>"},{"location":"Docker%20Apps/Minecraft/bluemap/#markers","title":"Markers","text":"<p>To see the changes <code>docker attach mcserver</code> then execute <code>bluemap reload</code></p>"},{"location":"Docker%20Apps/Minecraft/bluemap/#marker-set","title":"Marker Set","text":"<p>https://bluemap.bluecolored.de/wiki/customization/Markers.html <pre><code>debug-set: {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 label: \"Debug Set\"\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 toggleable: true\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 default-hidden: false\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sorting: 1\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 markers: {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\n\u00a0 \u00a0 }\n</code></pre></p> <ul> <li>multiple sets can be added in this format </li> <li><code>label</code> the name that is will appear (the <code>debug-set</code> is just an identifier)</li> <li><code>sorting</code> the order which it will appear</li> </ul>"},{"location":"Docker%20Apps/Minecraft/bluemap/#html","title":"HTML","text":"<p>Marker that shows an HTML element, for example a text label. <pre><code>\u00a0marker-html: {\n\u00a0 \u00a0 \u00a0type: \"html\"\n\u00a0 \u00a0 \u00a0position: { x: -132, y: 72, z: -202 }\n\u00a0 \u00a0 \u00a0label: \"Karis\"\n\u00a0 \u00a0 \u00a0html: \"&lt;html code&gt;\"\n\u00a0 \u00a0 \u00a0anchor: { x: 0, y: 0 }\n\u00a0 \u00a0 \u00a0sorting: 0\n\u00a0 \u00a0 \u00a0listed: true\n\u00a0 \u00a0 \u00a0min-distance: 50\n\u00a0 \u00a0 \u00a0max-distance: 750\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\n</code></pre></p> <ul> <li><code>type</code> set to <code>html</code></li> </ul> <p>HTML Code <pre><code>&lt;div style='line-height: 1em; font-size: 1.2em; color: black; font-weight: bold; background-color: white; transform: translate(-50%, -50%);'&gt;Karis&lt;/div&gt;\n</code></pre> This HTML code have black text with white background, bolded  To have a multiline text, just copy the <code>&lt;div&gt;</code> part again</p>"},{"location":"Docker%20Apps/Minecraft/bluemap/#line","title":"Line","text":"<p>Marker is a 3D line that can be clicked to show <code>label</code> or <code>detail</code>, color can be customized. <pre><code>line-marker: {\n\u00a0 \u00a0 \u00a0 type: \"line\"\n\u00a0 \u00a0 \u00a0 position: { x: -42, y: 70, z: -340 }\n\u00a0 \u00a0 \u00a0 label: \"Text to Display\"\n\u00a0 \u00a0 \u00a0 line: [\n\u00a0 \u00a0 \u00a0 \u00a0 { x: -42, y: 70, z: -340 },\n\u00a0 \u00a0 \u00a0 \u00a0 { x: 37, y: 90, z: -325 },\n\u00a0 \u00a0 \u00a0 \u00a0 { x: 102, y: 115, z: -312 }\n\u00a0 \u00a0 \u00a0 ]\n\u00a0 \u00a0 \u00a0 line-color: {r: 255, g: 0, b: 0, a: 1}\n\u00a0 \u00a0 \u00a0 line-width: 3\n\u00a0 \u00a0 \u00a0 detail: \"HTML code\"\n\u00a0 \u00a0 \u00a0 max-distance: 1500\n\u00a0 \u00a0 }\n</code></pre> </p> <ul> <li><code>position</code> - the starting position</li> <li><code>line</code> - array of xyz coordinates (can include starting position)</li> <li><code>line-color</code> - RGBA value</li> <li><code>label</code> and <code>detail</code> will both display the name of the line marker<ul> <li>setting anything in detail will override label It good idea to set the y above the value that is appears on map, if a line is covered by a block, that part of the line will not show.</li> </ul> </li> </ul>"},{"location":"Docker%20Apps/Minecraft/bluemap/#poi","title":"POI","text":"<p>Marker that can be clicked and shows the <code>label</code> text, with option to add custom icons. <pre><code>\u00a0 \u00a0 \u00a0 \u00a0 poi-marker-1: {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: \"poi\"\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 position: { x: 273, y: 62, z: 640 }\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 label: \"Village Marker 1\"\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 icon: \"assets/poi.svg\"\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max-distance: 400\n\u00a0 \u00a0 \u00a0 \u00a0 } \n</code></pre> </p> <p><code>icon</code> - can be any HTML image type</p> <ul> <li>the default icon size is <code>50px</code> as shown in preview</li> <li>icons must be stored in <code>/blue/web/assets</code> to be used </li> <li><code>svg</code> vector type is preferred over <code>png</code> due to small size constraint<ul> <li><code>svg</code> created in illustrator need <code>width=\"50px\" height=\"50px\"</code> for it to work properly</li> </ul> </li> </ul> Weird behavior with dark mode/different browsers <p>On Brave browser mobile dark mode, icons do not show. On Chrome Windows, while markers works, the text style such as <code>bold</code> do not work</p>"},{"location":"Docker%20Apps/Minecraft/bluemap/#shape","title":"Shape","text":"<p>Flat, 2D only box that covers an area. <pre><code>\u00a0 \u00a0 \u00a0 \u00a0 terrain-park: {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: \"shape\"\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 label: \"Example Shape Marker\"\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 position: { x: 186, z: -321 }\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 shape: [\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { x: 186, z: -321 }\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { x: 184, z: -374 }\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { x: 168, z: -368 }\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { x: 169, z: -316 }\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { x: 186, z: -308 }\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 line-width: 2\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 line-color: { r: 255, g: 0, b: 0, a: 1.0 }\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fill-color: { r: 200, g: 0, b: 0, a: 0.3 }\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 shape-y: 86\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max-distance: 1400\n\u00a0 \u00a0 \u00a0 \u00a0 }\n</code></pre></p> <ul> <li><code>shape</code>, only the x and z values are needed, no height</li> <li><code>shape-y</code> the height which the shape appears<ul> <li>if there are blocks above the plane of <code>shape-y</code>: part of that shape will be covered</li> <li>if there are no blocks below the plane of <code>shape-y</code>: the shape will appear floating (refer the image above)</li> </ul> </li> <li><code>color</code>, has a line and fill component, a fill with <code>a:</code> less than 1 decrease the opacity</li> </ul>"},{"location":"Docker%20Apps/Minecraft/bluemap/#render-distance","title":"Render Distance","text":"<ul> <li>for flat view, any view distance below 400 would not show</li> <li>as the view distance increase, the icon/html/line will gradually fade out</li> </ul>"},{"location":"Docker%20Apps/Minecraft/bluemap/#reverse-proxysso","title":"Reverse Proxy/SSO","text":"<p>The reverse proxy and authentication setup for subdomain is as usual in Nginx Proxy Manager. App has no built-in authentication so Authelia SSO is supported.</p>"},{"location":"Docker%20Apps/Minecraft/bluemap/#subpath-with-sso","title":"Subpath with SSO","text":"Nginx Proxy ManagerCaddy <p>The custom locations tab do not work, need to add it manually. Go to <code>Advanced</code> and edit these in the custom Nginx configuration. <pre><code>location /map/ {\n    include /snippets/proxy.conf;\n    include /snippets/authelia-authrequest.conf;\n    proxy_pass http://10.10.120.16:8100/;\n  }\n</code></pre></p> <ul> <li>Not tested yet </li> </ul>"},{"location":"Docker%20Apps/Minecraft/bluemap/#internal-use-only","title":"Internal Use Only","text":"<p>For public viewer, these parts are not relevant for setup. This is for setup of my specific server and guidelines.</p> <p>Ski Slopes Red - default color Black - default color Green -  <code>line-color: {r: 40, g: 255, b: 40, a: 1}</code> Blue - <code>line-color: {r: 0, g: 100, b: 200, a: 1}</code></p> <p>Roads Roads- <code>line-color: {r: 240, g: 220, b: 150, a: 1}</code></p>"},{"location":"Docker%20Apps/Minecraft/minecraft-prep-and-install/","title":"Minecraft Prep and Install","text":""},{"location":"Docker%20Apps/Minecraft/minecraft-prep-and-install/#client-setup-java-online","title":"Client Setup (Java + Online)","text":"<ol> <li>Download Java</li> <li>Download OptiFine the latest version.</li> <li>On the official Minecraft client, go add a new installation and match the version with OptiFine.</li> <li>Download and try the official version, then install OptiFine with Java.</li> <li>Under Settings -&gt; Keep the Launcher open while games are running</li> </ol>"},{"location":"Docker%20Apps/Minecraft/minecraft-prep-and-install/#client-setup-java-offline","title":"Client Setup (Java + Offline)","text":"<ol> <li>Use the client PolyMC to enable offline play.</li> <li>Go to the right corner, manage accounts and create an offline account.</li> <li>Click on add an instance and follow the guide.</li> <li>To install OptiFine, need the official launcher first, then download OptiFine</li> <li>Extract OptiFine, the extracted file should be ending in _MOD.jar</li> <li>Open the jar file in WinRAR, then move the files from notch folder into the base folder. Save the jar archive.</li> <li>Go to PolyMC, right click on the instance, click Edit -&gt; Versions -&gt; Add to minecraft.jar and select the modified OptiFine.</li> </ol>"},{"location":"Docker%20Apps/Minecraft/minecraft-prep-and-install/#docker-server-setup","title":"Docker Server Setup","text":"<p>Docker-compose for minecraft server</p> <pre><code>version: \"3.9\"\nservices:\n  minecraft:\n    image: marctv/minecraft-papermc-server:latest\n    restart: unless-stopped\n    container_name: mcserver\n    environment:\n\n      - MEMORYSIZE=4G\n      - PAPERMC_FLAGS=\"\"\n      - PUID=1000\n      - PGID=1000\n    volumes:\n      - ~/docker/minecraft:/data:rw\n    ports:\n      - 25565:25565\n      - 19132:19132\n      - 19132:19132/udp # geyser\n\u00a0 \u00a0 \u00a0 - 8100:8100 # bluemap\n    stdin_open: true\n    tty: true\n</code></pre> <p>This downloads the latest version of Minecraft, to use another PaperMC version, need to build the image from scratch.</p> <p>Warning: PaperMC cannot be downgraded, only newer version of PaperMC can be installed after first run.</p> <pre><code>git clone https://github.com/mtoensing/Docker-Minecraft-PaperMC-Server\n# go edit the \"ARG version=1.xx.x\" to the correct version\ndocker build -t marctv/mcserver:1.xx.x\n</code></pre>"},{"location":"Docker%20Apps/Minecraft/minecraft-prep-and-install/#folders-and-plugins","title":"Folders and Plugins","text":"<p>Plugins are located in folder <code>./plugins</code> some plugins have .yml files. To update or download plugins, use scp, wget on the server or VSCode.</p> <p>The <code>world</code> folder consists of the save data. It is separated into world, nether, the_end.</p> <p>Before starting the server, the <code>eula.txt</code> must have eula=true.</p> <p><code>bukkit</code> and <code>spigot.yml</code> in the root folder are configuration files for PaperMC.</p>"},{"location":"Docker%20Apps/Minecraft/minecraft-prep-and-install/#rcon-commands","title":"Rcon Commands","text":"<p>To access the rcon-cli, use <code>docker attach mcserver</code>, to exit, use Ctrl-P and Q, if using VSCode may need to edit keyboard shortcut.</p> <p>Editing VSCode Shortcut Press <code>Ctrl-Shift-P</code> and search for keyboard shortcut json.</p> <pre><code>[\n    {\n        \"key\": \"ctrl+p\",\n        \"command\": \"ctrl+p\",\n        \"when\": \"terminalFocus\"\n    },\n\n    {\n        \"key\": \"ctrl+q\",\n        \"command\": \"ctrl+q\",\n        \"when\": \"terminalFocus\"\n    },\n\n    {\n        \"key\": \"ctrl+e\",\n        \"command\": \"ctrl+e\",\n        \"when\": \"terminalFocus\"\n    }\n\n]\n</code></pre>"},{"location":"Docker%20Apps/Minecraft/useful-plugins/","title":"Useful Plugins","text":"<p>WorldEdit</p> <p>EssentialX</p> <p>CoreProtect</p> <p>ViaVersions - allow other similar version to join the server without conflict</p> <p>bluemap</p> <p>Geyser</p> <p>WorldGuard</p>"},{"location":"Docker%20Apps/Minecraft/useful-plugins/#offline-modemobile-bedrock","title":"Offline Mode/Mobile Bedrock","text":"<p>To allow offline play for PC version. Change <code>server.properties</code> and edit these lines <pre><code>enforce-whitelist=false\nonline-mode=false\n</code></pre> Refer to  Minecraft Prep and Install to install offline client.</p> <p>For bedrock compatibility, need the geyser plugin.</p> <p>To allows offline play for bedrock mobile version. Go to <code>./plugins/Geyser-Spigot/config.yml</code> and change these lines. Do not install the plugin floodgate, if it\u2019s installed, removed the plugin. ViaVersions is also needed for mobile play.</p> <pre><code>auth-type: offline\nenable-proxy-connections: true\n</code></pre> <p>Now client can play without login to Xbox or Java.</p>"},{"location":"Docker%20Apps/Web/authelia/","title":"Authelia Single Sign On (SSO)","text":"Docker Apps Rating U/GID TZ SSO/Users Existing FS Portable Subfolder Mobile \u274e \u2705 \u2705\ud83d\udc6a \u2705 n/a \u2705 \u2705 <p>Configuration example of Authelia https://gist.github.com/vttc08/cfa1f15c662ccddc1af2dcca3ed7009d The files regarding ldap and SSO will be placed in the docker folder <code>authentication</code> as such <pre><code>\u251c\u2500\u2500 authelia\n\u2502   \u251c\u2500\u2500 configuration.yml\n\u2502   \u251c\u2500\u2500 db.sqlite3\n\u2502   \u251c\u2500\u2500 notification.txt\n\u2502   \u2514\u2500\u2500 private.pem\n\u251c\u2500\u2500 lldap\n\u2502   \u251c\u2500\u2500 lldap_config.toml\n\u2502   \u2514\u2500\u2500 users.db\n\u251c\u2500\u2500 auth.env\n\u251c\u2500\u2500 compose.yml\n\u251c\u2500\u2500 oidc.env\n</code></pre></p> <ul> <li>a subfolder for each service\u2019s configuration and data files</li> <li>environments and compose are at the root folder The authentication stack will have it\u2019s own Docker bridge network <code>auth</code></li> </ul>"},{"location":"Docker%20Apps/Web/authelia/#secrets","title":"Secrets","text":"<p>To generate some alphanumeric strings for secret, this command is also set in <code>bash_aliases</code> as <code>secret</code>. <pre><code>tr -dc A-Za-z0-9 &lt;/dev/urandom | head -c 24; echo\n</code></pre></p> <p>The secrets are stored in <code>.env</code> files, alternatively Docker secrets can be used. There are 2 files</p> <ul> <li><code>auth.env</code> - secrets regarding Authelia and lldap</li> <li><code>oidc.env</code> - client secrets for Authelia OIDC supported apps</li> </ul> <p>auth.env<pre><code>DOMAIN_NAME=\"domain.mywire.org\"\nBASE_DN=\"dc=domain,dc=mywire,dc=org\"\n\nLLDAP_JWT_SECRET=\"sdafasdfasdfs\"\nLLDAP_LDAP_USER_PASS=\"securepassword\"\n\nAUTHELIA_JWT_SECRET=\"dfasdfasdfasd\"\nAUTHELIA_STORAGE_ENCRYPTION_KEY=\"dfsadfsadfasdfsd\"\n\nMAILUSER=\"thepartbefore_gmail_dot_com\"\nAUTHELIA_NOTIFIER_SMTP_PASSWORD=\"dsafasdfasdfsadf\"\n\nAUTHELIA_SESSION_DOMAIN=${DOMAIN_NAME}\nAUTHELIA_AUTHENTICATION_BACKEND_LDAP_PASSWORD=${LLDAP_LDAP_USER_PASS}\nAUTHELIA_TOTP_ISSUER=${DOMAIN_NAME}\nAUTHELIA_AUTHENTICATION_BACKEND_LDAP_BASE_DN=${BASE_DN}\nAUTHELIA_AUTHENTICATION_BACKEND_LDAP_USER=\"uid=admin,ou=people,${BASE_DN}\"\n\nLLDAP_LDAP_BASE_DN=${BASE_DN}\n</code></pre> oidc.env<pre><code>PORTAINER_SECRET=\"sdfasdfasdfsad\"\nANOTHER_APP_SECRET=\"dsfasdfasdfasdf\"\n</code></pre></p>"},{"location":"Docker%20Apps/Web/authelia/#lldap-config","title":"lldap Config","text":"<ul> <li><code>DOMAIN_NAME</code> is the full domain name</li> <li><code>BASE_DN</code> is the domain name but split by <code>.</code></li> <li><code>JWT_SECRET</code> randomly generated secret</li> <li><code>USER_PASS</code> the admin password used to login to admin interface <ul> <li>Special characters not allowed Since some configs such as <code>DOMAIN_NAME</code> is shared between Authelia and lldap, it is reused</li> </ul> </li> </ul>"},{"location":"Docker%20Apps/Web/authelia/#authelia-config","title":"Authelia Config","text":"<ul> <li><code>JWT_SECRET</code>, <code>ENCRYPTION_KEY</code> - randomly generated secret</li> <li><code>MAILUSER</code> the username before Gmail for SMTP email, in Authelia if <code>@</code> is provided in environment variable, it will crash</li> <li><code>SMTP_PASSWORD</code> - Gmail app password, Setup Gmail App Password</li> <li><code>ANOTHER_APP_SECRET</code> - for client secret of OIDC Authelia configuration options all start with <code>AUTHELIA</code>, the full list is here</li> </ul>"},{"location":"Docker%20Apps/Web/authelia/#lldap","title":"lldap","text":"<p>https://github.com/lldap/lldap</p>"},{"location":"Docker%20Apps/Web/authelia/#setup","title":"Setup","text":"<p>compose.yaml<pre><code>  lldap:\n    container_name: \"lldap\"\n    image: \"nitnelave/lldap:latest\"\n    restart: unless-stopped\n    networks:\n      - auth\n      - public\n    expose: # lldap only needed in the same docker network\n      - 3890 # LDAP service\n    ports: # can't expose because reverse proxy is running on another server\n      - 17170:17170 # Web service\n    env_file:\n      - auth.env\n    environment:\n      - UID=1000\n      - GID=1001\n      - TZ=America/Vancouver\n    volumes:\n      - ~/docker/authentication/lldap:/data:rw\n</code></pre>  Most of the configuration is already done with environment variables, it is also possible to configure options via <code>./lldap/lldap_config.toml</code>. Given the reverse proxy is located on another server and does not utilize Docker network and Authelia do utilize docker networks. The WebUI port is forwarded while LDAP is not.</p> <p>The configuration of users and groups are done in WebUI.</p> <ul> <li>only users in <code>lldap_admin</code> is allowed to login and manage users in WebUI</li> </ul>"},{"location":"Docker%20Apps/Web/authelia/#authelia","title":"Authelia","text":""},{"location":"Docker%20Apps/Web/authelia/#setup_1","title":"Setup","text":"<pre><code>  authelia:\n    container_name: authelia\n    image: \"authelia/authelia:latest\" # optional: pin version for stability\n    restart: unless-stopped\n    networks:\n      - auth\n      - public\n    ports: # has to be exposed because reverse proxy is running on another server\n      - 9091:9091\n    env_file:\n      - auth.env\n      - oidc.env\n    environment:\n      - PUID=1000\n      - PGID=1001\n      - TZ=America/Vancouver\n      - X_AUTHELIA_CONFIG_FILTERS=template\n    volumes:\n      - ~/docker/authentication/authelia:/config:rw\n</code></pre>"},{"location":"Docker%20Apps/Web/authelia/#environments","title":"Environments","text":"<p>Most of the environment variables and secrets are listed here and utilize <code>.env</code> files. The environment variable <code>X_AUTHELIA_CONFIG_FILTER=template</code> makes it possible to use environment variables in configuration</p> <ul> <li>to check the configuration is valid, use this Docker image <pre><code>docker exec -it --rm authelia authelia config template --config.experimental.filters template\n</code></pre></li> </ul>"},{"location":"Docker%20Apps/Web/authelia/#files","title":"Files","text":"<p>The folder  <code>./authentication/authelia</code> contain all the files</p> <ul> <li><code>configuration.yml</code> master configuration file, will be autogenerated by Authelia<ul> <li>MUST BE in <code>yml</code> format not <code>yaml</code></li> </ul> </li> <li><code>notification.txt</code> created by Authelia, in case SMTP setup is not possible</li> <li><code>private.pem</code> private key pair file required for OIDC</li> </ul>"},{"location":"Docker%20Apps/Web/authelia/#configuration","title":"Configuration","text":"<p>All the configuration in Authelia is done via <code>configuration.yaml</code>. A lot of configurable options are adapted from EasySelfHost.</p>"},{"location":"Docker%20Apps/Web/authelia/#environment-variables","title":"Environment Variables","text":"<p>The configuration can include sensitive values that is stored in Environments. This makes it safe to upload to public sites. The syntax are as follows: <pre><code>\u00a0 \u00a0 - domain:\n\u00a0 \u00a0 \u00a0 \u00a0 - 'auth.{{ env \"ENV_NAME\" }}'\n</code></pre></p> <ul> <li>the variable is templated with <code>{{ env</code></li> <li>when parsed this will result in <code>auth.yourdomain.tld</code></li> </ul>"},{"location":"Docker%20Apps/Web/authelia/#use-ldap","title":"Use LDAP","text":"<pre><code>authentication_backend:\n\u00a0 ldap:\n\u00a0 \u00a0 url: \"ldap://lldap:3890\"\n\u00a0 \u00a0 implementation: \"custom\"\n\u00a0 \u00a0 timeout: \"5s\"\n\u00a0 \u00a0 start_tls: false\n\u00a0 \u00a0 additional_users_dn: \"ou=people\"\n\u00a0 \u00a0 users_filter: \"(&amp;({username_attribute}={input})(objectClass=person))\"\n\u00a0 \u00a0 additional_groups_dn: \"ou=groups\"\n\u00a0 \u00a0 groups_filter: \"(member={dn})\"\n\u00a0 \u00a0 group_name_attribute: cn\n\u00a0 \u00a0 mail_attribute: mail\n\u00a0 \u00a0 display_name_attribute: displayName\n</code></pre>"},{"location":"Docker%20Apps/Web/authelia/#smtp","title":"SMTP","text":"<p>Before enabling SMTP, need to uncomment <code>filesystem</code> section in <code>notifier</code> <pre><code>notifier:\n\u00a0 disable_startup_check: false\n\u00a0 smtp:\n\u00a0 \u00a0 address: \"smtp://smtp.gmail.com:587\"\n\u00a0 \u00a0 sender: '{{ env \"MAILUSER\" }}@gmail.com'\n\u00a0 \u00a0 username: '{{ env \"MAILUSER\" }}@gmail.com'\n</code></pre> Gmail app password created at https://myaccount.google.com/apppasswords, requires 2FA on Google account.</p>"},{"location":"Docker%20Apps/Web/authelia/#sessions","title":"Sessions","text":"<p><pre><code>session:\n\u00a0 name: \"authelia_session\"\n\u00a0 same_site: \"lax\"\n\u00a0 inactivity: \"5m\"\n\u00a0 expiration: \"1h\"\n\u00a0 remember_me: \"2M\"\n</code></pre> Extend the session so user stay logged in for longer.</p>"},{"location":"Docker%20Apps/Web/authelia/#integrate-to-reverse-proxy-nginx-proxy-manager","title":"Integrate to Reverse Proxy (Nginx Proxy Manager)","text":"<p>Make a folder in NPM\u2019s data folder called <code>snippets</code> https://github.com/easyselfhost/self-host/tree/main/docker/authentication/authelia_snippets</p> <p>Use these snippets, <code>proxy.conf</code>, <code>authelia-location.conf</code> <code>authelia-authrequest.conf</code></p> <ul> <li>for <code>location.conf</code> - change the first line to the IP address and port of Authelia</li> <li>for <code>authrequest.conf</code> - change the last line to the domain of authentication portal Mount the snippet folder to NPM <pre><code>      - ~/docker/nginx-pm/snippets:/snippets\n</code></pre></li> </ul>"},{"location":"Docker%20Apps/Web/authelia/#providing-access-to-apps","title":"Providing Access to Apps","text":"<p>The code snippets are for Nginx Proxy Manager only. Other reverse proxies not tested.</p>"},{"location":"Docker%20Apps/Web/authelia/#access_control","title":"Access_Control","text":"<p>The <code>access_control</code> section in Authelia define who and which are allowed access. <pre><code>access_control:\n\u00a0 default_policy: \"deny\"\n</code></pre></p> <ul> <li>this is the Authelia default and best option as everything not mentioned will result in 403 also makes it easier to debug</li> <li>other policies include <code>one/two_factor</code> and <code>bypass</code></li> <li>the policies are parsed from top to bottom, so <code>bypass</code> rules should be placed first; the rules should go from specific to general</li> </ul>"},{"location":"Docker%20Apps/Web/authelia/#example","title":"Example","text":"<pre><code>    - domain:\n        - 'files.{{ env \"DOMAIN_NAME\" }}'\n        - 'files.{{ env \"DOMAIN_NAME\" }}'\n      policy: one_factor\n\u00a0 \u00a0 \u00a0 resources:\n\u00a0 \u00a0 \u00a0 \u00a0 - \"^.*/api/public/.*\" # File Browser bypass rules\n\u00a0 \u00a0 \u00a0 \u00a0 - \"^/api([/?].*)?$\" # arrs API whitelisting rule\n      subject:\n        - ['group:admin', 'group:minecraft']\n        - 'group:family'\n        - 'user:admin'\n</code></pre> <p>In this example configuration, only the specified are given the <code>one_factor</code> login, otherwise the default policy will apply</p> <ul> <li>access to <code>files.domain</code> and only if path contains <code>/api/</code> (the resources matching is also useful for bypass rules for APIs)<ul> <li>the example above is for <code>one_factor</code>, to whitelist APIs use <code>bypass</code></li> </ul> </li> <li>if the authenticated user belongs to family group or belongs to both admin and minecraft group For internal documentation use only. The Authelia environment will have a section for Minecraft friends, family members and bypass rules for anyone.</li> </ul>"},{"location":"Docker%20Apps/Web/authelia/#authentication-portal","title":"Authentication Portal","text":"<p><code>auth.domain.tld</code> <pre><code>location / {\n    include /snippets/proxy.conf;\n    proxy_pass $forward_scheme://$server:$port;\n}\n</code></pre></p>"},{"location":"Docker%20Apps/Web/authelia/#apps-without-auth-on-subdomain","title":"Apps without Auth on Subdomain","text":"<p><code>app.domain.tld</code> Protect applications (usually single user single session only) without internal authentication or it can be disabled (eg. Radarr, Kasm VNC), more definition here. <pre><code>include /snippets/authelia-location.conf;\n\nlocation / {\n  include /snippets/proxy.conf;\n  include /snippets/authelia-authrequest.conf;\n  proxy_pass $forward_scheme://$server:$port;\n}\n</code></pre></p>"},{"location":"Docker%20Apps/Web/authelia/#apps-without-auth-on-subfolder","title":"Apps without Auth on Subfolder","text":"<p><code>domain.tld/app</code> <pre><code>include /snippets/authelia-location.conf;\n\nlocation /baseurl/ {\n  include /snippets/proxy.conf;\n  include /snippets/authelia-authrequest.conf;\n  proxy_pass http://ip:port/;\n}\n</code></pre> If the app require websocket support, use these lines before <code>proxy_pass</code> <pre><code>  proxy_set_header Upgrade $http_upgrade;\n  proxy_set_header Connection $http_connection;\n</code></pre></p>"},{"location":"Docker%20Apps/Web/authelia/#oidc","title":"OIDC","text":"<p>Configure OIDC (OpenID Connect) in Authelia <pre><code>identity_providers:\n  oidc:\n    access_token_lifespan: 1h\n    authorize_code_lifespan: 1m\n    id_token_lifespan: 1h\n    refresh_token_lifespan: 90m\n    enable_client_debug_messages: false\n    enforce_pkce: public_clients_only\n    jwks:\n\n      - key: {{ secret \"/config/private.pem\" | mindent 10 \"|\" | msquote }}\n    cors:\n      endpoints:\n        - authorization\n        - token\n        - revocation\n        - introspection\n        - userinfo\n      allowed_origins:\n        - https://auth.{{ env \"DOMAIN_NAME\" }}\n      allowed_origins_from_client_redirect_uris: false\n</code></pre></p> <p>OIDC require generating a key for <code>jwks</code> <pre><code>openssl rsa -in private.pem -outform PEM -pubout -out public.pem\n</code></pre> The private key will be used as in <code>/config/private.pem</code>, use bind mounts accordingly</p>"},{"location":"Docker%20Apps/Web/authelia/#clients","title":"Clients","text":"<p>The example configuration for Portainer <pre><code>\u00a0 \u00a0 clients:\n\u00a0 \u00a0 \u00a0 - id: portainer\n\u00a0 \u00a0 \u00a0 \u00a0 client_name: Portainer\n\u00a0 \u00a0 \u00a0 \u00a0 client_secret: '$plaintext${{ env \"PORTAINER_SECRET\" }}'\n\u00a0 \u00a0 \u00a0 \u00a0 public: false\n\u00a0 \u00a0 \u00a0 \u00a0 authorization_policy: 'admin_only'\n\u00a0 \u00a0 \u00a0 \u00a0 redirect_uris:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - 'https://portainer.{{ env \"DOMAIN_NAME\"}}'\n\u00a0 \u00a0 \u00a0 \u00a0 scopes:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - 'openid'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - 'profile'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - 'groups'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - 'email'\n\u00a0 \u00a0 \u00a0 \u00a0 userinfo_signed_response_alg: 'none'\n</code></pre> Every client need an ID, secret (randomly generated). The secrets are loaded from <code>oidc.env</code>. Authorization policy can be one/two factor or a custom policy. </p> <p>Configuring on Portainer side (every clients are different):</p> <ul> <li>Go to Settings - Authentication - OAuth  The URLs for these are located in <code>auth.domain/.well-known/openid-configuration</code></li> </ul> <p>For clients to use Authelia that natively support OIDC, no special reverse proxy snippet is needed.</p>"},{"location":"Docker%20Apps/Web/authelia/#custom_policy","title":"Custom_Policy","text":"<p>The OIDC policies is different compared to standard ACL. It is a setting under  <code>identity_providers</code> -&gt; <code>oidc</code>, the syntax is similar. <pre><code>    authorization_policies:\n      admin_only:\n        default_policy: 'deny'\n        rules:\n\n          - policy: 'one_factor'\n            subject: \n            - 'group:admin'\n</code></pre></p> <ul> <li>the <code>admin_only</code> is the name of the rule will can be used for OIDC clients</li> <li>when logging in via OIDC with that rule, all users are not allowed to access except those belonging in admin group whom will get one factor</li> </ul>"},{"location":"Docker%20Apps/Web/caddy/","title":"Custom Caddy Lego","text":"<p>https://github.com/vttc08/caddy-lego Customized caddy docker container that has Dynu support for wildcard certificates.</p>"},{"location":"Docker%20Apps/Web/caddy/#install","title":"Install","text":"<p>Create a Docker network specific to publicly accessible container. <pre><code>docker network create public --subnet 172.80.0.0/16\n</code></pre></p> <ul> <li> <p>the Caddy container will have IP address of <code>172.80.44.3</code> <pre><code>services:\n  caddy:\n    image: vttc08/caddy\n    container_name: caddy\n    ports:\n      - 80:80\n      - 443:443\n    volumes:\n      - ~/docker/caddy/Caddyfile:/etc/caddy/Caddyfile\n      - ~/docker/caddy/www:/www\n    env_file:\n      - .env\n    environment:\n      - WHITELIST=${WHITELIST}\n    networks:\n      public:\n        ipv4_address: 172.80.44.3\n    restart: unless-stopped\n\nnetworks:\n  public:\n    external: true\n    name: public\n</code></pre></p> </li> <li> <p>the volume of caddy follows all other docker apps which is at <code>~/docker</code></p> </li> <li><code>.env</code> file for <code>DYNU_API_KEY</code> which will be used for SSL</li> <li>create a network <code>public</code> with the IP address</li> <li>it is not the best idea to use <code>user:</code> since it may break container function; however, it all the files are present when mounted Caddy should not change the permissions</li> <li><code>WHITELIST</code> is an environment variable that contains the IP address that can be only allowed on certain services<ul> <li>this can be created in <code>~/.bashrc</code> and sourced <pre><code>export WHITELIST=123.456.789.0\n</code></pre></li> </ul> </li> </ul> <p>The content of <code>.env</code> <pre><code>DYNU_API_KEY=\nWEBSITE=\nHTTPS=\nEMAIL=\n</code></pre></p> <ul> <li><code>HTTPS</code> list of domains so Caddy doesn\u2019t error when parsing comma; <code>\"*.website.dynu.com, website.dynu.com\"</code></li> <li><code>WEBSITE</code> just the website name <code>website.dynu.com</code></li> </ul>"},{"location":"Docker%20Apps/Web/caddy/#dockerfile","title":"Dockerfile","text":"<p>If the provided image doesn\u2019t work, need to build a image on the server itself. <pre><code>FROM caddy:2.7.5-builder-alpine AS builder\n\nRUN xcaddy build \\\n    --with github.com/caddy-dns/lego-deprecated\n\nFROM caddy:2.7.5\n\nCOPY --from=builder /usr/bin/caddy /usr/bin/caddy\n</code></pre> Then modify the <code>image</code> part of <code>compose.yml</code> <pre><code>    build:\n      context: .\n      dockerfile: Dockerfile\n</code></pre></p>"},{"location":"Docker%20Apps/Web/caddy/#caddyfile","title":"Caddyfile","text":"<pre><code>{\n    email {$EMAIL}\n}\n</code></pre>"},{"location":"Docker%20Apps/Web/caddy/#basic-website","title":"Basic Website","text":"<pre><code>:80 {\n        root * /usr/share/caddy\n        file_server\n}\n</code></pre>"},{"location":"Docker%20Apps/Web/caddy/#https","title":"HTTPS","text":"<pre><code>{$HTTPS} {\n        tls {\n                dns lego_deprecated dynu\n        }\n\n        # Standard reverse proxy\n        @web host web.{$WEBSITE$}\n        handle @web {\n                reverse_proxy mynginx:80\n        }\n}\n</code></pre> <ul> <li>start with <code>*.website</code> to indicate wildcard</li> <li>the tls block uses dynu</li> <li>declare <code>@web host</code> with the subdomain name <ul> <li>this is later used in <code>handle @web</code></li> <li>use <code>reverse_proxy</code> block to define the port to be reverse proxied In this method, only Docker containers that is in the same Docker network of <code>public</code> can be reverse proxied. By the internal port and via container names. Tailscale IP entries should also work.</li> </ul> </li> </ul>"},{"location":"Docker%20Apps/Web/caddy/#html-file-server","title":"HTML File Server","text":"<p>If caddy uses bind mount and access to the root of HTML files, it can be file server. First need to create the bind mount in <code>/www</code> of the container. Then edit the Caddyfile <pre><code>        @fs host fs.{$WEBSITE}\n        handle @fs {\n                root * /www\n                file_server\n                encode gzip\n        }\n</code></pre></p>"},{"location":"Docker%20Apps/Web/caddy/#environment-variables","title":"Environment Variables","text":"<p>The previous codeblock already utilize environment variables. The syntax is <code>{$NAME}</code>.</p>"},{"location":"Docker%20Apps/Web/caddy/#whitelisting","title":"Whitelisting","text":"<p><pre><code>                @blocked not remote_ip {$WHITELIST}\n                respond @blocked \"Unauthorized\" 403\n</code></pre> This respond 403 unauthorized on any IP addresses not in whitelist.</p>"},{"location":"Docker%20Apps/Web/caddy/#http-auth","title":"HTTP Auth","text":"<p>The option puts a simple HTTP login screen on endpoint. <pre><code>        handle @secure {\n            basicauth {\n                admin bcrypthashedpassword\n            }\n            reverse_proxy ariang:8080\n        }\n</code></pre></p>"},{"location":"Docker%20Apps/Web/caddy/#usage","title":"Usage","text":""},{"location":"Docker%20Apps/Web/caddy/#reloading","title":"Reloading","text":"<pre><code>docker exec -w /etc/caddy caddy caddy reload\n</code></pre>"},{"location":"Docker%20Apps/Web/ddns-update/","title":"Dynamic DNS Updater Docker","text":"<p>Official Image: https://hub.docker.com/r/linuxserver/duckdns Custom Github Page: https://github.com/vttc08/docker-duckdns-dynu</p> <p>This is a docker container that automatically updates the public IPv4 address of the server every 5 minutes to dynamic DNS services Dynu and DuckDNS. It is the fork of Linuxserver DuckDNS container.</p>"},{"location":"Docker%20Apps/Web/ddns-update/#docker-compose","title":"Docker Compose","text":"<pre><code>  services:\n      duckdns:\n        image: vttc08/docker-duckdns-dynu:latest\n        container_name: duckdns\n        env_file: ddns.env\n        environment:\n\n          - TZ=America/Vancouver\n          - PUID=1000\n          - PGID=1001\n        restart: unless-stopped\n</code></pre> <p>These need to be filled in the <code>ddns.env</code> <pre><code>DYNU_HOST= # full name of dynu domains\nDYNU_PASS= # md5 hashed dynu login pass\nSUBDOMAINS= # DuckDNS domains without the duckdns.org part\nTOKEN= # DuckDNS token \n</code></pre></p> <ul> <li>token will be visible in DuckDNS dashboard</li> <li>Dynu pass is the same as login; alternatively, it is possible to create a dedicated password  just for IP update  MD5 generator <pre><code>echo -n \"password\" | md5sum\n</code></pre></li> <li>when setting the IP to <code>10.0.0.0</code> in Dynu update API, dynu will automatically update the IP address to the IP address making that request</li> </ul>"},{"location":"Docker%20Apps/Web/ddns-update/#other-usage","title":"Other Usage","text":"<p><code>docker restart duckdns</code> will manually run IP update <code>docker exec -it duckdns /app/debug.sh</code> or other scripts, debug script will print out IP address of subdomains resolved by Cloudflare</p>"},{"location":"Docker%20Apps/Web/docker-3x-ui/","title":"3x-ui V2Ray","text":"Docker Apps Rating U/GID TZ SSO/Users Existing FS Portable Subfolder Mobile \u274c ? \u274c\ud83d\udc6a \u2705 \u2705 \u2705 \u2705 <p>https://github.com/MHSanaei/3x-ui The UI uses Xray for its core.</p>"},{"location":"Docker%20Apps/Web/docker-3x-ui/#install","title":"Install","text":"<p>Using compose <pre><code>services:\n  3x-ui:\n    image: ghcr.io/mhsanaei/3x-ui:latest\n    container_name: 3x-ui\n    volumes:\n\n      - ~/docker/db/:/etc/x-ui/\n      - ~/docker/cert/:/root/cert/\n    environment:\n      XRAY_VMESS_AEAD_FORCED: \"false\"\n      X_UI_ENABLE_FAIL2BAN: \"true\"\n    tty: true\n    network_mode: host\n    restart: unless-stopped\n</code></pre></p> <p>Create the folders in the current folder beforehand <pre><code>mkdir db cert\n</code></pre></p>"},{"location":"Docker%20Apps/Web/docker-3x-ui/#configuration","title":"Configuration","text":"<p>This is the default configuration <pre><code>Port: 2053\nUser: admin\nPassword: admin\nPath: /\n</code></pre></p> <p>The configuration for the administration are changed under <code>Panel Settings</code></p> <ul> <li>Port: <code>General</code> &gt; <code>Listen Port</code></li> <li>Path: <code>General</code> &gt; <code>URI Path</code>, it can be anything for easy memorization</li> <li>User/Pass: <code>Authentication</code> &gt; <code>Admin credentials</code> By default the UI is served over HTTP, this is fine for LAN access; but on public access VPS, it\u2019s recommended to use a reverse proxy for TLS. Without TLS, the page will show <code>Security Alert</code> warning in red.</li> </ul>"},{"location":"Docker%20Apps/Web/docker-3x-ui/#lan-access","title":"LAN Access","text":"<p>To achieve similar level of access as Tailscale for secure LAN access, this must be changed</p> <ul> <li><code>Xray Configs</code> &gt; <code>Basic Routing</code> &gt; <code>Blocked IPs</code> and uncheck LAN</li> </ul>"},{"location":"Docker%20Apps/Web/docker-3x-ui/#inbounds","title":"Inbounds","text":"<p>This is where the V2Ray endpoints are added. For the purpose of</p> <ul> <li>allowing LAN access like Tailscale and Wireguard</li> <li>TLS termination via a reverse proxy already running on 443 Only these options are possible, VMESS/VLESS + WebSocket (WS) + TLS Additional options such as Trojan, Shadowsocks, VLESS + XTLS (Vision or Reality) and more, but these would require additional port forwarding or not compatible with existing reverse proxy setup.</li> </ul> <p>To add a compatible inbound </p> <ul> <li>Protocol: choose either <code>vless</code> or <code>vmess</code></li> <li>choose any port</li> <li>under Transmission, choose <code>WebSocket</code></li> <li>change the Path to <code>/anything</code></li> <li>do not enable TLS for now</li> </ul> <p>After adding, click the add arrow, and there is a QR Code button, click to copy, it will show a QR code and copy to clipboard. (The QR Code might need to be expanded for 180% for mobile camera to see)</p>"},{"location":"Docker%20Apps/Web/docker-3x-ui/#reverse-proxy","title":"Reverse Proxy","text":""},{"location":"Docker%20Apps/Web/docker-3x-ui/#nginx-proxy-manager","title":"Nginx Proxy Manager","text":"<p>Under proxy hosts, add a new one or an existing one and adapt the config to the following </p> <ul> <li>the first page host and port doesn\u2019t matter, except Websockets Support must be checked</li> <li>add the <code>/anything</code> path as a custom location and the forwarded port is the same as the one chosen in 3x-ui<ul> <li>more VL/Mess path can be added</li> </ul> </li> <li>under SSL, choose a self-signed cert or one that is verified and enable HTTP/2 </li> </ul>"},{"location":"Docker%20Apps/Web/docker-3x-ui/#self-signed","title":"Self-Signed","text":"<p>ChatGPT reference: https://chatgpt.com/share/67fa180a-4e28-800b-a7b4-8f379a9d0556 Under Nginx Proxy Manager, <code>add SSL certificate</code> and choose <code>custom</code></p> <ul> <li>the Certificate Key is the Private Key</li> <li>the Certificate is the Fullchain File</li> </ul>"},{"location":"Docker%20Apps/Web/docker-3x-ui/#caddy","title":"Caddy","text":"<p>The configuration for websocket is straightforward <pre><code>fake.or.real.sni.com {\n        tls /usr/local/etc/v2ray/fullchain.pem /usr/local/etc/v2ray/privkey.pem\n        reverse_proxy http://localhost:11111\n        handle /wp-content {\n                reverse_proxy localhost:10181\n        }\n}\n</code></pre></p> <ul> <li>the <code>handle /path</code> must match the WS path and port</li> <li><code>tls fullchain privkey</code> is only necessary when using a self-signed cert, when using a real name, Caddy will automatically issue certs</li> </ul>"},{"location":"Docker%20Apps/Web/docker-3x-ui/#clients","title":"Clients","text":"<p>Self-Signed certs in Android are subject to MITM attacks</p> <p>In Windows, self-signed CA can be added to ROOT authority while on Android it\u2019s only possible as user. Even after installing self signed certs, it\u2019s installed as user rather than system. The TLS library in V2RayNG and any other Android app does not trust it. Hence, to use SNI names, the <code>allowInsecure</code> must be turn OFF, making it vulnerable to MITM. Only the app developers can fix this. It is unlikely firewalls will willingly MITM whitelisted SNI so this could be safe, but more testing is needed.</p>"},{"location":"Docker%20Apps/Web/docker-3x-ui/#android-v2rayng","title":"Android V2RayNG","text":"<p>Use the clipboard link. But change the port to 443, and add the appropriate SNI. Disable allowInsecure is used a fake SNI. In the App</p> <ul> <li><code>Settings</code> &gt; <code>Does VPN Bypass Lan</code> -&gt; <code>Not Bypass</code></li> <li><code>Routing Settings</code>, and if a rule called <code>\u7ed5\u8fc7\u5c40\u57df\u7f51IP</code>, exists, turn it off.</li> </ul> <p>After configuration, the Android phone on mobile data will be able to access locally hosted services, including UDP traffic like game streaming. In other Android clients</p> <p></p>"},{"location":"Docker%20Apps/Web/docker-3x-ui/#backupmaintenance","title":"Backup/Maintenance","text":"<p>In normal 3x-ui, the master <code>config.json</code> is located in <code>/usr/local/x-ui/bin/config.json</code>, which is the same in Docker, but this is not exposed and doesn\u2019t need to be, this the <code>x-ui.db</code> is the master file that is needed to recreate everything. The files created by 3x-ui are owned by root, use <code>chown</code> before moving to another server, otherwise, the files can be transferred directly to another server simple by copy/paste. </p>"},{"location":"Docker%20Apps/Web/docker-3x-ui/#todo","title":"Todo","text":"<p>DNS MITM Proof of Concept Advanced Routing iOS apps Clash/NekoBox Subscription Convenience</p>"},{"location":"Docker%20Apps/Web/tailscale-docker/","title":"Tailscale in Docker","text":""},{"location":"Docker%20Apps/Web/tailscale-docker/#tailscale-cloudflare-warp","title":"Tailscale Cloudflare Warp","text":"When subnet routing to Warp, latency is high <p>When tailscale residing in the Gluetun container and tailscale is advertising the routes (provided via GlueTun container) of local network. There will be high latency since all traffic even DNS are routed through Cloudflare and back to local subnet. Only use it when nessecary. It may be possible to change the route settings via API.</p>"},{"location":"Docker%20Apps/Web%20Archive/youtube-archiving/","title":"YouTube Archive","text":""},{"location":"Docker%20Apps/Web%20Archive/youtube-archiving/#pinchflat","title":"Pinchflat","text":"Docker Apps Rating <p>| U/GID | TZ  | SSO/Users | ExistingFS | Portable | Mobile | | ----- | \u2014 | --------- | -------- | -------- | ------- | -------- | | \u274e     | \u2705*  | n/a      | \u274c        | \u2705 | \u2714 |</p> <p>YouTube archiving solution. Default port <code>8945</code>. Default credential <code>none</code>.</p>"},{"location":"Docker%20Apps/Web%20Archive/youtube-archiving/#install","title":"Install","text":"<pre><code>services:\n  pinchflat:\n    container_name: pinchflat\n    image: ghcr.io/kieraneglin/pinchflat:latest\n    user: 1000:1001\n    environment:\n\n      - TZ=America/Vancouver\n    ports:\n      - '8945:8945'\n    networks:\n      - archive\n    volumes:\n      - ~/docker/pinchflat:/config\n      - /mnt/nvme/share/youtube:/downloads\n    restart: unless-stopped\n\nnetworks:\n  archive:\n    name: archive\n</code></pre> <ul> <li><code>user</code> definition to fix permission issues, container will run fine</li> <li>app uses <code>sqlite</code> database for volumes in <code>/config</code></li> <li>create custom network <code>archive</code> to easy container access with other apps</li> </ul>"},{"location":"Docker%20Apps/Web%20Archive/youtube-archiving/#usage","title":"Usage","text":""},{"location":"Docker%20Apps/Web%20Archive/youtube-archiving/#media-profile","title":"Media-Profile","text":"<p>The profile eg. resolution sponsorblock settings that is used to download videos also consist of renaming. The syntax are listed like such <code>/{{ source_custom_name }}/{{ channel }}/{{ upload_yyyy_mm_dd }} - {{ title }}.{{ ext }}</code>. More templates are available for customization. The example above shows a good naming for Jellyfin.</p>"},{"location":"Docker%20Apps/Web%20Archive/youtube-archiving/#sources","title":"Sources","text":"<p>The sources are YouTube playlist or channels. To download a Media Profile must be applied for the source. </p> <ul> <li>each source can have a custom name that can be applied as <code>{{ source_custom_name }}</code> The preferred method for indexing is <code>fast indexing</code> The <code>Download Cutoff Date</code> can be set and only videos uploaded after that day will be downloaded.</li> </ul> <p>Cutoff Date != Index</p> <p>The cutoff date set there does not prevent indexing. When a source is added, everything will be indexed even before the cutoff date. This will take a very long time on a big channel.</p>"},{"location":"Docker%20Apps/Web%20Archive/youtube-archiving/#cookies","title":"Cookies","text":"<p>The app support downloading private playlists via YouTube cookies. Cookies appears to be short-lived, more observations needed. (maybe use oauth2 plugin)</p>"},{"location":"Docker%20Apps/Web%20Archive/youtube-archiving/#api-key","title":"API Key","text":"<p>https://github.com/kieraneglin/pinchflat/wiki/Generating-a-YouTube-API-key API key can be used to for fast indexing.</p>"},{"location":"Docker%20Apps/Web%20Archive/youtube-archiving/#behavior","title":"Behavior","text":"<p>Pinchflat is not a media server, it only manage downloads not the files. Hence previous media cannot be imported. Everything is stored in its internal database.</p> <p>Deleting or managing the file in other applications will not get reflected by the app.</p> <ul> <li>if the file is deleted in the filesystem, it will still exist in app but no video will exists and the video needs to be redownloaded</li> <li>the only way to delete files is to delete via the app</li> </ul> <p>Media profile for each sources only have 1 chance of settings it right</p> <ul> <li>when changing media profile or editing profiles after a source is added and downloaded, even refreshing the metadata does not work</li> </ul> <p>New changes to sources are not reflected immediately</p> <ul> <li>eg. when changing the cutoff date or when a new video is added to playlist or channel</li> <li>to have it download new videos immediately, need to manually <code>force index</code></li> </ul>"},{"location":"Docker%20Apps/notification/apprise-api/","title":"Apprise API","text":"<p>https://github.com/caronc/apprise-api</p> Docker Apps Rating U/GID TZ SSO/Users Existing FS Portable Subfolder Mobile \u2705 \u2705 \u274c\ud83e\udd35 \u274c \u2705 n/a \u274c"},{"location":"Docker%20Apps/notification/apprise-api/#install","title":"Install","text":"<p>App is installed on port 7000 as port 8000 is taken. <pre><code>services:\n\u00a0 apprise-api:\n\u00a0 \u00a0 image: lscr.io/linuxserver/apprise-api:latest\n\u00a0 \u00a0 container_name: apprise-api\n\u00a0 \u00a0 environment:\n\u00a0 \u00a0 \u00a0 - PUID=${PUID}\n\u00a0 \u00a0 \u00a0 - PGID=${PGID}\n\u00a0 \u00a0 \u00a0 - TZ=America/Vancouver\n\u00a0 \u00a0 \u00a0 - APPRISE_ATTACH_SIZE=500\n\u00a0 \u00a0 volumes:\n\u00a0 \u00a0 \u00a0 - ~/docker/apprise-api/config:/config\n\u00a0 \u00a0 \u00a0 - ~/docker/apprise-api/attachments:/attachments\n\u00a0 \u00a0 ports:\n\u00a0 \u00a0 \u00a0 - 7000:8000\n\u00a0 \u00a0 restart: unless-stopped\n</code></pre></p> Attachment Folder <p>To use attachments, a local folder that is bind mount to <code>/attachments</code> must be created before spinning up the container otherwise there will be permission issues, despite PUGID, the <code>/attachments</code> folder permission is not set by the container. </p> <p>The option <code>APPRISE_ATTACH_SIZE</code> is the maximum size the server will accept for attachments and sending it, by default it\u2019s 200MB or other number in MB.</p>"},{"location":"Docker%20Apps/notification/apprise-api/#usage","title":"Usage","text":"<p>For a list of endpoints https://github.com/caronc/apprise?tab=readme-ov-file#productivity-based-notifications Specifics about endpoints will be used in internal documentation, this is for API server only.</p>"},{"location":"Docker%20Apps/notification/apprise-api/#adding-endpoint","title":"Adding Endpoint","text":"<p>To add a configuration entry, navigate to the IP:Port running the API and add <code>cfg/&lt;your-apprise-id&gt;</code> to create a new configuration.</p> <ul> <li>the same id is used for checking and editing configuration</li> </ul> <p>Example configuration <pre><code>apprise,discord=discord://&lt;userid&gt;/&lt;webhook&gt;/?avatar_url=https%3A//raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/apprise.png\nnewapp=slack://&lt;&gt;/&lt;&gt;/#channel\nadmin=apprise,newapp\n</code></pre></p>"},{"location":"Docker%20Apps/notification/apprise-api/#tags","title":"Tags","text":"<p>https://github.com/caronc/apprise-api/blob/master/README.md#screenshots After configuring API, apprise tags can be used, in above example.</p> <ul> <li>apprise or discord tag will send discord notification; while newapp tag will send slack notification</li> <li>with tag admin, both apprise and newapp tag are included</li> </ul>"},{"location":"Docker%20Apps/notification/apprise-api/#sending-notifications","title":"Sending Notifications","text":"<p>https://github.com/caronc/apprise/wiki/Notify_apprise_api Everything in single URL (to put into app configuration) <pre><code>apprise://10.10.120.12:7000/&lt;my-id&gt;?tags=tags\n</code></pre></p> <p>Apprise CLI <pre><code>apprise --config=http://10.10.120.12:7000/get/apprise -b \"\" -a \"/path/to/attachment\" --tag=tags\n</code></pre> Alternative, apprise configuration files can be used to include the API configuration.</p> <p>Standard cURL <pre><code>curl\u00a0-X\u00a0POST -F \"body=Test Message\"\u00a0-F \"tags=all\" \\\n    http://10.10.120.12:7000/notify/apprise\n</code></pre></p>"},{"location":"Docker%20Apps/notification/apprise-api/#maintenance","title":"Maintenance","text":"<p>Backup and restore is simple although the configurations are encrypted and not viewable, all it\u2019s needed is to copy the entire Docker folder to another folder and ensure volume mappings are correct. All the configurations are located.</p> <p>For internal documentation: The Apprise-API include a configuration with the id <code>apprise</code> which include all the internal Discord, Telegram and other endpoints. It is mostly for easy use with third-party apps and to send a notification, just put this URL in the supported applications as there is no need to remember all the Discord API webhooks. <pre><code>apprise://10.10.120.12:7000/apprise?tags=discord\n</code></pre></p>"},{"location":"Docker%20Apps/notification/apprise-api/#mailrise","title":"Mailrise","text":"<p>Convert standard SMTP mail into Apprise compatible messages.</p>"},{"location":"Docker%20Apps/notification/apprise-api/#setup","title":"Setup","text":"<p>Mailrise uses port 8025 by default rather than port 25. <pre><code>\u00a0 mailrise:\n\u00a0 \u00a0 image: yoryan/mailrise:latest\n\u00a0 \u00a0 container_name: mailrise\n\u00a0 \u00a0 ports:\n\u00a0 \u00a0 \u00a0 - '8025:8025'\n\u00a0 \u00a0 restart: unless-stopped\n\u00a0 \u00a0 volumes:\n\u00a0 \u00a0 \u00a0 - ~/docker/apprise-api/mailrise/mailrise.conf:/etc/mailrise.conf\n</code></pre> Docker compose deploy, must run as root.</p>"},{"location":"Docker%20Apps/notification/apprise-api/#configuration","title":"Configuration","text":"<p>The configuration is located in <code>/etc/mailrise.conf</code>, the file must be created before starting the container otherwise a folder will be created</p> <ul> <li>each entries consist of a name and a list of apprise style URLs <pre><code>configs:\n\u00a0 apprise:\n\u00a0 \u00a0 urls:\n\u00a0 \u00a0 \u00a0 - apprise://10.10.120.12:7000/apprise/?tags=nzbget\n\u00a0 qbitdiscord:\n\u00a0 \u00a0 urls:\n\u00a0 \u00a0 \u00a0 - apprise://10.10.120.12:7000/apprise/?tags=qbittorrent\n\u00a0 \u00a0 \u00a0 - discord://anotherurl/apikey\n</code></pre> Editing configuration may require docker restart</li> </ul>"},{"location":"Docker%20Apps/notification/apprise-api/#client","title":"Client","text":"<p>For mail client to send email using Apprise server. It must change the SMTP server address and port. </p> <ul> <li>the server address is the server running mailrise and port is 8025</li> <li>the recipient is <code>&lt;name&gt;@mailrise.xyz</code></li> <li>the from can be anything, as it will be displayed in the subject of message</li> </ul> <p>Powershell example <pre><code>send-mailmessage -from \"admin@homelab.local\" -to \"apprise@mailrise.xyz\" -subject \"Windows Test\" -body \"Test message\" -smtpserver laptopserver -port 8025\n# Windows Test (admin@homelab.local)\n</code></pre></p> <p>Qbittorrent  The email notification works in qbittorrent as expected.</p>"},{"location":"Linux%20Server/debian-based-server-setup/","title":"Debian-Based Server Setup","text":"<p>Run update and upgrade distro first. Install NTP package is there are errors with that. Reboot</p> <p>Setup powertop and powersaving features</p> <pre><code>sudo apt install powertop\npowertop --auto-tune\n</code></pre> <p>Powersave governor and at reboot. Remember to run the command again</p> <pre><code>@reboot echo \"powersave\" | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor &gt;/dev/null 2&gt;&amp;1\n</code></pre> <p>Ensure these packages are installed</p> <pre><code>powertop htop iotop fio curl gnupg wget ntfs-3g neofetch ca-certificates lsb-release hdparm hd-idle openssh-server at autojump screen bash-completion\n</code></pre> <ul> <li>after installing <code>bash-completion</code>, need to source <code>.bashrc</code> for Docker autocomplete to work</li> </ul>"},{"location":"Linux%20Server/debian-based-server-setup/#hdd","title":"HDD","text":"<p><code>lsblk</code> and <code>blkid</code> to get the ntfs hard drive /dev name and the /dev/by-uuid/\u2026</p> <p>Edit the fstab to mount the drive, same entry for nvme drive</p> <pre><code>UUID=CC34294F34293E38 /mnt/data ntfs-3g 0 0\n</code></pre> <p>If the mounted device is HDD array, need to spindown disk with hdparm</p> <pre><code>hdparm -B 120 /dev/sdb # set the APM level\nhdparm -S 241 /dev/sdb\n</code></pre> <p>For the -S spindown, 0-240 is multiple of 5s, 241-255 is multiple of 30 min. The above command set spindown every 30min.</p> <p>If hdparm does not work, hd-idle can be used. Edit the file in <code>/etc/defaults/hd-idle</code></p> <pre><code>-i 60 -a disk/by-uuid/xxx -l /var/log/hd-idle.log\n</code></pre> <p>Sudo without password, go to visudo and add the lines to the bottom, replace $USER with the actual username.</p> <pre><code>$USER ALL=(ALL) NOPASSWD: ALL\n</code></pre> <p>Edit shortcuts in bashrc</p> <pre><code>source .bashrc\n</code></pre>"},{"location":"Linux%20Server/debian-based-server-setup/#openssh-with-keys","title":"OpenSSH with Keys","text":"<p>^7003a1</p>"},{"location":"Linux%20Server/debian-based-server-setup/#generate-the-key-using-the-terminal","title":"Generate the key using the terminal","text":"<pre><code>ssh-keygen\n</code></pre> <ul> <li>give a location to put the key pair</li> <li>this generate a public (.pub) and private key pair</li> </ul> <pre><code>ssh-copy-id -i key.pub username@server\n</code></pre> <ul> <li><code>key.pub</code> is the public key that was generated</li> </ul> <p>The key is ready to use for authorization.</p>"},{"location":"Linux%20Server/debian-based-server-setup/#generate-keys-using-putty-software","title":"Generate keys using PuTTY software","text":"<ol> <li>Copy the red part and use nano to add it in the server <code>~/.ssh/authorized_keys</code></li> <li>Make sure permissions are correct <pre><code>mkdir -p ~/.ssh\nchmod 700 ~/.ssh\nchmod 600 ~/.ssh/authorized_keys\nnano ~/.ssh/authorized_keys\n</code></pre></li> <li>Save private key as ppk file on the root ssh folder.</li> <li> <p>If the client with private key is Linux machine, need to change the permission of the private key.</p> <pre><code>chmod 600 private.key\n</code></pre> </li> <li> <p>Convert the private key Conversion &gt; Export OpenSSH Keys and save the file to a folder OpenSSH Keys</p> </li> </ol>"},{"location":"Linux%20Server/debian-based-server-setup/#ssh-config","title":"SSH Config","text":"<p>Configuration file for easy SSH access. The permission for that file is 644. <pre><code>Host server\n  HostName 10.10.120.1\n  User ubuntu\n  IdentityFile ~/keys/server.key\n</code></pre></p> <p>Use with OliveTin</p> <p>To have seamless ssh experience with OliveTin, make sure to copy the <code>ssh config</code> file and all the keys to <code>/root</code>, since in OliveTin <code>~</code> means <code>/root</code> not your user home directory.</p>"},{"location":"Linux%20Server/debian-based-server-setup/#setting-up-smb","title":"Setting Up SMB","text":"<p>Refer to Samba(SMB) Setup to setup SMB server.</p>"},{"location":"Linux%20Server/debian-based-server-setup/#bashrc","title":"bashrc","text":"<p>The bashrc will contain server specific variable and functions. <pre><code>if [ -f ~/.bash_aliases ]; then\n    . ~/.bash_aliases\nfi\n</code></pre> Bashrc can also link to <code>bash_aliases</code> for more alias. Installing additional software may require bashrc changes, eg. <code>ffsubsync</code>, <code>autojump</code> To have a consistent experience even when using SSH, ensure these lines are removed from <code>bashrc</code>. However, contents of <code>bash_aliases</code> and aliases will not be loaded, only variables. <pre><code>case $- in\n    *i*) ;;\n      *) return;;\nesac\n</code></pre></p>"},{"location":"Linux%20Server/debian-based-server-setup/#desktop-environment-setup","title":"Desktop Environment Setup","text":""},{"location":"Linux%20Server/debian-based-server-setup/#firefox","title":"Firefox","text":"<p>The location of firefox profile is at /home/$USER/.mozilla/firefox/xxxxx.default</p> <p>Make a tarball and copy it and extract it in destination.</p> <p>In the profile folder, look for compatibility.ini, go to a random profile in the dest machine and copy the compatibility.ini settings to the one that is copied over. This ensure compatibility so that the new profile works without warning.</p> <p>Check the profile.ini with the name and the location of the new profile folder, firefox should be the same as before.</p> <pre><code>[Profile0]\nName=karis\nIsRelative=1\nPath=ims58kbd.default-esr-1\n</code></pre> <p>Themes</p> <p>To backup/restore settings of cinnamon</p> <p>Icons</p> <p>The icons are located at these locations.</p> <pre><code>/usr/share/icons\n~/.icons\n</code></pre> <p>Scripts</p> <p>Copy the scripts and put it into ~/script for organization and copy the old crontab for executing these scripts.</p>"},{"location":"Linux%20Server/olivetin/","title":"OliveTin","text":"<p>OliveTin exposes a webpage with buttons that execute shell command (eg. docker, scripts) on the server and allow others for easy access. It should be used internally only.</p> <p>Main Interface  Log Interface </p>"},{"location":"Linux%20Server/olivetin/#installation","title":"Installation","text":"<p>Download the correct file from this site. https://github.com/OliveTin/OliveTin/releases OliveTin_linux_amd64.deb</p> <p>Go to the directory and install the package.</p> <ul> <li>if a previous <code>config.yaml</code> is already present, installer will ask what to do, the default is to keep the previous config <pre><code>sudo dpkg -i OliveTin\u2026\u200bdeb\nsudo systemctl enable --now OliveTin\n</code></pre></li> </ul> <p>Uninstall <pre><code>sudo dpkg -r OliveTin # the installed app name, not the deb file\n</code></pre></p>"},{"location":"Linux%20Server/olivetin/#configuration","title":"Configuration","text":"<p>The configuration file is located at <code>/etc/OliveTin/config.yaml</code></p> Script Execution User <p>By default, OliveTin always execute script as root!! This have complications. With an example script that echo some location,  create a file in<code>/opt</code> dir owned by user 1000 and cd into <code>~/Downloads</code> user 1000\u2019s download dir.</p> default <p><code>/root/Downloads/</code> <code>line 7: cd: /root/Downloads: No such file or directory</code> The file created by the script is owned by root and not editable in VSCode or other editor unless using <code>sudo</code></p> as user 1000 <p><code>/home/test/Downloads/</code> The file created by the script is owned by user and can be freely edited.</p> <p>Run command as user user <code>sudo -u user /path/to/script</code>. </p> <ul> <li><code>~</code> path works as intended</li> <li>all files created and modified will be owned by user not root</li> <li><code>bashrc</code> variables do not work, to use environment variables, it must be sourced elsewhere</li> <li>by default, the script has a <code>$PWD</code> at <code>/root</code>, so relative path do not work regarding files</li> </ul> <p>Example Configuration <pre><code>listenAddressSingleHTTPFrontend: 0.0.0.0:1378 # set the port to 1378\n\n# Choose from INFO (default), WARN and DEBUG\nlogLevel: \"INFO\"\n\nactions:\n\n- title: Update Music\n  shell: /home/karis/scripts/script\n  icon: '&amp;#127925'\n  timeout: 2\n  hidden: true\n</code></pre> Configuration consists of list of actions, each action consist of <code>title</code>, <code>shell</code>, icon</p> <ul> <li><code>timeout</code> is also optional, the task will be killed if it takes longer (in seconds) to complete</li> <li><code>hidden</code> will hide it from dashboard<ul> <li>to unhide, a service restart is needed</li> </ul> </li> <li><code>maxConcurrent</code> optional, only allow x runs for the duration of the execution, any more will be blocked </li> <li>rateLimit more advance limiting<ul> <li>to clear a rate limit, OliveTin has to be restarted <pre><code>    maxRate:\n      - limit: 3\n        duration: 5m\n</code></pre></li> </ul> </li> </ul>"},{"location":"Linux%20Server/olivetin/#arguments","title":"Arguments","text":""},{"location":"Linux%20Server/olivetin/#textbox-input","title":"Textbox Input","text":"<pre><code>- title: Restart a Docker CT\n  icon: '&lt;img src = \"icons/restart.png\" width=\"48px\" /&gt;'\n  shell: docker restart {{ container }}\n  arguments:\n    - name: container\n      type: ascii\n</code></pre> <ul> <li>use <code>{{ }}</code> and give a variable</li> <li>under arguments type, assign a type for it, <code>ascii</code> only allows letters and numbers</li> </ul>"},{"location":"Linux%20Server/olivetin/#dropdown-choices","title":"Dropdown Choices","text":"<p><pre><code>- title: Manage Docker Stack Services\n  icon: \"&amp;#128736;\"\n  shell: docker-compose -f /home/karis/docker/bookstack/docker-compose.yml {{ action }}\n  arguments:\n    - name: action\n      choices:\n        - title: Start Stack\n          value: up -d\n        - title: Stop Stack\n          value: down\n</code></pre> This example give choices to start or stop a docker stack of a docker-compose file. If a argument is given the parameter choices, it will be in dropdown mode.</p>"},{"location":"Linux%20Server/olivetin/#suggestion","title":"Suggestion","text":"<p>Suggestion is a hybrid between dropdown and textbox. It will suggest the list of possible items in browser but do not restrict choices. <pre><code>  arguments:\n    - name: action\n      title: Action Name\n      suggestions:\n        - value: Information\n</code></pre></p> <ul> <li><code>value</code> is what is passed onto the shell and <code>Information</code> is a text display for clarification  After modifying configuration, it require a restart to clear out previous suggestions for browsers.</li> </ul>"},{"location":"Linux%20Server/olivetin/#execute-on-files-created-in-a-directory","title":"Execute on files created in a directory","text":"<p><pre><code>- title: Update Songs\n  icon: &lt;iconify-icon icon=\"mdi:music\"&gt;&lt;/iconify-icon&gt;\n  shell: /home/test/scripts/file.sh {{ filepath }}\n  arguments:\n    - name: filepath\n      type: unicode_identifier\n  execOnFileCreatedInDir: \n    - /home/test/Downloads/\n    - /another/folder\n</code></pre> Whenever a new file is created the action will execute.</p> <ul> <li><code>execOnFileCreatedInDir</code><ul> <li>it is possible to add multiple path to monitor; however, adding a path require a restart of OliveTin service</li> </ul> </li> <li>same principle as <code>Arguments</code>, whereas OliveTin provides predefined arguments for files. <code>filepath</code> is the full absolute path of the file that is created</li> </ul>"},{"location":"Linux%20Server/olivetin/#execution-feedback","title":"Execution Feedback","text":"<pre><code>- title: some action\n  popupOnstart: default, execution-dialog-stdout-only, execution-dialog, execution-button\n</code></pre> default stdout-only dialog button <ul> <li>popup dialog have an option to only show <code>stdout</code> or show full log output with exit code</li> <li>button will show how long the process take</li> <li>the design of popup box may not be easy to close, use the keyboard ++Esc++ key to close</li> </ul>"},{"location":"Linux%20Server/olivetin/#confirmation","title":"Confirmation","text":"<p>It is possible to have a confirmation before completing action. <pre><code>  arguments:\n\n    - type: confirmation\n      title: Click start to begin.\n</code></pre></p> <ul> <li>user must click a checkbox and then start before the action will execute</li> <li>API do not have such restrictions</li> </ul>"},{"location":"Linux%20Server/olivetin/#ssh-to-another-server","title":"SSH to Another Server","text":"<p>Since OliveTin by default runs command as root, it is necessary to copy the SSH <code>config</code> file and all the keys from a user\u2019s folder into <code>/root/.ssh</code></p> <ul> <li>if the permission is setup correctly for a user, the permissions will copy over</li> </ul> <p>On the first try, need to have this option when using SSH command <code>-o StrictHostChecking=no</code> and on the subsequent logins, ssh via ssh configs will work as normal.</p>"},{"location":"Linux%20Server/olivetin/#icons","title":"Icons","text":"<p>Icon Default Location Change</p> <p>The icon is now moved from <code>/var/www/olivetin</code> to the configuration directory which by default is at <code>/etc/Olivetin</code> and followed by <code>custom-webui/icons</code>, if these are not moved, the images will break.</p> <p>The icons need to be placed in a folder in <code>/etc/Olivetin/custom-webui/icons/icon.png</code> To use the icons, offline image or web address, it should be in HTML format. The size of 48px is the default size of OliveTin icons. Other CSS options such as <code>style=\"background-color: white;\"</code> also works. <pre><code>icon: '&lt;img src = \"custom-webui/icons/minecraft.png\" size=\"48px\" /&gt;'\n</code></pre> Icon with emoji, to use emoji, need to use the html code. https://symbl.cc/en/emoji/ For example, <code>&amp;#9786;</code> \ud83d\ude0a. <pre><code>icon: \"&amp;#9786;\"\n</code></pre></p> <p>For documentation purpose, the customs icons are in a <code>OliveTin Custom.psd</code> files</p>"},{"location":"Linux%20Server/olivetin/#third-party","title":"Third-Party","text":"<p>Olivetin only support iconify icons. To use it, search for an icon, under <code>components</code> select <code>Iconify Icon</code>  Add the pasted line into the configuration. <pre><code>  - title: Title\n    icon: &lt;iconify-icon icon=\"openmoji:jellyfin\"&gt;&lt;/iconify-icon&gt;\n</code></pre> Any valid CSS can be used for OliveTin icons, however, the CSS must not contain <code>#</code> for example the color code otherwise OliveTin will crash.</p>"},{"location":"Linux%20Server/olivetin/#icon-management","title":"Icon Management","text":"<p>The default icon folder is <code>/var/www/olivetin/icons</code> The icon folder of all homelab icons is in <code>~/icons/homelab</code></p>"},{"location":"Linux%20Server/olivetin/#api","title":"API","text":"<p>Simple action button. <pre><code>curl -X POST \"http://mediaserver:1378/api/StartAction\" -d '{\"actionId\": \"Update Music\"}'\n</code></pre> Action with Arguments. <pre><code>curl -X POST 'http://mediaserver:1378/api/StartAction' -d '{\"actionId\": \"Rename Movies\", \"arguments\": [{\"name\": \"location\", \"value\": \"value\"}]}'\n</code></pre></p> Arguments variable cannot be \u201cpath\u201d <p>If <code>path</code> is used as argument, when executing commands with arguments, it will replace the system <code>$PATH</code> variable, this will render most commands useless even basic ones like <code>sleep</code>, <code>date</code> etc. Use another variable such as <code>directory</code> or <code>location</code></p> Newest Olivetin Version Break Old API Method <p>The <code>actionName</code> key is deprecated and no longer works, newest Olivetin API only allow <code>actionId</code> for <code>StartAction</code> API endpoint. The scripts above are adjusted accordingly. To migrate, the easiest way it to create a ID in configuration that has the same value as action name. <pre><code>- title: action name\n- id: action name\n</code></pre></p>"},{"location":"Linux%20Server/olivetin/#dashboard","title":"Dashboard","text":"<p>Dashboard are a separate page from the default OliveTin page, Fieldsets and Folders are allowed to group actions only in dashboard.</p> <ul> <li>when an action is in dashboards, it does not appear in main view.</li> <li>when refreshing the page, it will always go back to main view even if the page is currently at a dashboard <pre><code>dashboards:\n  - title: My Dashboard\n    contents:\n      - title: Title Desc\n        type: fieldset\n        contents:\n          - title: Fix Epic Games\n          - title: Restart Minecraft\n      - title: Update Metadata\n        type: fieldset\n        contents:\n          - title: Stuff\n            icon: '&lt;img src = \"icons/mcrestart.png\" width=\"64px\" /&gt;'\n            contents:\n               - title: Update Songs\n</code></pre></li> </ul> Preview <p></p>"},{"location":"Linux%20Server/olivetin/#fieldsets","title":"Fieldsets","text":"<p>Fieldsets are group of actions under a title. Any <code>title</code> that has <code>type: fieldset</code> defined is a fieldset, any actions are grouped under <code>contents</code> key and need to have matching title.</p>"},{"location":"Linux%20Server/olivetin/#folders","title":"Folders","text":"<p>Folders also group actions together in a dashboard and user need to click into the folder to see the actions.</p> <ul> <li>it is possible to use custom icons or title for folders as long as <code>type:</code> is not set and it has <code>contents:</code></li> </ul>"},{"location":"Linux%20Server/olivetin/#entities","title":"Entities","text":"<p>To use entities, an action, a dashboard entry, entities json/yaml file and entity update method is needed (when the action interact with the entity).</p> Preview of Entities Flowchart <p></p>"},{"location":"Linux%20Server/olivetin/#entities-file","title":"entities-file","text":"<p>It\u2019s  possible to use json or  YAML <pre><code>entities:\n  - file: /etc/OliveTin/entities/containers.json\n    name: container\n</code></pre></p> <ul> <li>entities file are stored in <code>/etc/OliveTin/entities</code></li> <li>the name of the entity will be reference as <code>container.attributes</code> in configuration</li> </ul>"},{"location":"Linux%20Server/olivetin/#entity-update","title":"entity update","text":"<pre><code>- title: Update container entity file\n  shell: 'docker ps -a --format \"{{ json . }}\" &gt; /etc/OliveTin/entities/entity.json\n  hidden: true\n  execOnStartup: true\n  execOnCron: '*/5 * * * *'\n</code></pre> <ul> <li>this is an action that is trigger by other actions that need to modify the entity, the purpose is to update the entity file</li> </ul>"},{"location":"Linux%20Server/olivetin/#entity-actions","title":"entity-actions","text":"<pre><code>- title: Check {{ container.Names }} Status\n  shell: echo {{ container.Status }}\n  entity: container\n  trigger: Update container entity file\n</code></pre> <p>The entity action is defined the same way as other actions.</p> <ul> <li><code>entity</code> need to be defined</li> <li><code>trigger</code> automatically update entity attributes (since executing this actions could change some attribute of an entity like starting a container)</li> <li>both title and shell can use <code>entity.attributes</code></li> </ul>"},{"location":"Linux%20Server/olivetin/#dashboard-entry","title":"dashboard-entry","text":"<pre><code> - title: CPanel\n    contents:\n      - title: 'Container {{ container.Names }} ({{ container.Image }})'\n        entity: container\n        type: fieldset\n        contents:\n          - type: display\n            title: |\n              {{ container.Status }} &lt;br /&gt;&lt;br /&gt;&lt;strong&gt;{{ container.State }}&lt;&gt;\n          - title: 'Check {{ container.Names }} Status'\n</code></pre> Preview <ul> <li>dashboard is the same configuration as in previous but now is able to utilize entities. </li> </ul>"},{"location":"Linux%20Server/sambasmb-setup/","title":"Samba(SMB) Setup","text":""},{"location":"Linux%20Server/sambasmb-setup/#setting-up-smb-server-on-linux","title":"Setting up SMB Server on Linux","text":"<p>Install the samba tool on Linux.</p> <pre><code>sudo apt update\nsudo apt install samba -y\n</code></pre> <p>Edit the <code>/etc/samba/smb.conf</code></p> <pre><code>[nvme_share]\n   comment = NVMe Share\n   path = /mnt/nvme/share\n   browseable = yes\n   read only = no\n</code></pre> <p><code>nvme_share</code> is the name of the Samba path which will appear in SMB clients and its path is accessed by <code>\\\\192.168.0.1\\nvme_share</code></p> <p></p> <p><code>path</code> is the location where the files are stored</p> <p><code>browseable</code> and <code>read only</code> are flags that are needed to make sure read/write access on the SMB share</p> <p>Lastly, add the user and password for the SMB share</p> <pre><code>sudo smbpasswd -a $USER # enter the password twice\n</code></pre> <p>In the case when Windows fail to write files in the samba share for odd reason. Go to <code>Manage Credentials</code> -&gt; <code>Windows Credentials</code> -&gt; <code>Add a Windows Credential</code> and fill the necessary address, username and password.</p>"},{"location":"Linux%20Server/sambasmb-setup/#setting-smb-client-in-linux","title":"Setting SMB Client in Linux","text":"<p>Install required apps <pre><code>sudo apt-get install cifs-utils -y\n</code></pre> Make the required folders for mounting SMB drive. <pre><code>sudo mkdir -p /mnt/cifs/nvme\n</code></pre> Create a credential file for the server.</p> <ul> <li>put it in a root folder directory eg <code>/root/.server-smbcred</code></li> <li>the file must be owned by root with 600 permission <pre><code>username=user\npassword=password\n</code></pre> <pre><code>sudo chown root: /root/.smbcred\nsudo chmod 600 /root/.smbcred\n</code></pre> Test the mount <pre><code>sudo mount -t cifs -o credentials=/root/.mediaserver-smbcred,uid=1000,gid=1001,dir_mode=0755,file_mode=0755 //serverip/nvme_share /mnt/cifs/nvme\n</code></pre></li> <li>make sure to set the <code>uid,gid,dir_mode,file_mode</code> so the files are not owned by root and not writable</li> </ul> <p>FSTAB permeant mount <code>/etc/fstab</code> <pre><code>//10.10.120.16/tv_2_share  /mnt/cifs/tv2  cifs  credentials=/root/.mediaserver-smbcred,uid=1000,gid=1001,file_mode=0755,dir_mode=0755 0 0\n</code></pre></p>"},{"location":"Linux%20Server/wireguard/","title":"Wireguard","text":"<p>Setup Wireguard from scratch on Linux.</p> <pre><code>sudo apt install wireguard wireguard-tools -y\n</code></pre> <p>This documentation will also focus on exposing private LAN devices. It covers IPv4 only.</p>"},{"location":"Linux%20Server/wireguard/#architecture","title":"Architecture","text":"<p>While using a fast VPN like CloudFlare WARP will significantly improve bad routes, this only works for services that are exposed on the internet via a port forward (e.g. Jellyfin). Or local services and LAN devices not exposed on the open internet, WARP cannot access it, this is usually the job of tailscale and similar. However, because tailscale is \u201ctoo good\u201d trying a direct connection, hence the traffic gets throttled. Attempts has been made trying to route tailscale over WARP (which we don\u2019t control) and without substantial testing, it\u2019s not working as expected.</p> <p>The approach below attempts to connect to a Oracle Cloud Free Tier VPS Oracle Cloud VPS which is fully controllable and unthrottled in order to improve the traffic between home LAN and remote devices. Additionally, this Wireguard tunnel is useful for future CG-NAT situations where a public IPv4 is not possible.</p> <p> The overall design of this architecture is to fix network throttling and routing problems. However, due to distance, this greatly increases latency. </p> <p>The architecture contains 3 devices to setup</p> <ul> <li>VPS - handling routing of all traffic</li> <li>LAN - act as local subnet router</li> <li>Client(s) - mobile phones, laptops out and about</li> </ul>"},{"location":"Linux%20Server/wireguard/#setup","title":"Setup","text":"<p>Wireguard configurations are located in <code>/etc/wireguard</code> as <code>wg.conf</code> where <code>wg</code> is the interface name. The command <code>wg</code> and <code>wg-quick</code> are used to manage it.</p> <ul> <li>Wireguard commands and config files editing requires root account</li> </ul>"},{"location":"Linux%20Server/wireguard/#prep","title":"Prep","text":"<p>Must make sure IP forwarding is turned on all machines. Do so by editing the file <pre><code>sudo nano /etc/sysctl.conf\nsudo sysctl -p\n</code></pre> Make sure this line in uncommented and available, reboot if necessary. <pre><code>net.ipv4.ip_forward=1\n</code></pre></p>"},{"location":"Linux%20Server/wireguard/#key","title":"Key","text":"<p>Wireguard uses asymmetric key cryptography, each client needs to public and private key. <pre><code>for i in vps lan client; do $(wg genkey | tee $i\\_private | wg pubkey &gt; $i\\_public); done;\n</code></pre> Manual <pre><code>wg genkey | tee privatekey | wg pubkey &gt; pubkey\n</code></pre></p>"},{"location":"Linux%20Server/wireguard/#vps","title":"VPS","text":"<pre><code>[Interface]\nPrivateKey= # private key of VPS\nAddress={ipv4} # 10.200.200.1/24\nListenPort=51820\nPostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o $IF -j MASQUERADE\nPostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o $IF -j MASQUERADE\n\n# LAN\n[Peer]\nPublicKey= # public key of LAN\nAllowedIPs= # 10.200.200.2/24, or additional networks\n\n# Client\n[Peer]\nPublicKey=# public key of client\nAllowedIPs= # 10.200.200.3/32\n</code></pre> <ul> <li>the <code>Address</code> is interface is the subnet of the Wireguard interface, it can be anything as long as it doesn\u2019t conflict</li> <li><code>ListenPort</code> needs to be forwarded in the firewall or it to work</li> <li><code>PostUp</code> and <code>PostDown</code> are commands to execute when the tunnel is setup</li> <li>the LAN subnet CIDR goes in <code>AllowedIPs</code> of LAN peer only</li> </ul> <p>Do not forget to replace <code>$IF</code> with the real network interface of VPS</p>"},{"location":"Linux%20Server/wireguard/#lan","title":"LAN","text":"<p>This is the subnet router on the local network <pre><code>[Interface]\nAddress = # 10.200.200.2/24\nPrivateKey = # private key of LAN\nPostUp = iptables -A FORWARD -i wgnat -j ACCEPT; iptables -t nat -A POSTROUTING -o $IF -j MASQUERADE\nPostDown = iptables -D FORWARD -i wgnat -j ACCEPT; iptables -t nat -D POSTROUTING -o $IF -j MASQUERADE\n\n[Peer]\nPublicKey = # public key of VPS\nEndpoint = # VPS IP and ListenPort\nAllowedIPs = # VPS Wireguard IP, must be a /24 not /32\nPersistentKeepalive = 60\n</code></pre></p> <ul> <li><code>PersistentKeepalive</code> always send a packet to keep the connection for every interval</li> <li>no need to put information of the client, just the VPS</li> </ul> <p>Do not forget to replace <code>$IF</code> with the real network interface of LAN device Additional <code>iptables</code> rules are required (install if <code>iptables</code> not found). Todo later, make the rule available on startup. <pre><code>iptables -A FORWARD -i $WGIF -j ACCEPT\niptables -t nat -A POSTROUTING -o $IF -j MASQUERADE\n</code></pre></p>"},{"location":"Linux%20Server/wireguard/#client","title":"Client","text":"<p>Any laptop, mobile phone connecting from outside <pre><code>[Interface]\nAddress = # 10.200.200.3/24\nPrivateKey = # private key of client\n\n[Peer]\nPublicKey = # public key of VPS\nEndpoint = # VPS IP and ListenPort\nAllowedIPs = 0.0.0.0/0\nPersistentKeepalive = 60\n</code></pre></p> <ul> <li><code>AllowedIPs</code> can also be set to CIDR of LAN subnet and Wireguard interface IP only route local traffic</li> </ul> <p>Use this website to generate QR Code from configuration file. Use this command to add more clients <pre><code>sudo wg set wgIF peer $pubkey_of_client allowed-ips $ip # e.g. 10.0.0.2/32\n</code></pre> Use <code>qrencode</code> package <pre><code>qrencode -t ansiutf8 &lt; /etc/wireguard/wgX.conf\n</code></pre> Save as file <pre><code>qrencode -t png -o qrcode.png -r /etc/wireguard/wg-client.conf\n</code></pre></p>"},{"location":"Linux%20Server/wireguard/#usage","title":"Usage","text":"<p>Use <code>wg-quick</code> to start the service in VPS and LAN <pre><code>wg-quick up wg # name of WG .conf file in /etc/wireguard interface\n</code></pre> Command for quickly restarting WG interface <pre><code>alias wgreset=\"wg-quick down wg &amp;&amp; wg-quick up wg\"\n</code></pre></p>"},{"location":"Linux%20Server/wireguard/#port-forwarding","title":"Port Forwarding","text":"<p>On Oracle Cloud, only <code>firewall-cmd</code> works</p> <p><code>firewall-cmd</code> or <code>firewalld</code> must be installed on Oracle Cloud VPS, <code>ufw</code> or plain <code>iptables</code> won\u2019t work. It is possible to try Docker. More testing are needed.</p> <p>The commands for opening ports for <code>firewall-cmd</code> <pre><code> sudo firewall-cmd --add-port 1234/udp --zone=public --permanent\n sudo firewall-cmd --reload\n</code></pre></p> <ul> <li>replace the port with the <code>ListenPort</code> of Wireguard</li> </ul> <p>Here are some of the recommended ports; however, these would be futile again DPI.</p> <ul> <li>3478/UDP - STUN protocol, usually for video meeting apps like Zoom, Teams</li> <li>443/UDP - QUIC protocol for web browsing</li> <li>53/UDP - Domain Name System</li> <li>123/UDP - Network Time Protocol</li> </ul> <p>Additional <code>firewall-cmd</code> commands may be needed. Reference: tailscale <pre><code>firewall-cmd --permanent --add-masquerade\nsudo firewall-cmd --add-interface=$WGIF --zone=trusted --permanent\nsudo firewall-cmd --reload\n</code></pre></p>"},{"location":"Linux%20Server/wireguard/#reference","title":"Reference","text":"<p>https://hacdias.com/2020/11/30/access-network-behind-cgnat/ https://blog.alekc.org/posts/how-to-expose-service-behind-nat-with-wireguard-and-vps/</p>"},{"location":"Website/ghost-cms/","title":"Ghost CMS","text":""}]}